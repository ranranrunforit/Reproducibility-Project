{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_A1M01Ux5gl"
      },
      "source": [
        "# CS598 DL4H: Reproducibility Project Proposal for Graph Attention Networks model (Spring 2024)\n",
        "Chaoran Zhou\n",
        "\n",
        "cz78@illinois.edu\n",
        "\n",
        "Group ID: 33\n",
        "\n",
        "Paper ID: 13\n",
        "\n",
        "Github: https://github.com/ranranrunforit/Reproducibility-Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlv6knX04FiY"
      },
      "source": [
        "### Mount Notebook to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfk8Zrul_E8V",
        "outputId": "215a2617-6cf8-4de2-8d56-435c08fb2b18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tS2I_7e0x5gq",
        "outputId": "67d5a7f2-4ffe-4b7e-dc2f-694904c5c09a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive\n"
          ]
        }
      ],
      "source": [
        "# change the working directory to the Drive root\n",
        "%cd /content/drive/My\\ Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdGmGxThx5gq"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "## Background of the Problem\n",
        "\n",
        "Many critical tasks involve data represented in irregular domains such as 3D meshes, social networks, telecommunication networks, biological networks, and brain connectomes, which can be modeled as graphs. Traditional Convolutional Neural Networks (CNNs) struggle with irregular data structures, leading to the introduction of Graph Neural Networks (GNNs) to handle general graphs. Attention mechanisms, widely used in sequence-based tasks, offer the flexibility to focus on relevant parts of varying inputs. Graph Attention Networks (GAT) represent a significant advancement in effectively applying neural network architectures to graph-structured data, particularly in scenarios where the data does not adhere to a grid-like structure.\n",
        "\n",
        "## Paper Explanation\n",
        "\n",
        "The paper introduces Graph Attention Networks (GAT) as an innovative approach to processing graph-structured data using convolution-style neural networks with masked self-attentional layers. GAT enables nodes to assign varying weights to their neighbors without costly matrix operations or prior knowledge of the entire graph structure. By leveraging attention mechanisms efficiently within the graph attentional layer, GAT achieves computational efficiency and can parallelize operations across all nodes in the graph. This dynamic assignment of node importance based on relationships leads to improved information propagation and feature learning, resulting in state-of-the-art performance on node classification benchmarks. GAT offers a more effective and scalable solution for handling complex relationships in graph data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uygL9tTPSVHB"
      },
      "source": [
        "# Scope of Reproducibility:\n",
        "\n",
        "Hypotheses\n",
        "\n",
        "- Hypothesis 1: The GAT model is expected to achieve a mean classification accuracy of 83.0 ± 0.7% on transductive tasks using Cora datasets.\n",
        "\n",
        "- Hypothesis 2: The GAT model is expected to achieve a mean classification accuracy of 72.5 ± 0.7% on transductive tasks using Citeseer datasets.\n",
        "\n",
        "- Hypothesis 3: The GAT model is expected to achieve a mean microaveraged F1 score of 97.3 ± 0.2% on inductive tasks using PPI datasets.\n",
        "\n",
        "\n",
        "It is important to note that the hypothesis has been adjusted based on the implementation and testing with the GAT model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      },
      "source": [
        "# Methodology\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ocUPVe2ZiWn"
      },
      "source": [
        "It is important to note that in each section of this draft report, two GAT models' codes are presented.\n",
        "\n",
        "It is recommended to run one model's code at a time to accurately assess the performance of each model, given the time constraints of less than 8 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpncfiKLsK13",
        "outputId": "db6f387d-c888-4a2a-a190-c70f9dd7dd2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Apr 21 06:02:21 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yu61Jp1xrnKk",
        "outputId": "15549afc-c50d-4f3c-810d-aad194633a26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.9.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.4.0)\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.5.3\n"
          ]
        }
      ],
      "source": [
        "# Install library torch-torch-geometric\n",
        "!pip install torch-geometric\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NbPHUTMbkD3"
      },
      "source": [
        "##  Data\n",
        "\n",
        "### Data descriptions\n",
        "\n",
        "The project utilized three classic graph datasets, namely Cora, CiteSeer, and Protein-Protein Interactions (PPI) datasets.\n",
        "\n",
        "For transductive learning, both Cora and CiteSeer Datasets were used, representing networks of research papers with each connection representing a citation. The Cora dataset consists of 2708 scientific publications classified into seven classes, with a citation network of 5429 links. Each publication is described by a word vector (node features) indicating the presence or absence of 1433 unique words in a paper. The CiteSeer dataset consists of 3327 scientific publications classified into six classes, with a citation network of 4732 links. Each publication is described by a word vector indicating the presence or absence of 3703 unique words.\n",
        "\n",
        "For inductive learning, the PPI dataset contains 24 graphs corresponding to different human tissues, with 20 graphs for training, 2 for validation, and 2 for testing. The average number of nodes per graph is 2372, and each node has 50 features composed of positional gene sets, motif gene sets, and immunological signatures, as well as 121 labels from the Molecular Signatures Database."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzVUQS0CHry0"
      },
      "source": [
        "### Implementation code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9TiRFLux5gs"
      },
      "source": [
        "The data processing involves loading data from files for the Cora dataset, extracting features and labels, adapting the data to PyTorch format, creating an adjacency matrix, adding self-loops to the adjacency matrix, and transferring the data to the device. Similarly, for the Citeseer dataset, isolated nodes in the graph are identified and added as zero vectors, followed by the same data pre-processing steps as the Cora dataset.\n",
        "\n",
        "The trained models' predictive power is evaluated on 1000 test nodes, with an additional 500 nodes used for validation. In the case of the Cora dataset, there are 140 training nodes, while the Citeseer dataset has 120 training nodes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27lEopmix5gs"
      },
      "outputs": [],
      "source": [
        "# Function to load Cora\n",
        "import os\n",
        "import time\n",
        "import requests\n",
        "import tarfile\n",
        "import numpy as np\n",
        "import argparse\n",
        "import torch\n",
        "\n",
        "def load_cora(path='./datasets/cora/', device='cpu'):\n",
        "\n",
        "    # Set the paths to the data files\n",
        "    content_path = os.path.join(path, 'cora.content')\n",
        "    cites_path = os.path.join(path, 'cora.cites')\n",
        "\n",
        "    # Load data from files\n",
        "    content_tensor = np.genfromtxt(content_path, dtype=np.dtype(str))\n",
        "    cites_tensor = np.genfromtxt(cites_path, dtype=np.int32)\n",
        "\n",
        "    # Process features\n",
        "    features = torch.FloatTensor(content_tensor[:, 1:-1].astype(np.int32)) # Extract feature values\n",
        "    scale_vector = torch.sum(features, dim=1) # Compute sum of features for each node\n",
        "    scale_vector = 1 / scale_vector # Compute reciprocal of the sums\n",
        "    scale_vector[scale_vector == float('inf')] = 0 # Handle division by zero cases\n",
        "    scale_vector = torch.diag(scale_vector).to_sparse() # Convert the scale vector to a sparse diagonal matrix\n",
        "    features = scale_vector @ features # Scale the features using the scale vector\n",
        "\n",
        "    # Process labels\n",
        "    classes, labels = np.unique(content_tensor[:, -1], return_inverse=True) # Extract unique classes and map labels to indices\n",
        "    labels = torch.LongTensor(labels) # Convert labels to a tensor\n",
        "\n",
        "    # Process adjacency matrix\n",
        "    idx = content_tensor[:, 0].astype(np.int32) # Extract node indices\n",
        "    idx_map = {id: pos for pos, id in enumerate(idx)} # Create a dictionary to map indices to positions\n",
        "\n",
        "    # Map node indices to positions in the adjacency matrix\n",
        "    edges = np.array(\n",
        "        list(map(lambda edge: [idx_map[edge[0]], idx_map[edge[1]]],\n",
        "            cites_tensor)), dtype=np.int32)\n",
        "\n",
        "    V = len(idx) # Number of nodes\n",
        "    E = edges.shape[0] # Number of edges\n",
        "    adj_mat = torch.sparse_coo_tensor(edges.T, torch.ones(E), (V, V), dtype=torch.int64) # Create the initial adjacency matrix as a sparse tensor\n",
        "    adj_mat = torch.eye(V) + adj_mat # Add self-loops to the adjacency matrix\n",
        "\n",
        "    return features.to(device), labels.to(device), adj_mat.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zK4uSmNJx5gt",
        "outputId": "b6b60937-badf-4a19-929c-287c2487abac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "import requests\n",
        "import tarfile\n",
        "import glob\n",
        "\n",
        "# Set the parameters directly\n",
        "args = {\n",
        "    'seed': 9,  # 42\n",
        "    'no_cuda': False,\n",
        "    'no_mps': False,\n",
        "    'hidden_dim': 64,\n",
        "    'num_heads': 8,\n",
        "    'concat_heads': False,\n",
        "    'dropout_p': 0.6,\n",
        "    'lr': 0.005,\n",
        "    'l2': 5e-4,\n",
        "    'epochs': 300,\n",
        "    'patience': 100,\n",
        "    'print': True,\n",
        "}\n",
        "\n",
        "torch.manual_seed(args['seed'])\n",
        "use_cuda = not args['no_cuda'] and torch.cuda.is_available()\n",
        "use_mps = not args['no_mps'] and torch.backends.mps.is_available()\n",
        "\n",
        "# Set the device to run on\n",
        "if use_cuda:\n",
        "    device = torch.device('cuda')\n",
        "elif use_mps:\n",
        "    device = torch.device('mps')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "print(f'Using {device} device')\n",
        "\n",
        "# Load the cora dataset\n",
        "features, labels, adj_mat = load_cora(device=device)\n",
        "# Split the dataset into training, validation, and test sets\n",
        "idx = torch.randperm(len(labels)).to(device)\n",
        "idx_test, idx_val, idx_train = idx[:1000], idx[1000:1500], idx[1500:1620]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8NPP0hHx5gt"
      },
      "outputs": [],
      "source": [
        "# Function to load CiteSeer\n",
        "# (This is for the demonstration, you can run it. However running this cell will add chance to exceed the 8 minutes limits depends on the device )\n",
        "import os\n",
        "import time\n",
        "import requests\n",
        "import tarfile\n",
        "import numpy as np\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "\n",
        "def preprocess_index(cites_data, content_data):\n",
        "    # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
        "    # Find isolated nodes, add them as zero-vecs into the right position\n",
        "\n",
        "    # Get unique IDs from both files\n",
        "    content_ids = np.unique(content_data[:, 0])\n",
        "    cites_ids = np.unique(np.concatenate((cites_data[:, 0], cites_data[:, 1])))\n",
        "\n",
        "    # Create ID mappings\n",
        "    content_id_mapping = dict(zip(content_ids, range(len(content_ids))))\n",
        "    cites_id_mapping = {id: content_id_mapping.get(id, len(content_id_mapping) + idx) for idx, id in enumerate(np.setdiff1d(cites_ids, content_ids))}\n",
        "    len_diff = len(cites_id_mapping)\n",
        "    cites_id_mapping.update(content_id_mapping)\n",
        "\n",
        "    # Update IDs in .cites file\n",
        "    cites_data[:, 0] = [cites_id_mapping[id] for id in cites_data[:, 0]]\n",
        "    cites_data[:, 1] = [cites_id_mapping[id] for id in cites_data[:, 1]]\n",
        "\n",
        "    # Update IDs in .content file\n",
        "    content_data[:, 0] = [content_id_mapping[id] for id in content_data[:, 0]]\n",
        "    # Add len_diff rows to content_data\n",
        "    additional_rows = np.zeros((len_diff, content_data.shape[1]),  dtype=content_data.dtype)\n",
        "    additional_rows[:, 1:-1] = np.array([['0'] * (content_data.shape[1] - 2)] * len_diff)\n",
        "    additional_rows[:, 0] = np.arange(len(content_ids), len(content_ids) + len_diff)\n",
        "    additional_rows[:, -1] = np.array(['NONE'] * len_diff)\n",
        "\n",
        "    new_content_data = np.vstack((content_data, additional_rows))\n",
        "\n",
        "    return new_content_data, cites_data.astype(np.int64)\n",
        "\n",
        "def load_citeseer(path='./datasets/citeseer/', device='cpu'):\n",
        "\n",
        "    # Set the paths to the data files\n",
        "    content_path = os.path.join(path, 'citeseer.content')\n",
        "    cites_path = os.path.join(path, 'citeseer.cites')\n",
        "\n",
        "    # Load data from files\n",
        "    content_data = np.genfromtxt(content_path, dtype=np.dtype(str))\n",
        "    cites_data = np.genfromtxt(cites_path, dtype=np.dtype(str))\n",
        "\n",
        "    content_tensor, cites_tensor = preprocess_index(cites_data, content_data)\n",
        "\n",
        "    # Process features\n",
        "    features = torch.FloatTensor(content_tensor[:, 1:-1].astype(np.int32)) # Extract feature values\n",
        "    scale_vector = torch.sum(features, dim=1) # Compute sum of features for each node\n",
        "    scale_vector = 1 / scale_vector # Compute reciprocal of the sums\n",
        "    scale_vector[scale_vector == float('inf')] = 0 # Handle division by zero cases\n",
        "    scale_vector = torch.diag(scale_vector).to_sparse() # Convert the scale vector to a sparse diagonal matrix\n",
        "    features = scale_vector @ features # Scale the features using the scale vector\n",
        "\n",
        "    # Process labels\n",
        "    classes, labels = np.unique(content_tensor[:, -1], return_inverse=True) # Extract unique classes and map labels to indices\n",
        "    labels = torch.LongTensor(labels) # Convert labels to a tensor\n",
        "\n",
        "    # Process adjacency matrix\n",
        "    idx = content_tensor[:, 0].astype(np.int32) # Extract node indices\n",
        "    idx_map = {id: pos for pos, id in enumerate(idx)} # Create a dictionary to map indices to positions\n",
        "\n",
        "    # Map node indices to positions in the adjacency matrix\n",
        "    edges = np.array(\n",
        "        list(map(lambda edge: [idx_map[edge[0]], idx_map[edge[1]]],\n",
        "            cites_tensor)), dtype=np.int32)\n",
        "\n",
        "    V = len(idx) # Number of nodes\n",
        "    E = edges.shape[0] # Number of edges\n",
        "    adj_mat = torch.sparse_coo_tensor(edges.T, torch.ones(E), (V, V), dtype=torch.int64) # Create the initial adjacency matrix as a sparse tensor\n",
        "    adj_mat = torch.eye(V) + adj_mat # Add self-loops to the adjacency matrix\n",
        "\n",
        "    # return features.to_sparse().to(device), labels.to(device), adj_mat.to_sparse().to(device)\n",
        "    return features.to(device), labels.to(device), adj_mat.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A968BI7Dx5gt"
      },
      "outputs": [],
      "source": [
        "# Load the citeseer dataset\n",
        "# (This is for the demonstration, you can run it. However running this cell will add chance to exceed the 8 minutes limits depends on the device )\n",
        "features, labels, adj_mat = load_citeseer(device=device)\n",
        "# Split the dataset into training, validation, and test sets\n",
        "idx = torch.randperm(len(labels)).to(device)\n",
        "idx_test, idx_val, idx_train = idx[:1000], idx[1000:1500], idx[1500:1640]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYcGYzZIx5gt"
      },
      "source": [
        "For inductive learning using the PPI dataset, the data and dataloader were directly loaded from the torch_geometric library. The dataset contained 20 graphs for training, 2 for validation, and 2 for testing, with a batch size of 2 graphs utilized during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HTOHuaS-x5gu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "155ad527-dfed-4ecb-d825-02e01fb89b84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://data.dgl.ai/dataset/ppi.zip\n",
            "Extracting ./ppi.zip\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.datasets import PPI\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.transforms import AddSelfLoops\n",
        "\n",
        "# Load the PPI dataset\n",
        "train_dataset = PPI(root='', split='train', transform=AddSelfLoops())\n",
        "val_dataset = PPI(root='', split='val', transform=AddSelfLoops())\n",
        "test_dataset = PPI(root='', split='test', transform=AddSelfLoops())\n",
        "\n",
        "num_features = 50\n",
        "num_labels = 121\n",
        "train_loader = DataLoader(train_dataset, batch_size=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3muyDPFPbozY"
      },
      "source": [
        "##   Model\n",
        "###  Model descriptions\n",
        "The model architecture for Graph Attention Networks (GAT) as proposed by Veličković et al. consists of two main model configurations, transductive learning model on Cora and Citeseer datasets, and inductive learning model on the PPI dataset. Both models are initialized using Glorot initialization and trained to minimize cross-entropy on the training nodes using the Adam SGD optimizer with an initial learning rate of 0.005 for all datasets. In both cases, an early stopping strategy is employed on both the cross-entropy loss and accuracy (transductive) or micro-F1 (inductive) score on the validation nodes, with a patience of 100 epochs.\n",
        "\n",
        "For transductive learning, a two-layer GAT model is applied. The first layer consists of K = 8 attention heads computing F = 8 features each, followed by an exponential linear unit (ELU) nonlinearity. This results in a total of 64 features. The second layer is used for classification and consists of a single attention head that computes C features, where C is the number of classes, followed by a softmax activation. To address the small training set sizes, regularization is applied within the model. Specifically, L2 regularization with lambda = 0.0005 is applied during training. Additionally, dropout with p = 0.6 is applied to both layers’ inputs, as well as to the normalized attention coefficients.\n",
        "The architectural hyperparameters have been optimized on the Cora dataset and then reused for the Citeseer dataset.\n",
        "\n",
        "For inductive learning, a three-layer GAT model is applied. Both of the first two layers consist of K = 4 attention heads computing F = 256 features each, followed by an ELU nonlinearity, resulting in a total of 1024 features. The final layer is used for (multi-label) classification and consists of K = 6 attention heads computing 121 features each, which are averaged and followed by a logistic sigmoid activation. In this case, the training sets are sufficiently large, and there is no need to apply L2 regularization or dropout. However, skip connections across the intermediate attentional layer have been successfully employed. A batch size of 2 graphs is utilized during training for this task.\n",
        "\n",
        "\n",
        "Reference\n",
        "\n",
        "\n",
        "```\n",
        "@article{\n",
        "  velickovic2018graph,\n",
        "  title=\"{Graph Attention Networks}\",\n",
        "  author={Veli{\\v{c}}kovi{\\'{c}}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\\`{o}}, Pietro and Bengio, Yoshua},\n",
        "  journal={International Conference on Learning Representations},\n",
        "  year={2018},\n",
        "  url={https://openreview.net/forum?id=rJXMpikCZ},\n",
        "}\n",
        "```\n",
        "- Paper on arxiv: [arXiv:1710.10903v3](https://doi.org/10.48550/arXiv.1710.10903)\n",
        "- Original paper repository: [https://github.com/PetarV-/GAT](https://github.com/PetarV-/GAT)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjJn7-kmx5gu"
      },
      "source": [
        "### Implementation code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_Zmdmkqx5gu"
      },
      "source": [
        "For transductive learning GAT:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDQxSHayyLuG"
      },
      "outputs": [],
      "source": [
        "# implementation of GAT for Cora, Citeseer dataset\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GraphAttentionLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Graph Attention Layer (GAT) as described in the paper `\"Graph Attention Networks\" <https://arxiv.org/pdf/1710.10903.pdf>`.\n",
        "\n",
        "        This operation can be mathematically described as:\n",
        "\n",
        "            e_ij = a(W h_i, W h_j)\n",
        "            α_ij = softmax_j(e_ij) = exp(e_ij) / Σ_k(exp(e_ik))\n",
        "            h_i' = σ(Σ_j(α_ij W h_j))\n",
        "\n",
        "            where h_i and h_j are the feature vectors of nodes i and j respectively, W is a learnable weight matrix,\n",
        "            a is an attention mechanism that computes the attention coefficients e_ij, and σ is an activation function.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features: int, out_features: int, n_heads: int, concat: bool = False, dropout: float = 0.4, leaky_relu_slope: float = 0.2):\n",
        "        super(GraphAttentionLayer, self).__init__()\n",
        "\n",
        "        self.n_heads = n_heads # Number of attention heads\n",
        "        self.concat = concat # wether to concatenate the final attention heads\n",
        "        self.dropout = dropout # Dropout rate\n",
        "\n",
        "        if concat: # concatenating the attention heads\n",
        "            self.out_features = out_features # Number of output features per node\n",
        "            assert out_features % n_heads == 0 # Ensure that out_features is a multiple of n_heads\n",
        "            self.n_hidden = out_features // n_heads\n",
        "        else: # averaging output over the attention heads (Used in the main paper)\n",
        "            self.n_hidden = out_features\n",
        "\n",
        "        #  A shared linear transformation, parametrized by a weight matrix W is applied to every node\n",
        "        #  Initialize the weight matrix W\n",
        "        self.W = nn.Parameter(torch.empty(size=(in_features, self.n_hidden * n_heads)))\n",
        "\n",
        "        # Initialize the attention weights a\n",
        "        self.a = nn.Parameter(torch.empty(size=(n_heads, 2 * self.n_hidden, 1)))\n",
        "\n",
        "        self.leakyrelu = nn.LeakyReLU(leaky_relu_slope) # LeakyReLU activation function\n",
        "        self.softmax = nn.Softmax(dim=1) # softmax activation function to the attention coefficients\n",
        "\n",
        "        self.reset_parameters() # Reset the parameters\n",
        "\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        \"\"\"\n",
        "        Reinitialize learnable parameters.\n",
        "        \"\"\"\n",
        "        nn.init.xavier_normal_(self.W)\n",
        "        nn.init.xavier_normal_(self.a)\n",
        "\n",
        "\n",
        "    def forward(self,  h: torch.Tensor, adj_mat: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Performs a graph attention layer operation.\n",
        "\n",
        "        Args:\n",
        "            h (torch.Tensor): Input tensor representing node features.\n",
        "            adj_mat (torch.Tensor): Adjacency matrix representing graph structure.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor after the graph convolution operation.\n",
        "        \"\"\"\n",
        "        n_nodes = h.shape[0]\n",
        "\n",
        "        # Apply linear transformation to node feature -> W h\n",
        "        # output shape (n_nodes, n_hidden * n_heads)\n",
        "        h_transformed = torch.mm(h, self.W)\n",
        "        h_transformed = F.dropout(h_transformed, self.dropout, training=self.training)\n",
        "\n",
        "        # splitting the heads by reshaping the tensor and putting heads dim first\n",
        "        # output shape (n_heads, n_nodes, n_hidden)\n",
        "        h_transformed = h_transformed.view(n_nodes, self.n_heads, self.n_hidden).permute(1, 0, 2)\n",
        "\n",
        "        # getting the attention scores\n",
        "        # output shape (n_heads, n_nodes, n_nodes)\n",
        "        # e = self._get_attention_scores(h_transformed) ORGINIAL\n",
        "        source_scores = torch.matmul(h_transformed, self.a[:, :self.n_hidden, :])\n",
        "        target_scores = torch.matmul(h_transformed, self.a[:, self.n_hidden:, :])\n",
        "\n",
        "        # broadcast add\n",
        "        # (n_heads, n_nodes, 1) + (n_heads, 1, n_nodes) = (n_heads, n_nodes, n_nodes)\n",
        "        e = source_scores + target_scores.mT\n",
        "        e = self.leakyrelu(e)\n",
        "\n",
        "        # Set the attention score for non-existent edges to -9e15 (MASKING NON-EXISTENT EDGES)\n",
        "        connectivity_mask = -9e16 * torch.ones_like(e)\n",
        "        e = torch.where(adj_mat > 0, e, connectivity_mask) # masked attention scores\n",
        "\n",
        "        # attention coefficients are computed as a softmax over the rows\n",
        "        # for each column j in the attention score matrix e\n",
        "        attention = F.softmax(e, dim=-1)\n",
        "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
        "\n",
        "        # final node embeddings are computed as a weighted average of the features of its neighbors\n",
        "        h_prime = torch.matmul(attention, h_transformed)\n",
        "\n",
        "        # concatenating/averaging the attention heads\n",
        "        # output shape (n_nodes, out_features)\n",
        "        if self.concat:\n",
        "            h_prime = h_prime.permute(1, 0, 2).contiguous().view(n_nodes, self.out_features)\n",
        "        else:\n",
        "            h_prime = h_prime.mean(dim=0)\n",
        "\n",
        "        return h_prime\n",
        "\n",
        "class GAT(nn.Module):\n",
        "    \"\"\"\n",
        "    Graph Attention Network (GAT) as described in the paper `\"Graph Attention Networks\" <https://arxiv.org/pdf/1710.10903.pdf>`.\n",
        "    Consists of a 2-layer stack of Graph Attention Layers (GATs). The fist GAT Layer is followed by an ELU activation.\n",
        "    And the second (final) layer is a GAT layer with a single attention head and softmax activation function.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "        in_features,\n",
        "        n_hidden,\n",
        "        n_heads,\n",
        "        num_classes,\n",
        "        concat=False,\n",
        "        dropout=0.4,\n",
        "        leaky_relu_slope=0.2):\n",
        "        \"\"\" Initializes the GAT model.\n",
        "\n",
        "        Args:\n",
        "            in_features (int): number of input features per node.\n",
        "            n_hidden (int): output size of the first Graph Attention Layer.\n",
        "            n_heads (int): number of attention heads in the first Graph Attention Layer.\n",
        "            num_classes (int): number of classes to predict for each node.\n",
        "            concat (bool, optional): Wether to concatinate attention heads or take an average over them for the\n",
        "                output of the first Graph Attention Layer. Defaults to False.\n",
        "            dropout (float, optional): dropout rate. Defaults to 0.4.\n",
        "            leaky_relu_slope (float, optional): alpha (slope) of the leaky relu activation. Defaults to 0.2.\n",
        "        \"\"\"\n",
        "\n",
        "        super(GAT, self).__init__()\n",
        "\n",
        "        # Define the Graph Attention layers\n",
        "        self.gat1 = GraphAttentionLayer(\n",
        "            in_features=in_features, out_features=n_hidden, n_heads=n_heads,\n",
        "            concat=concat, dropout=dropout, leaky_relu_slope=leaky_relu_slope\n",
        "            )\n",
        "\n",
        "        self.gat2 = GraphAttentionLayer(\n",
        "            in_features=n_hidden, out_features=num_classes, n_heads=1,\n",
        "            concat=False, dropout=dropout, leaky_relu_slope=leaky_relu_slope\n",
        "            )\n",
        "\n",
        "    def forward(self, input_tensor: torch.Tensor , adj_mat: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Performs a forward pass through the network.\n",
        "\n",
        "        Args:\n",
        "            input_tensor (torch.Tensor): Input tensor representing node features.\n",
        "            adj_mat (torch.Tensor): Adjacency matrix representing graph structure.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor after the forward pass.\n",
        "        \"\"\"\n",
        "\n",
        "        # Apply the first Graph Attention layer\n",
        "        x = self.gat1(input_tensor, adj_mat)\n",
        "        x = F.elu(x) # Apply ELU activation function to the output of the first layer\n",
        "\n",
        "        # Apply the second Graph Attention layer\n",
        "        x = self.gat2(x, adj_mat)\n",
        "\n",
        "        return F.log_softmax(x, dim=1) # Apply log softmax activation function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import requests\n",
        "import tarfile\n",
        "import numpy as np\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "# train function\n",
        "\n",
        "\n",
        "def train_iter(epoch, model, optimizer, criterion, input, target, mask_train, mask_val):\n",
        "    start_t = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(*input)\n",
        "    loss = criterion(output[mask_train], target[mask_train])  # Compute the loss using the training mask\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Evaluate the model performance on training and validation sets\n",
        "    loss_train, acc_train = evaluate(model, criterion, input, target, mask_train)\n",
        "    loss_val, acc_val = evaluate(model, criterion, input, target, mask_val)\n",
        "\n",
        "    #if epoch % print_every == 0:\n",
        "        # Print the training progress at specified intervals\n",
        "    # print(f'Epoch: {epoch:04d} ({(time.time() - start_t):.4f}s) loss_train: {loss_train:.4f} acc_train: {acc_train:.4f} loss_val: {loss_val:.4f} acc_val: {acc_val:.4f}')\n",
        "    # Print the training progress\n",
        "    print(\" | \".join([f\"Epoch: {(epoch + 1) :04d}\", f\"time: {(time.time() - start_t):.4f}s\", f\"Train loss: {loss_train:.4f}\",\n",
        "                          f\"Train Accuracy: {acc_train:.4f}\",\n",
        "                          f\"Val loss: {loss_val:.4f}\",\n",
        "                          f\"Val Accuracy: {acc_val:.4f}\"]))\n",
        "\n",
        "    return loss_train, acc_train, loss_val, acc_val\n",
        "\n",
        "\n",
        "def training_and_evaluate(model, input, target, mask_train, mask_val, epochs, patience):\n",
        "    optimizer = Adam(model.parameters(), lr=args['lr'], weight_decay=args['l2'])\n",
        "    # model.to(device).reset_parameters()\n",
        "    # model.reset_parameters()\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    t_total = time.time()\n",
        "    loss_values = []\n",
        "    acc_values = []\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    best_val_loss = np.inf\n",
        "    curr_step = 0\n",
        "\n",
        "    best_model_state_dict = None\n",
        "    best_optimizer_state_dict = None\n",
        "\n",
        "    best = -1\n",
        "    best_epoch = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Evaluate the model performance on training and validation sets\n",
        "        loss_train, acc_train, loss_val, acc_val = train_iter(epoch + 1, gat_net, optimizer, criterion, (features, adj_mat), labels, idx_train, idx_val, args.val_every)\n",
        "\n",
        "        loss_values.append(loss_val)\n",
        "        acc_values.append(acc_val)\n",
        "\n",
        "\n",
        "        #if print_result:\n",
        "        #    print(f'Epoch: {epoch + 1:04d} ({(time.time() - start_t):.4f}s) loss_train: {loss_train:.4f} acc_train: {acc_train:.4f} loss_val: {loss_val:.4f} acc_val: {acc_val:.4f}')\n",
        "\n",
        "        if acc_val > best_val_acc or loss_val < best_val_loss:\n",
        "            best_val_acc = max(acc_val, best_val_acc)\n",
        "            best_val_loss = min(loss_val, best_val_loss)\n",
        "            best_model_state_dict = model.state_dict()\n",
        "            best_optimizer_state_dict = optimizer.state_dict()\n",
        "            best_epoch = epoch\n",
        "            curr_step = 0\n",
        "            best_so_far = True\n",
        "        else:\n",
        "            curr_step += 1\n",
        "            best_so_far = False\n",
        "\n",
        "        if best_so_far:\n",
        "            torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': best_model_state_dict,\n",
        "                    'optimizer_state_dict': best_optimizer_state_dict,\n",
        "                }, 'GAT')\n",
        "\n",
        "        if curr_step == patience:\n",
        "            print('Early stopping...')\n",
        "            print(f'Best Val Loss: {best_val_loss:.4f}, Best Val Acc: {best_val_acc:.4f}')\n",
        "            break\n",
        "\n",
        "    print(\"Optimization Finished!\")\n",
        "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "    # Restore best model\n",
        "    print('Loading {}th epoch'.format(best_epoch + 1))\n",
        "    state = torch.load('GAT')\n",
        "    model.load_state_dict(state['model_state_dict'])\n",
        "    loss_test, acc_test = evaluate(model, criterion, (features, adj_mat), labels, idx_test)\n",
        "    print(f'Test set results: loss {loss_test:.4f} accuracy {acc_test:.4f}')\n",
        "    return model, loss_values, acc_values, loss_test, acc_test\n",
        "\n",
        "# evaluate function\n",
        "def evaluate(model, criterion, input, target, mask):\n",
        "    # model = model.to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(*input)\n",
        "        output, target = output[mask], target[mask]\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "        acc = (output.argmax(dim=1) == target).float().sum() / len(target)\n",
        "\n",
        "    return loss.item(), acc.item()"
      ],
      "metadata": {
        "id": "YsRkWXof-hwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HJsME5g3vJj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import requests\n",
        "import tarfile\n",
        "import numpy as np\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "# train function\n",
        "def train_and_evaluate(model, input, target, mask_train, mask_val, epochs, patience, print_result):\n",
        "    optimizer = Adam(model.parameters(), lr=args['lr'], weight_decay=args['l2'])\n",
        "    # model.to(device).reset_parameters()\n",
        "    # model.reset_parameters()\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    t_total = time.time()\n",
        "    loss_values = []\n",
        "    acc_values = []\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    best_val_loss = np.inf\n",
        "    curr_step = 0\n",
        "\n",
        "    best_model_state_dict = None\n",
        "    best_optimizer_state_dict = None\n",
        "\n",
        "    best = -1\n",
        "    best_epoch = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Configure the optimizer and loss function\n",
        "\n",
        "        start_t = time.time()\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(*input)\n",
        "        loss = criterion(output[mask_train], target[mask_train])  # Compute the loss using the training mask\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Evaluate the model performance on training and validation sets\n",
        "        loss_train, acc_train = evaluate(model, criterion, input, target, mask_train)\n",
        "        loss_val, acc_val = evaluate(model, criterion, input, target, mask_val)\n",
        "\n",
        "        loss_values.append(loss_val)\n",
        "        acc_values.append(acc_val)\n",
        "\n",
        "        # Print the training progress\n",
        "        if print_result:\n",
        "            print(f'Epoch: {epoch + 1:04d} ({(time.time() - start_t):.4f}s) loss_train: {loss_train:.4f} acc_train: {acc_train:.4f} loss_val: {loss_val:.4f} acc_val: {acc_val:.4f}')\n",
        "\n",
        "        if acc_val > best_val_acc or loss_val < best_val_loss:\n",
        "            best_val_acc = max(acc_val, best_val_acc)\n",
        "            best_val_loss = min(loss_val, best_val_loss)\n",
        "            best_model_state_dict = model.state_dict()\n",
        "            best_optimizer_state_dict = optimizer.state_dict()\n",
        "            best_epoch = epoch\n",
        "            curr_step = 0\n",
        "            best_so_far = True\n",
        "        else:\n",
        "            curr_step += 1\n",
        "            best_so_far = False\n",
        "\n",
        "        if best_so_far:\n",
        "            torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': best_model_state_dict,\n",
        "                    'optimizer_state_dict': best_optimizer_state_dict,\n",
        "                }, 'GAT')\n",
        "\n",
        "        if curr_step == patience:\n",
        "            print('Early stopping...')\n",
        "            print(f'Best Val Loss: {best_val_loss:.4f}, Best Val Acc: {best_val_acc:.4f}')\n",
        "            break\n",
        "\n",
        "    print(\"Optimization Finished!\")\n",
        "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "    # Restore best model\n",
        "    print('Loading {}th epoch'.format(best_epoch + 1))\n",
        "    state = torch.load('GAT')\n",
        "    model.load_state_dict(state['model_state_dict'])\n",
        "    loss_test, acc_test = evaluate(model, criterion, (features, adj_mat), labels, idx_test)\n",
        "    print(f'Test set results: loss {loss_test:.4f} accuracy {acc_test:.4f}')\n",
        "    return model, loss_values, acc_values, loss_test, acc_test\n",
        "\n",
        "# evaluate function\n",
        "def evaluate(model, criterion, input, target, mask):\n",
        "    # model = model.to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(*input)\n",
        "        output, target = output[mask], target[mask]\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "        acc = (output.argmax(dim=1) == target).float().sum() / len(target)\n",
        "\n",
        "    return loss.item(), acc.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BesK4K5F8WvZ"
      },
      "outputs": [],
      "source": [
        "# Create the model\n",
        "# The model consists of a 2-layer stack of Graph Attention Layers (GATs).\n",
        "GAT_cora = GAT(\n",
        "    in_features=features.shape[1],          # Number of input features per node\n",
        "    n_hidden=args['hidden_dim'],            # Output size of the first Graph Attention Layer\n",
        "    n_heads=args['num_heads'],               # Number of attention heads in the first Graph Attention Layer\n",
        "    num_classes=labels.max().item() + 1,     # Number of classes to predict for each node\n",
        "    concat=args['concat_heads'],             # Whether to concatenate attention heads\n",
        "    dropout=args['dropout_p'],                # Dropout rate\n",
        "    leaky_relu_slope=0.2                     # Alpha (slope) of the leaky ReLU activation\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQk7PeHN4Z6q"
      },
      "outputs": [],
      "source": [
        "# Create the model\n",
        "# The model consists of a 2-layer stack of Graph Attention Layers (GATs).\n",
        "GAT_citeseer = GAT(\n",
        "    in_features=features.shape[1],          # Number of input features per node\n",
        "    n_hidden=args['hidden_dim'],            # Output size of the first Graph Attention Layer\n",
        "    n_heads=args['num_heads'],               # Number of attention heads in the first Graph Attention Layer\n",
        "    num_classes=labels.max().item() + 1,     # Number of classes to predict for each node\n",
        "    concat=args['concat_heads'],             # Whether to concatenate attention heads\n",
        "    dropout=args['dropout_p'],                # Dropout rate\n",
        "    leaky_relu_slope=0.2                     # Alpha (slope) of the leaky ReLU activation\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5F3wRiIyx5gu"
      },
      "source": [
        "For Inductive Learning GAT:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gBdVZoTvsSFV"
      },
      "outputs": [],
      "source": [
        "# implementation of GAT for PPI dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.utils import softmax, scatter\n",
        "import numpy as np\n",
        "\n",
        "def glorot(value, name=None):\n",
        "    \"\"\"Glorot & Bengio (AISTATS 2010) init.\"\"\"\n",
        "    init_range = np.sqrt(6.0 / (value.size(-2) + value.size(-1)))\n",
        "    initial = value.data.uniform_(-init_range, init_range)\n",
        "    return nn.Parameter(initial, requires_grad=True)\n",
        "\n",
        "class GATLayer(nn.Module):\n",
        "\n",
        "  def __init__(self,\n",
        "               D_in: int,\n",
        "               D_out: int,\n",
        "               num_heads: int = 1,\n",
        "               act=F.elu,\n",
        "               dropout: float = 0.0,\n",
        "               reduce='none',\n",
        "               skip=False):\n",
        "    super().__init__()\n",
        "    self.D_in = D_in\n",
        "    self.D_out = D_out\n",
        "    self.N_h = num_heads\n",
        "    self.act = act\n",
        "\n",
        "    self.W = nn.Parameter(torch.zeros((num_heads, D_out, D_in)))\n",
        "    self.W_skip = nn.Parameter(torch.zeros((num_heads, D_out, D_in)))\n",
        "    self.A_src = nn.Parameter(torch.zeros((num_heads, D_out, 1)))\n",
        "    self.A_tgt = nn.Parameter(torch.zeros((num_heads, D_out, 1)))\n",
        "\n",
        "    self.reduce = reduce\n",
        "    self.dropout = dropout\n",
        "    self.skip = skip\n",
        "\n",
        "    self.reset_parameters()\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    glorot(self.W)\n",
        "    glorot(self.W_skip)\n",
        "    glorot(self.A_src)\n",
        "    glorot(self.A_tgt)\n",
        "\n",
        "\n",
        "  def forward(self, H_in: torch.tensor, edge_index: torch.tensor) -> torch.tensor:\n",
        "    edge_src, edge_tgt = edge_index\n",
        "\n",
        "    N, _ = H_in.shape\n",
        "    W = self.W\n",
        "    W_skip = self.W_skip\n",
        "\n",
        "    A_src = self.A_src\n",
        "    A_tgt = self.A_tgt\n",
        "    D_in = self.D_in\n",
        "    D_out = self.D_out\n",
        "    N_h = self.N_h\n",
        "    act = self.act\n",
        "    dropout = self.dropout\n",
        "    training = self.training\n",
        "    skip = self.skip\n",
        "\n",
        "    W = W.view((N_h, D_out, D_in))\n",
        "    W_skip = W.view((N_h, D_out, D_in))\n",
        "\n",
        "    A_src = A_src.view((N_h, D_out))\n",
        "    A_tgt = A_tgt.view((N_h, D_out))\n",
        "\n",
        "    H_in = F.dropout(H_in, dropout, training)\n",
        "\n",
        "    # H_w = torch.matmul(H_in, W.transpose(1, 2)) # works too!\n",
        "    H_w = torch.matmul(H_in, W.permute(0, 2, 1)) # works too!\n",
        "\n",
        "    H_w = F.dropout(H_w, dropout, training)  # (N_h, |V|, D_out)\n",
        "\n",
        "\n",
        "    H_w_src = H_w[:, edge_src]\n",
        "    H_w_tgt = H_w[:, edge_tgt]\n",
        "\n",
        "\n",
        "    E_pre_src = torch.matmul(H_w_src, A_src.unsqueeze(-1)).squeeze()\n",
        "    E_pre_tgt = torch.matmul(H_w_tgt, A_tgt.unsqueeze(-1)).squeeze()\n",
        "\n",
        "    E_pre = E_pre_src + E_pre_tgt  # (N_h, |E|, 1), a^T [Whi || Whj]\n",
        "    E = F.leaky_relu(E_pre, negative_slope=0.2)  # (N_h, |E|, 1), LeakyRelu(a^T [Whi || Whj])\n",
        "\n",
        "    alpha_scores = softmax(E, edge_tgt, dim=1).view((N_h, *edge_src.shape, 1))  # (N_h, |E|, 1)\n",
        "\n",
        "\n",
        "    alpha_scores = F.dropout(alpha_scores, dropout, training)\n",
        "    Alpha = alpha_scores.repeat(1, 1, D_out)  # (N_h, |E|, D_out)\n",
        "\n",
        "    self.attention_scores = alpha_scores\n",
        "\n",
        "    H_out_pre = scatter(Alpha * H_w_src, edge_tgt, dim=1, reduce='sum')  # (N_h, |V|, D_out)\n",
        "\n",
        "    if skip:\n",
        "      H_skip_to_add = torch.zeros_like(H_out_pre)\n",
        "      if D_in != D_out:\n",
        "        H_skip_to_add = torch.matmul(H_in, W_skip.transpose(1, 2))\n",
        "      else:\n",
        "        H_skip_to_add = H_in.repeat(N_h, 1, 1)\n",
        "\n",
        "      H_out_pre += H_skip_to_add\n",
        "\n",
        "    if self.reduce == 'none':\n",
        "      H_out = act(H_out_pre)  # (N_h, |V|, D_out)\n",
        "      assert (H_out.shape == (N_h, N, D_out))\n",
        "      return H_out\n",
        "    elif self.reduce == 'concat':\n",
        "      H_out = act(H_out_pre)\n",
        "      H_out_per_head = torch.tensor_split(H_out, N_h)\n",
        "      H_out_cat = torch.cat(H_out_per_head, dim=-1).squeeze()\n",
        "      self.embeddings = H_out_cat\n",
        "      assert (H_out_cat.shape == (N, N_h * D_out))\n",
        "      return H_out_cat\n",
        "    else:\n",
        "      H_out_pre_avg = torch.mean(H_out_pre, dim=0)\n",
        "      H_out = act(H_out_pre_avg)\n",
        "      self.embeddings = H_out\n",
        "      assert (H_out.shape == (N, D_out))\n",
        "      return H_out\n",
        "\n",
        "\n",
        "\n",
        "class GAT_PPI(nn.Module):\n",
        "\n",
        "  def __init__(self, dim_in: int, num_classes: int):\n",
        "    super().__init__()\n",
        "    self.model_name = 'GAT_PPI'\n",
        "    self.layers = nn.ModuleList([\n",
        "        GATLayer(dim_in, 256, 4, act=F.elu, reduce='concat'),\n",
        "        GATLayer(1024, 256, 4, act=F.elu, reduce='concat', skip=True),\n",
        "        GATLayer(1024, num_classes, 6, act=nn.Identity(), reduce='avg', skip=True)\n",
        "    ])\n",
        "    self.reset_parameters()\n",
        "\n",
        "  def forward(self, X: torch.tensor, edge_index: torch.tensor) -> torch.tensor:\n",
        "    out = X\n",
        "    for layer in self.layers:\n",
        "      out = layer(out, edge_index)\n",
        "    return out\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    for layer in self.layers:\n",
        "      layer.reset_parameters()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Y0HevKBax5gv"
      },
      "outputs": [],
      "source": [
        "# evaluate function\n",
        "# set device\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "\n",
        "def evaluate(model, batch_loader):\n",
        "    # model = model.to(device)\n",
        "\n",
        "    loss_fcn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    total_score = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for (batch_id, batched_graph) in enumerate(batch_loader):\n",
        "            node_features = batched_graph.x.to(device)\n",
        "            edge_index = batched_graph.edge_index.to(device)\n",
        "            labels = batched_graph.y.to(device)\n",
        "\n",
        "            logits = model(node_features, edge_index)\n",
        "            pred = (logits >= 0).float().cpu().numpy() # torch.where(logits >= 0, 1, 0)\n",
        "            loss = loss_fcn(logits, labels)\n",
        "            score = torch.tensor(f1_score(labels.cpu().numpy(), pred, average='micro'), dtype=torch.float32, device=device) #\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_score += score.item()\n",
        "\n",
        "    avg_loss = total_loss / (batch_id + 1)\n",
        "    avg_score = total_score / (batch_id + 1)\n",
        "\n",
        "    return avg_loss, avg_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3GGWBh5__7rG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "# set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def train_one_iter(train_batch, model, loss_func, optimizer):\n",
        "    model.train()\n",
        "\n",
        "    node_features, edge_index, labels = (train_batch.x.to(device), train_batch.edge_index.to(device),\n",
        "                                         train_batch.y.to(device))\n",
        "\n",
        "    # perform training step\n",
        "    optimizer.zero_grad()\n",
        "    logits = model(node_features, edge_index)\n",
        "    pred = torch.where(logits >= 0, 1, 0)\n",
        "    loss = loss_func(logits, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    pred_np = pred.cpu().numpy()\n",
        "    labels_np = labels.cpu().numpy()\n",
        "    f1 = f1_score(labels_np, pred_np, average='micro')\n",
        "\n",
        "    return (loss.item(), torch.tensor(f1, dtype=torch.float32, device=device).item())\n",
        "\n",
        "def training_loop(model, params: dict, verbose: bool = True) -> torch.nn.Module:\n",
        "    #print('training model {}'.format(params['model_name']))\n",
        "    #print(model)\n",
        "    print('Training model...')\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
        "    model.reset_parameters()\n",
        "    batch_criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # Train model\n",
        "    t_total = time.time()\n",
        "    best_f1 = -1\n",
        "    best_loss = pow(10, 9)\n",
        "    patience = params['patience']\n",
        "    underperformed = 0\n",
        "\n",
        "    train_losses, train_scores = [], []\n",
        "    val_losses, val_scores = [], []\n",
        "\n",
        "    best_model_state_dict = None\n",
        "    best_optimizer_state_dict = None\n",
        "    best_epoch = 0\n",
        "\n",
        "    for epoch in range(params['epochs']):\n",
        "        start_t = time.time()\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        total_train_score = 0\n",
        "\n",
        "        for (batch_ix, batch) in enumerate(train_loader):\n",
        "\n",
        "            batch_loss, batch_score = train_one_iter(batch, model, batch_criterion, optimizer )\n",
        "            total_train_loss += batch_loss\n",
        "            total_train_score += batch_score\n",
        "\n",
        "        avg_train_loss = total_train_loss / (batch_ix + 1)\n",
        "        avg_train_score = total_train_score / (batch_ix + 1)\n",
        "\n",
        "        val_loss, val_score = evaluate(model, val_loader)\n",
        "\n",
        "        if verbose:\n",
        "            # print('epoch {:05d}'.format(epoch + 1))\n",
        "            # print('\\t{}_loss: {:.4f} | {}_micro_f1: {:.4f}'.format('train', avg_train_loss,'train', avg_train_score))\n",
        "            # print('\\t{}_loss: {:.4f} | {}_micro_f1: {:.4f}'.format('val', val_loss,'val', val_score))\n",
        "            print(\" | \".join([f\"Epoch: {(epoch + 1) :04d}\", f\"time: {(time.time() - start_t):.4f}s\", f\"Train loss: {avg_train_loss:.4f}\",\n",
        "                          f\"Train micro F1: {avg_train_score:.4f}\",\n",
        "                          f\"Val loss: {val_loss:.4f}\",\n",
        "                          f\"Val micro F1: {val_score:.4f}\"]))\n",
        "\n",
        "        train_losses.append(avg_train_loss)\n",
        "        train_scores.append(avg_train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        val_scores.append(val_score)\n",
        "\n",
        "        if (val_score > best_f1 or val_loss < best_loss):\n",
        "            best_f1 = max(best_f1, val_score)\n",
        "            best_loss = min(best_loss, val_loss)\n",
        "            underperformed = 0\n",
        "            was_best_so_far = True\n",
        "            best_model_state_dict = model.state_dict()\n",
        "            best_optimizer_state_dict = optimizer.state_dict()\n",
        "            best_epoch = epoch\n",
        "        else:\n",
        "            underperformed += 1\n",
        "            was_best_so_far = False\n",
        "\n",
        "        if underperformed >= patience:\n",
        "            print('Early stopping condition met. Stopped at Epoch {:04d}: '.format(epoch + 1))\n",
        "            break\n",
        "\n",
        "        if was_best_so_far:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': best_model_state_dict,\n",
        "                'optimizer_state_dict': best_optimizer_state_dict,\n",
        "            }, params['model_name'])\n",
        "\n",
        "\n",
        "    print()\n",
        "    #print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "    print(f\"Total time elapsed: {(time.time() - t_total):.4f}s  |  Average time elapsed: {((time.time() - t_total)/params['epochs']):.4f}s\")\n",
        "\n",
        "    state = torch.load(params['model_name'])\n",
        "    model.load_state_dict(state['model_state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    best_epoch, best_model = state['epoch'], model\n",
        "    best_val_loss, best_val_score = evaluate(model, val_loader)\n",
        "    best_test_loss, best_test_score = evaluate(model, test_loader)\n",
        "\n",
        "    print()\n",
        "    print('Best model performance @ Epoch {:04d}: '.format(best_epoch + 1))\n",
        "    print(f\"Test loss: {best_test_loss:.4f}  |  Test micro F1: {best_test_score:.4f}\")\n",
        "    #print('\\t{}_loss: {:.4f} | {}_micro_f1: {:.4f}'.format('val', best_val_loss, 'val', best_val_score))\n",
        "    #print('\\t{}_loss: {:.4f} | {}_micro_f1: {:.4f}'.format('test', best_test_loss,'test', best_test_score))\n",
        "\n",
        "    return best_model, train_losses, train_scores, val_losses, val_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMAxHvGfx5gw",
        "outputId": "513b1622-1329-4195-c789-5d4cce381267"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7e3a840b8550>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import torch\n",
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "oIIDw3ljx5gw"
      },
      "outputs": [],
      "source": [
        "model = GAT_PPI(num_features, num_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tkKx8aZx5gx"
      },
      "source": [
        "##   Training\n",
        "\n",
        "###  Computational requirements\n",
        "For transductive learning:\n",
        "* type of hardware: CPU / GPU\n",
        "* GPU hrs used: ~3 GPU hrs\n",
        "* avg runtime for each epoch : ~0.07 sec for GPU ~7 sec for CPU (still need to test/verify)\n",
        "* number of training epochs : 200 for citeseer dataset, 300 for cora dataset\n",
        "* total number of trial: 10\n",
        "\n",
        "For inductive learning:\n",
        "* type of hardware: CPU / GPU\n",
        "* GPU hrs used: ~12 GPU hrs\n",
        "* avg runtime for each epoch : ~3.7 sec for GPU, ~7 min for CPU (still need to test/verify)\n",
        "* number of training epochs : 200 for PPI dataset\n",
        "* total number of trial: 10\n",
        "\n",
        "\n",
        "\n",
        "###  Hyperparams\n",
        "For transductive learning:\n",
        " * learning rate = 0.005\n",
        " * No batch size used\n",
        " * dropout p = 0.6\n",
        " * L2 regularization(weight decay) lambda = 0.0005\n",
        "\n",
        "     \n",
        "For inductive learning:\n",
        " * learning rate = 0.005\n",
        " * batch size = 2\n",
        " * dropout p = 0.6\n",
        " * L2 regularization(weight decay) = 0 (No L2 used)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_Cf-FDSx5gx"
      },
      "source": [
        "###  Implementation code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62VN1mFq1Ia7"
      },
      "source": [
        "For transductive learning GAT:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwhETYa7w1Of",
        "outputId": "f215070d-a5e2-40d8-a9cf-70f3c589adc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "# set the epochs to 1 to test train_and_evaluate function\n",
        "import torch\n",
        "import os\n",
        "import requests\n",
        "import tarfile\n",
        "import glob\n",
        "# Set the parameters directly\n",
        "args = {\n",
        "    'seed': 42,  # 9\n",
        "    'no_cuda': False,\n",
        "    'no_mps': False,\n",
        "    'hidden_dim': 64,\n",
        "    'num_heads': 8,\n",
        "    'concat_heads': False,\n",
        "    'dropout_p': 0.6,\n",
        "    'lr': 0.005,\n",
        "    'l2': 5e-4,\n",
        "    'epochs': 1,   # 300\n",
        "    'patience': 100,\n",
        "}\n",
        "\n",
        "torch.manual_seed(args['seed'])\n",
        "use_cuda = not args['no_cuda'] and torch.cuda.is_available()\n",
        "use_mps = not args['no_mps'] and torch.backends.mps.is_available()\n",
        "\n",
        "# Set the device to run on\n",
        "if use_cuda:\n",
        "    device = torch.device('cuda')\n",
        "elif use_mps:\n",
        "    device = torch.device('mps')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "print(f'Using {device} device')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7al34qsTw95J",
        "outputId": "9a622ce0-83cd-498a-a899-1b95a0b99255"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0001 (8.8123s) loss_train: 1.9420 acc_train: 0.4667 loss_val: 1.9440 acc_val: 0.3160\n",
            "Optimization Finished!\n",
            "Total time elapsed: 14.3566s\n",
            "Loading 1th epoch\n",
            "Test set results: loss 1.9437 accuracy 0.3530\n"
          ]
        }
      ],
      "source": [
        "# set the epochs to 1 to test train_and_evaluate function\n",
        "best_model, loss_values, acc_values, loss_test, acc_test = train_and_evaluate(GAT_cora, (features, adj_mat), labels, idx_train, idx_val, args['epochs'], args['patience'], args['print'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2asZcR5C_Peh",
        "outputId": "afe4eec4-2c25-4b7b-9810-3f49fe52ad04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "# demonstrate train_and_evaluate function on cora\n",
        "import torch\n",
        "import os\n",
        "import requests\n",
        "import tarfile\n",
        "import glob\n",
        "# Set the parameters directly\n",
        "args = {\n",
        "    'seed': 42,  # 9\n",
        "    'no_cuda': False,\n",
        "    'no_mps': False,\n",
        "    'hidden_dim': 64,\n",
        "    'num_heads': 8,\n",
        "    'concat_heads': False,\n",
        "    'dropout_p': 0.6,\n",
        "    'lr': 0.005,\n",
        "    'l2': 5e-4,\n",
        "    'epochs': 300,\n",
        "    'patience': 100,\n",
        "    'print': True,\n",
        "}\n",
        "\n",
        "torch.manual_seed(args['seed'])\n",
        "use_cuda = not args['no_cuda'] and torch.cuda.is_available()\n",
        "use_mps = not args['no_mps'] and torch.backends.mps.is_available()\n",
        "\n",
        "# Set the device to run on\n",
        "if use_cuda:\n",
        "    device = torch.device('cuda')\n",
        "elif use_mps:\n",
        "    device = torch.device('mps')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "print(f'Using {device} device')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qg12BtSX8qHh",
        "outputId": "10f0e417-1cac-488e-d544-6b7b564123d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0001 (0.0908s) loss_train: 1.4414 acc_train: 0.8962 loss_val: 1.5312 acc_val: 0.7650\n",
            "Epoch: 0002 (0.0908s) loss_train: 1.4451 acc_train: 0.8935 loss_val: 1.5328 acc_val: 0.7625\n",
            "Epoch: 0003 (0.0864s) loss_train: 1.4458 acc_train: 0.8890 loss_val: 1.5319 acc_val: 0.7625\n",
            "Epoch: 0004 (0.0761s) loss_train: 1.4454 acc_train: 0.8863 loss_val: 1.5302 acc_val: 0.7575\n",
            "Epoch: 0005 (0.0766s) loss_train: 1.4459 acc_train: 0.8863 loss_val: 1.5299 acc_val: 0.7550\n",
            "Epoch: 0006 (0.0746s) loss_train: 1.4455 acc_train: 0.8863 loss_val: 1.5292 acc_val: 0.7550\n",
            "Epoch: 0007 (0.0746s) loss_train: 1.4452 acc_train: 0.8818 loss_val: 1.5284 acc_val: 0.7525\n",
            "Epoch: 0008 (0.0745s) loss_train: 1.4454 acc_train: 0.8836 loss_val: 1.5281 acc_val: 0.7525\n",
            "Epoch: 0009 (0.0748s) loss_train: 1.4450 acc_train: 0.8836 loss_val: 1.5275 acc_val: 0.7475\n",
            "Epoch: 0010 (0.0747s) loss_train: 1.4452 acc_train: 0.8890 loss_val: 1.5278 acc_val: 0.7575\n",
            "Epoch: 0011 (0.0755s) loss_train: 1.4446 acc_train: 0.8899 loss_val: 1.5275 acc_val: 0.7650\n",
            "Epoch: 0012 (0.0747s) loss_train: 1.4439 acc_train: 0.8908 loss_val: 1.5271 acc_val: 0.7625\n",
            "Epoch: 0013 (0.0770s) loss_train: 1.4430 acc_train: 0.8926 loss_val: 1.5266 acc_val: 0.7575\n",
            "Epoch: 0014 (0.0757s) loss_train: 1.4420 acc_train: 0.8899 loss_val: 1.5262 acc_val: 0.7575\n",
            "Epoch: 0015 (0.0745s) loss_train: 1.4410 acc_train: 0.8899 loss_val: 1.5257 acc_val: 0.7575\n",
            "Epoch: 0016 (0.0750s) loss_train: 1.4397 acc_train: 0.8899 loss_val: 1.5247 acc_val: 0.7600\n",
            "Epoch: 0017 (0.0746s) loss_train: 1.4388 acc_train: 0.8881 loss_val: 1.5242 acc_val: 0.7575\n",
            "Epoch: 0018 (0.0747s) loss_train: 1.4383 acc_train: 0.8908 loss_val: 1.5239 acc_val: 0.7575\n",
            "Epoch: 0019 (0.0746s) loss_train: 1.4380 acc_train: 0.8926 loss_val: 1.5239 acc_val: 0.7600\n",
            "Epoch: 0020 (0.0747s) loss_train: 1.4377 acc_train: 0.8935 loss_val: 1.5239 acc_val: 0.7625\n",
            "Epoch: 0021 (0.0753s) loss_train: 1.4371 acc_train: 0.8944 loss_val: 1.5236 acc_val: 0.7625\n",
            "Epoch: 0022 (0.0746s) loss_train: 1.4368 acc_train: 0.8953 loss_val: 1.5234 acc_val: 0.7625\n",
            "Epoch: 0023 (0.0748s) loss_train: 1.4362 acc_train: 0.8953 loss_val: 1.5229 acc_val: 0.7625\n",
            "Epoch: 0024 (0.0748s) loss_train: 1.4359 acc_train: 0.8980 loss_val: 1.5229 acc_val: 0.7625\n",
            "Epoch: 0025 (0.0748s) loss_train: 1.4357 acc_train: 0.8971 loss_val: 1.5229 acc_val: 0.7675\n",
            "Epoch: 0026 (0.0751s) loss_train: 1.4353 acc_train: 0.8989 loss_val: 1.5227 acc_val: 0.7700\n",
            "Epoch: 0027 (0.0748s) loss_train: 1.4352 acc_train: 0.9025 loss_val: 1.5227 acc_val: 0.7700\n",
            "Epoch: 0028 (0.0749s) loss_train: 1.4348 acc_train: 0.9043 loss_val: 1.5222 acc_val: 0.7775\n",
            "Epoch: 0029 (0.0755s) loss_train: 1.4345 acc_train: 0.9070 loss_val: 1.5219 acc_val: 0.7725\n",
            "Epoch: 0030 (0.0749s) loss_train: 1.4343 acc_train: 0.9079 loss_val: 1.5217 acc_val: 0.7725\n",
            "Epoch: 0031 (0.0754s) loss_train: 1.4341 acc_train: 0.9116 loss_val: 1.5217 acc_val: 0.7750\n",
            "Epoch: 0032 (0.0757s) loss_train: 1.4337 acc_train: 0.9097 loss_val: 1.5214 acc_val: 0.7750\n",
            "Epoch: 0033 (0.0747s) loss_train: 1.4328 acc_train: 0.9116 loss_val: 1.5206 acc_val: 0.7725\n",
            "Epoch: 0034 (0.0748s) loss_train: 1.4316 acc_train: 0.9116 loss_val: 1.5196 acc_val: 0.7750\n",
            "Epoch: 0035 (0.0747s) loss_train: 1.4304 acc_train: 0.9088 loss_val: 1.5186 acc_val: 0.7775\n",
            "Epoch: 0036 (0.0747s) loss_train: 1.4292 acc_train: 0.9097 loss_val: 1.5175 acc_val: 0.7775\n",
            "Epoch: 0037 (0.0749s) loss_train: 1.4279 acc_train: 0.9088 loss_val: 1.5165 acc_val: 0.7800\n",
            "Epoch: 0038 (0.0747s) loss_train: 1.4264 acc_train: 0.9052 loss_val: 1.5154 acc_val: 0.7775\n",
            "Epoch: 0039 (0.0748s) loss_train: 1.4248 acc_train: 0.9043 loss_val: 1.5141 acc_val: 0.7725\n",
            "Epoch: 0040 (0.0747s) loss_train: 1.4228 acc_train: 0.9043 loss_val: 1.5126 acc_val: 0.7700\n",
            "Epoch: 0041 (0.0747s) loss_train: 1.4212 acc_train: 0.9052 loss_val: 1.5113 acc_val: 0.7700\n",
            "Epoch: 0042 (0.0747s) loss_train: 1.4196 acc_train: 0.9043 loss_val: 1.5100 acc_val: 0.7700\n",
            "Epoch: 0043 (0.0748s) loss_train: 1.4184 acc_train: 0.9043 loss_val: 1.5089 acc_val: 0.7700\n",
            "Epoch: 0044 (0.0747s) loss_train: 1.4179 acc_train: 0.9043 loss_val: 1.5083 acc_val: 0.7725\n",
            "Epoch: 0045 (0.0749s) loss_train: 1.4175 acc_train: 0.9061 loss_val: 1.5079 acc_val: 0.7725\n",
            "Epoch: 0046 (0.0748s) loss_train: 1.4177 acc_train: 0.9079 loss_val: 1.5079 acc_val: 0.7700\n",
            "Epoch: 0047 (0.0759s) loss_train: 1.4180 acc_train: 0.9097 loss_val: 1.5083 acc_val: 0.7725\n",
            "Epoch: 0048 (0.0752s) loss_train: 1.4183 acc_train: 0.9134 loss_val: 1.5087 acc_val: 0.7750\n",
            "Epoch: 0049 (0.0751s) loss_train: 1.4189 acc_train: 0.9125 loss_val: 1.5095 acc_val: 0.7750\n",
            "Epoch: 0050 (0.0752s) loss_train: 1.4198 acc_train: 0.9143 loss_val: 1.5105 acc_val: 0.7775\n",
            "Epoch: 0051 (0.0754s) loss_train: 1.4209 acc_train: 0.9152 loss_val: 1.5119 acc_val: 0.7750\n",
            "Epoch: 0052 (0.0759s) loss_train: 1.4213 acc_train: 0.9170 loss_val: 1.5125 acc_val: 0.7775\n",
            "Epoch: 0053 (0.0749s) loss_train: 1.4217 acc_train: 0.9179 loss_val: 1.5130 acc_val: 0.7800\n",
            "Epoch: 0054 (0.0754s) loss_train: 1.4218 acc_train: 0.9170 loss_val: 1.5134 acc_val: 0.7775\n",
            "Epoch: 0055 (0.0778s) loss_train: 1.4217 acc_train: 0.9152 loss_val: 1.5135 acc_val: 0.7750\n",
            "Epoch: 0056 (0.0754s) loss_train: 1.4217 acc_train: 0.9161 loss_val: 1.5136 acc_val: 0.7750\n",
            "Epoch: 0057 (0.0756s) loss_train: 1.4214 acc_train: 0.9152 loss_val: 1.5137 acc_val: 0.7725\n",
            "Epoch: 0058 (0.0760s) loss_train: 1.4209 acc_train: 0.9170 loss_val: 1.5135 acc_val: 0.7725\n",
            "Epoch: 0059 (0.0753s) loss_train: 1.4199 acc_train: 0.9161 loss_val: 1.5128 acc_val: 0.7725\n",
            "Epoch: 0060 (0.0756s) loss_train: 1.4189 acc_train: 0.9161 loss_val: 1.5121 acc_val: 0.7750\n",
            "Epoch: 0061 (0.0756s) loss_train: 1.4182 acc_train: 0.9170 loss_val: 1.5116 acc_val: 0.7725\n",
            "Epoch: 0062 (0.0754s) loss_train: 1.4176 acc_train: 0.9179 loss_val: 1.5112 acc_val: 0.7725\n",
            "Epoch: 0063 (0.0752s) loss_train: 1.4171 acc_train: 0.9206 loss_val: 1.5108 acc_val: 0.7700\n",
            "Epoch: 0064 (0.0755s) loss_train: 1.4168 acc_train: 0.9224 loss_val: 1.5105 acc_val: 0.7725\n",
            "Epoch: 0065 (0.0755s) loss_train: 1.4161 acc_train: 0.9233 loss_val: 1.5098 acc_val: 0.7725\n",
            "Epoch: 0066 (0.0753s) loss_train: 1.4162 acc_train: 0.9215 loss_val: 1.5099 acc_val: 0.7750\n",
            "Epoch: 0067 (0.0759s) loss_train: 1.4160 acc_train: 0.9188 loss_val: 1.5097 acc_val: 0.7750\n",
            "Epoch: 0068 (0.0755s) loss_train: 1.4159 acc_train: 0.9170 loss_val: 1.5095 acc_val: 0.7775\n",
            "Epoch: 0069 (0.0769s) loss_train: 1.4155 acc_train: 0.9188 loss_val: 1.5090 acc_val: 0.7800\n",
            "Epoch: 0070 (0.0752s) loss_train: 1.4153 acc_train: 0.9197 loss_val: 1.5087 acc_val: 0.7800\n",
            "Epoch: 0071 (0.0761s) loss_train: 1.4155 acc_train: 0.9188 loss_val: 1.5085 acc_val: 0.7800\n",
            "Epoch: 0072 (0.0753s) loss_train: 1.4156 acc_train: 0.9179 loss_val: 1.5084 acc_val: 0.7800\n",
            "Epoch: 0073 (0.0760s) loss_train: 1.4160 acc_train: 0.9161 loss_val: 1.5085 acc_val: 0.7825\n",
            "Epoch: 0074 (0.0747s) loss_train: 1.4161 acc_train: 0.9179 loss_val: 1.5085 acc_val: 0.7825\n",
            "Epoch: 0075 (0.0759s) loss_train: 1.4163 acc_train: 0.9179 loss_val: 1.5087 acc_val: 0.7825\n",
            "Epoch: 0076 (0.0755s) loss_train: 1.4167 acc_train: 0.9188 loss_val: 1.5089 acc_val: 0.7825\n",
            "Epoch: 0077 (0.0758s) loss_train: 1.4167 acc_train: 0.9179 loss_val: 1.5089 acc_val: 0.7800\n",
            "Epoch: 0078 (0.0756s) loss_train: 1.4167 acc_train: 0.9188 loss_val: 1.5087 acc_val: 0.7850\n",
            "Epoch: 0079 (0.0747s) loss_train: 1.4161 acc_train: 0.9215 loss_val: 1.5083 acc_val: 0.7850\n",
            "Epoch: 0080 (0.0759s) loss_train: 1.4159 acc_train: 0.9215 loss_val: 1.5081 acc_val: 0.7825\n",
            "Epoch: 0081 (0.0761s) loss_train: 1.4156 acc_train: 0.9215 loss_val: 1.5076 acc_val: 0.7850\n",
            "Epoch: 0082 (0.0746s) loss_train: 1.4153 acc_train: 0.9197 loss_val: 1.5073 acc_val: 0.7850\n",
            "Epoch: 0083 (0.0746s) loss_train: 1.4149 acc_train: 0.9188 loss_val: 1.5068 acc_val: 0.7875\n",
            "Epoch: 0084 (0.0748s) loss_train: 1.4151 acc_train: 0.9206 loss_val: 1.5070 acc_val: 0.7900\n",
            "Epoch: 0085 (0.0747s) loss_train: 1.4153 acc_train: 0.9188 loss_val: 1.5073 acc_val: 0.7900\n",
            "Epoch: 0086 (0.0759s) loss_train: 1.4152 acc_train: 0.9206 loss_val: 1.5074 acc_val: 0.7900\n",
            "Epoch: 0087 (0.0755s) loss_train: 1.4152 acc_train: 0.9215 loss_val: 1.5076 acc_val: 0.7925\n",
            "Epoch: 0088 (0.0747s) loss_train: 1.4149 acc_train: 0.9224 loss_val: 1.5075 acc_val: 0.7875\n",
            "Epoch: 0089 (0.0751s) loss_train: 1.4142 acc_train: 0.9215 loss_val: 1.5071 acc_val: 0.7875\n",
            "Epoch: 0090 (0.0752s) loss_train: 1.4137 acc_train: 0.9188 loss_val: 1.5069 acc_val: 0.7850\n",
            "Epoch: 0091 (0.0759s) loss_train: 1.4129 acc_train: 0.9197 loss_val: 1.5064 acc_val: 0.7875\n",
            "Epoch: 0092 (0.0751s) loss_train: 1.4124 acc_train: 0.9206 loss_val: 1.5060 acc_val: 0.7850\n",
            "Epoch: 0093 (0.0751s) loss_train: 1.4118 acc_train: 0.9215 loss_val: 1.5055 acc_val: 0.7825\n",
            "Epoch: 0094 (0.0748s) loss_train: 1.4116 acc_train: 0.9206 loss_val: 1.5053 acc_val: 0.7850\n",
            "Epoch: 0095 (0.0758s) loss_train: 1.4113 acc_train: 0.9179 loss_val: 1.5049 acc_val: 0.7850\n",
            "Epoch: 0096 (0.0748s) loss_train: 1.4112 acc_train: 0.9179 loss_val: 1.5047 acc_val: 0.7850\n",
            "Epoch: 0097 (0.0749s) loss_train: 1.4111 acc_train: 0.9152 loss_val: 1.5044 acc_val: 0.7850\n",
            "Epoch: 0098 (0.0747s) loss_train: 1.4111 acc_train: 0.9116 loss_val: 1.5044 acc_val: 0.7825\n",
            "Epoch: 0099 (0.0752s) loss_train: 1.4107 acc_train: 0.9125 loss_val: 1.5041 acc_val: 0.7825\n",
            "Epoch: 0100 (0.0759s) loss_train: 1.4109 acc_train: 0.9116 loss_val: 1.5046 acc_val: 0.7800\n",
            "Epoch: 0101 (0.0760s) loss_train: 1.4108 acc_train: 0.9106 loss_val: 1.5049 acc_val: 0.7775\n",
            "Epoch: 0102 (0.0758s) loss_train: 1.4108 acc_train: 0.9116 loss_val: 1.5052 acc_val: 0.7750\n",
            "Epoch: 0103 (0.0758s) loss_train: 1.4107 acc_train: 0.9097 loss_val: 1.5054 acc_val: 0.7800\n",
            "Epoch: 0104 (0.0760s) loss_train: 1.4110 acc_train: 0.9106 loss_val: 1.5058 acc_val: 0.7850\n",
            "Epoch: 0105 (0.0755s) loss_train: 1.4114 acc_train: 0.9116 loss_val: 1.5063 acc_val: 0.7775\n",
            "Epoch: 0106 (0.0757s) loss_train: 1.4120 acc_train: 0.9125 loss_val: 1.5069 acc_val: 0.7800\n",
            "Epoch: 0107 (0.0759s) loss_train: 1.4128 acc_train: 0.9152 loss_val: 1.5078 acc_val: 0.7825\n",
            "Epoch: 0108 (0.0760s) loss_train: 1.4137 acc_train: 0.9170 loss_val: 1.5090 acc_val: 0.7800\n",
            "Epoch: 0109 (0.0754s) loss_train: 1.4147 acc_train: 0.9170 loss_val: 1.5101 acc_val: 0.7825\n",
            "Epoch: 0110 (0.0761s) loss_train: 1.4154 acc_train: 0.9188 loss_val: 1.5112 acc_val: 0.7850\n",
            "Epoch: 0111 (0.0776s) loss_train: 1.4156 acc_train: 0.9170 loss_val: 1.5116 acc_val: 0.7825\n",
            "Epoch: 0112 (0.0769s) loss_train: 1.4155 acc_train: 0.9143 loss_val: 1.5116 acc_val: 0.7850\n",
            "Epoch: 0113 (0.0759s) loss_train: 1.4151 acc_train: 0.9152 loss_val: 1.5113 acc_val: 0.7825\n",
            "Epoch: 0114 (0.0758s) loss_train: 1.4144 acc_train: 0.9161 loss_val: 1.5108 acc_val: 0.7825\n",
            "Epoch: 0115 (0.0758s) loss_train: 1.4134 acc_train: 0.9170 loss_val: 1.5099 acc_val: 0.7800\n",
            "Epoch: 0116 (0.0761s) loss_train: 1.4125 acc_train: 0.9170 loss_val: 1.5090 acc_val: 0.7825\n",
            "Epoch: 0117 (0.0759s) loss_train: 1.4117 acc_train: 0.9170 loss_val: 1.5082 acc_val: 0.7800\n",
            "Epoch: 0118 (0.0759s) loss_train: 1.4105 acc_train: 0.9179 loss_val: 1.5071 acc_val: 0.7800\n",
            "Epoch: 0119 (0.0768s) loss_train: 1.4093 acc_train: 0.9197 loss_val: 1.5060 acc_val: 0.7800\n",
            "Epoch: 0120 (0.0754s) loss_train: 1.4080 acc_train: 0.9197 loss_val: 1.5045 acc_val: 0.7800\n",
            "Epoch: 0121 (0.0762s) loss_train: 1.4067 acc_train: 0.9197 loss_val: 1.5033 acc_val: 0.7850\n",
            "Epoch: 0122 (0.0750s) loss_train: 1.4060 acc_train: 0.9197 loss_val: 1.5025 acc_val: 0.7800\n",
            "Epoch: 0123 (0.0749s) loss_train: 1.4058 acc_train: 0.9197 loss_val: 1.5020 acc_val: 0.7825\n",
            "Epoch: 0124 (0.0751s) loss_train: 1.4061 acc_train: 0.9197 loss_val: 1.5018 acc_val: 0.7825\n",
            "Epoch: 0125 (0.0749s) loss_train: 1.4062 acc_train: 0.9224 loss_val: 1.5018 acc_val: 0.7900\n",
            "Epoch: 0126 (0.0749s) loss_train: 1.4066 acc_train: 0.9206 loss_val: 1.5019 acc_val: 0.7900\n",
            "Epoch: 0127 (0.0758s) loss_train: 1.4073 acc_train: 0.9197 loss_val: 1.5022 acc_val: 0.7875\n",
            "Epoch: 0128 (0.0759s) loss_train: 1.4079 acc_train: 0.9197 loss_val: 1.5025 acc_val: 0.7875\n",
            "Epoch: 0129 (0.0762s) loss_train: 1.4080 acc_train: 0.9224 loss_val: 1.5024 acc_val: 0.7850\n",
            "Epoch: 0130 (0.0760s) loss_train: 1.4080 acc_train: 0.9224 loss_val: 1.5020 acc_val: 0.7875\n",
            "Epoch: 0131 (0.0760s) loss_train: 1.4079 acc_train: 0.9215 loss_val: 1.5016 acc_val: 0.7875\n",
            "Epoch: 0132 (0.0749s) loss_train: 1.4078 acc_train: 0.9233 loss_val: 1.5013 acc_val: 0.7875\n",
            "Epoch: 0133 (0.0753s) loss_train: 1.4077 acc_train: 0.9224 loss_val: 1.5011 acc_val: 0.7875\n",
            "Epoch: 0134 (0.0749s) loss_train: 1.4079 acc_train: 0.9233 loss_val: 1.5011 acc_val: 0.7825\n",
            "Epoch: 0135 (0.0759s) loss_train: 1.4078 acc_train: 0.9242 loss_val: 1.5009 acc_val: 0.7875\n",
            "Epoch: 0136 (0.0757s) loss_train: 1.4074 acc_train: 0.9251 loss_val: 1.5006 acc_val: 0.7850\n",
            "Epoch: 0137 (0.0749s) loss_train: 1.4069 acc_train: 0.9224 loss_val: 1.5002 acc_val: 0.7925\n",
            "Epoch: 0138 (0.0749s) loss_train: 1.4066 acc_train: 0.9215 loss_val: 1.5000 acc_val: 0.7925\n",
            "Epoch: 0139 (0.0749s) loss_train: 1.4064 acc_train: 0.9215 loss_val: 1.5000 acc_val: 0.7925\n",
            "Epoch: 0140 (0.0748s) loss_train: 1.4063 acc_train: 0.9206 loss_val: 1.5000 acc_val: 0.7900\n",
            "Epoch: 0141 (0.0752s) loss_train: 1.4059 acc_train: 0.9188 loss_val: 1.4998 acc_val: 0.7950\n",
            "Epoch: 0142 (0.0749s) loss_train: 1.4053 acc_train: 0.9188 loss_val: 1.4997 acc_val: 0.7950\n",
            "Epoch: 0143 (0.0749s) loss_train: 1.4050 acc_train: 0.9188 loss_val: 1.4997 acc_val: 0.7950\n",
            "Epoch: 0144 (0.0750s) loss_train: 1.4046 acc_train: 0.9179 loss_val: 1.4997 acc_val: 0.7950\n",
            "Epoch: 0145 (0.0758s) loss_train: 1.4044 acc_train: 0.9188 loss_val: 1.4999 acc_val: 0.7925\n",
            "Epoch: 0146 (0.0759s) loss_train: 1.4043 acc_train: 0.9179 loss_val: 1.5001 acc_val: 0.7925\n",
            "Epoch: 0147 (0.0771s) loss_train: 1.4041 acc_train: 0.9197 loss_val: 1.5005 acc_val: 0.7925\n",
            "Epoch: 0148 (0.0762s) loss_train: 1.4041 acc_train: 0.9206 loss_val: 1.5012 acc_val: 0.7850\n",
            "Epoch: 0149 (0.0761s) loss_train: 1.4039 acc_train: 0.9197 loss_val: 1.5017 acc_val: 0.7800\n",
            "Epoch: 0150 (0.0763s) loss_train: 1.4037 acc_train: 0.9206 loss_val: 1.5020 acc_val: 0.7800\n",
            "Epoch: 0151 (0.0764s) loss_train: 1.4037 acc_train: 0.9206 loss_val: 1.5023 acc_val: 0.7800\n",
            "Epoch: 0152 (0.0764s) loss_train: 1.4037 acc_train: 0.9224 loss_val: 1.5027 acc_val: 0.7825\n",
            "Epoch: 0153 (0.0768s) loss_train: 1.4042 acc_train: 0.9224 loss_val: 1.5035 acc_val: 0.7875\n",
            "Epoch: 0154 (0.0765s) loss_train: 1.4049 acc_train: 0.9233 loss_val: 1.5045 acc_val: 0.7850\n",
            "Epoch: 0155 (0.0765s) loss_train: 1.4057 acc_train: 0.9233 loss_val: 1.5054 acc_val: 0.7850\n",
            "Epoch: 0156 (0.0771s) loss_train: 1.4061 acc_train: 0.9224 loss_val: 1.5057 acc_val: 0.7875\n",
            "Epoch: 0157 (0.0763s) loss_train: 1.4064 acc_train: 0.9215 loss_val: 1.5059 acc_val: 0.7875\n",
            "Epoch: 0158 (0.0759s) loss_train: 1.4065 acc_train: 0.9233 loss_val: 1.5058 acc_val: 0.7850\n",
            "Epoch: 0159 (0.0760s) loss_train: 1.4064 acc_train: 0.9197 loss_val: 1.5055 acc_val: 0.7850\n",
            "Epoch: 0160 (0.0759s) loss_train: 1.4064 acc_train: 0.9215 loss_val: 1.5053 acc_val: 0.7850\n",
            "Epoch: 0161 (0.0759s) loss_train: 1.4067 acc_train: 0.9224 loss_val: 1.5054 acc_val: 0.7800\n",
            "Epoch: 0162 (0.0761s) loss_train: 1.4071 acc_train: 0.9233 loss_val: 1.5055 acc_val: 0.7800\n",
            "Epoch: 0163 (0.0758s) loss_train: 1.4075 acc_train: 0.9242 loss_val: 1.5057 acc_val: 0.7850\n",
            "Epoch: 0164 (0.0761s) loss_train: 1.4077 acc_train: 0.9242 loss_val: 1.5058 acc_val: 0.7850\n",
            "Epoch: 0165 (0.0765s) loss_train: 1.4073 acc_train: 0.9251 loss_val: 1.5054 acc_val: 0.7850\n",
            "Epoch: 0166 (0.0772s) loss_train: 1.4072 acc_train: 0.9251 loss_val: 1.5051 acc_val: 0.7825\n",
            "Epoch: 0167 (0.0766s) loss_train: 1.4066 acc_train: 0.9242 loss_val: 1.5046 acc_val: 0.7825\n",
            "Epoch: 0168 (0.0763s) loss_train: 1.4062 acc_train: 0.9242 loss_val: 1.5043 acc_val: 0.7775\n",
            "Epoch: 0169 (0.0762s) loss_train: 1.4067 acc_train: 0.9215 loss_val: 1.5046 acc_val: 0.7800\n",
            "Epoch: 0170 (0.0767s) loss_train: 1.4073 acc_train: 0.9215 loss_val: 1.5050 acc_val: 0.7775\n",
            "Epoch: 0171 (0.0765s) loss_train: 1.4077 acc_train: 0.9206 loss_val: 1.5052 acc_val: 0.7800\n",
            "Epoch: 0172 (0.0757s) loss_train: 1.4081 acc_train: 0.9197 loss_val: 1.5057 acc_val: 0.7775\n",
            "Epoch: 0173 (0.0760s) loss_train: 1.4082 acc_train: 0.9197 loss_val: 1.5059 acc_val: 0.7700\n",
            "Epoch: 0174 (0.0760s) loss_train: 1.4081 acc_train: 0.9215 loss_val: 1.5058 acc_val: 0.7700\n",
            "Epoch: 0175 (0.0762s) loss_train: 1.4074 acc_train: 0.9224 loss_val: 1.5052 acc_val: 0.7725\n",
            "Epoch: 0176 (0.0758s) loss_train: 1.4068 acc_train: 0.9224 loss_val: 1.5047 acc_val: 0.7725\n",
            "Epoch: 0177 (0.0761s) loss_train: 1.4062 acc_train: 0.9224 loss_val: 1.5040 acc_val: 0.7725\n",
            "Epoch: 0178 (0.0763s) loss_train: 1.4055 acc_train: 0.9215 loss_val: 1.5034 acc_val: 0.7750\n",
            "Epoch: 0179 (0.0774s) loss_train: 1.4048 acc_train: 0.9188 loss_val: 1.5027 acc_val: 0.7750\n",
            "Epoch: 0180 (0.0763s) loss_train: 1.4045 acc_train: 0.9188 loss_val: 1.5026 acc_val: 0.7775\n",
            "Epoch: 0181 (0.0763s) loss_train: 1.4041 acc_train: 0.9188 loss_val: 1.5023 acc_val: 0.7800\n",
            "Epoch: 0182 (0.0756s) loss_train: 1.4042 acc_train: 0.9197 loss_val: 1.5025 acc_val: 0.7775\n",
            "Epoch: 0183 (0.0767s) loss_train: 1.4045 acc_train: 0.9170 loss_val: 1.5028 acc_val: 0.7750\n",
            "Epoch: 0184 (0.0763s) loss_train: 1.4048 acc_train: 0.9170 loss_val: 1.5033 acc_val: 0.7750\n",
            "Epoch: 0185 (0.0759s) loss_train: 1.4053 acc_train: 0.9143 loss_val: 1.5040 acc_val: 0.7725\n",
            "Epoch: 0186 (0.0753s) loss_train: 1.4059 acc_train: 0.9152 loss_val: 1.5049 acc_val: 0.7725\n",
            "Epoch: 0187 (0.0760s) loss_train: 1.4063 acc_train: 0.9161 loss_val: 1.5057 acc_val: 0.7725\n",
            "Epoch: 0188 (0.0759s) loss_train: 1.4066 acc_train: 0.9134 loss_val: 1.5061 acc_val: 0.7650\n",
            "Epoch: 0189 (0.0755s) loss_train: 1.4069 acc_train: 0.9134 loss_val: 1.5064 acc_val: 0.7675\n",
            "Epoch: 0190 (0.0754s) loss_train: 1.4075 acc_train: 0.9152 loss_val: 1.5069 acc_val: 0.7725\n",
            "Epoch: 0191 (0.0765s) loss_train: 1.4083 acc_train: 0.9170 loss_val: 1.5074 acc_val: 0.7750\n",
            "Epoch: 0192 (0.0761s) loss_train: 1.4093 acc_train: 0.9206 loss_val: 1.5079 acc_val: 0.7775\n",
            "Epoch: 0193 (0.0766s) loss_train: 1.4093 acc_train: 0.9215 loss_val: 1.5077 acc_val: 0.7800\n",
            "Epoch: 0194 (0.0762s) loss_train: 1.4087 acc_train: 0.9215 loss_val: 1.5068 acc_val: 0.7825\n",
            "Epoch: 0195 (0.0755s) loss_train: 1.4080 acc_train: 0.9224 loss_val: 1.5058 acc_val: 0.7800\n",
            "Epoch: 0196 (0.0767s) loss_train: 1.4073 acc_train: 0.9233 loss_val: 1.5051 acc_val: 0.7800\n",
            "Epoch: 0197 (0.0761s) loss_train: 1.4067 acc_train: 0.9233 loss_val: 1.5045 acc_val: 0.7800\n",
            "Epoch: 0198 (0.0760s) loss_train: 1.4060 acc_train: 0.9260 loss_val: 1.5039 acc_val: 0.7825\n",
            "Epoch: 0199 (0.0754s) loss_train: 1.4050 acc_train: 0.9242 loss_val: 1.5030 acc_val: 0.7850\n",
            "Epoch: 0200 (0.0757s) loss_train: 1.4039 acc_train: 0.9215 loss_val: 1.5021 acc_val: 0.7900\n",
            "Epoch: 0201 (0.0759s) loss_train: 1.4026 acc_train: 0.9224 loss_val: 1.5011 acc_val: 0.7925\n",
            "Epoch: 0202 (0.0781s) loss_train: 1.4016 acc_train: 0.9215 loss_val: 1.5003 acc_val: 0.7925\n",
            "Epoch: 0203 (0.0760s) loss_train: 1.4010 acc_train: 0.9206 loss_val: 1.4999 acc_val: 0.7950\n",
            "Epoch: 0204 (0.0756s) loss_train: 1.4010 acc_train: 0.9197 loss_val: 1.4998 acc_val: 0.7950\n",
            "Epoch: 0205 (0.0755s) loss_train: 1.4014 acc_train: 0.9206 loss_val: 1.5003 acc_val: 0.7925\n",
            "Epoch: 0206 (0.0760s) loss_train: 1.4021 acc_train: 0.9224 loss_val: 1.5008 acc_val: 0.7925\n",
            "Epoch: 0207 (0.0765s) loss_train: 1.4029 acc_train: 0.9233 loss_val: 1.5013 acc_val: 0.7900\n",
            "Epoch: 0208 (0.0762s) loss_train: 1.4038 acc_train: 0.9206 loss_val: 1.5019 acc_val: 0.7800\n",
            "Epoch: 0209 (0.0756s) loss_train: 1.4044 acc_train: 0.9197 loss_val: 1.5022 acc_val: 0.7825\n",
            "Epoch: 0210 (0.0767s) loss_train: 1.4045 acc_train: 0.9206 loss_val: 1.5020 acc_val: 0.7825\n",
            "Epoch: 0211 (0.0762s) loss_train: 1.4049 acc_train: 0.9206 loss_val: 1.5019 acc_val: 0.7800\n",
            "Epoch: 0212 (0.0764s) loss_train: 1.4055 acc_train: 0.9215 loss_val: 1.5021 acc_val: 0.7825\n",
            "Epoch: 0213 (0.0787s) loss_train: 1.4059 acc_train: 0.9233 loss_val: 1.5023 acc_val: 0.7850\n",
            "Epoch: 0214 (0.0754s) loss_train: 1.4060 acc_train: 0.9260 loss_val: 1.5022 acc_val: 0.7875\n",
            "Epoch: 0215 (0.0775s) loss_train: 1.4061 acc_train: 0.9269 loss_val: 1.5022 acc_val: 0.7850\n",
            "Epoch: 0216 (0.0766s) loss_train: 1.4066 acc_train: 0.9260 loss_val: 1.5027 acc_val: 0.7825\n",
            "Epoch: 0217 (0.0758s) loss_train: 1.4068 acc_train: 0.9260 loss_val: 1.5032 acc_val: 0.7800\n",
            "Epoch: 0218 (0.0760s) loss_train: 1.4070 acc_train: 0.9278 loss_val: 1.5036 acc_val: 0.7850\n",
            "Epoch: 0219 (0.0761s) loss_train: 1.4069 acc_train: 0.9269 loss_val: 1.5039 acc_val: 0.7825\n",
            "Epoch: 0220 (0.0766s) loss_train: 1.4070 acc_train: 0.9260 loss_val: 1.5043 acc_val: 0.7800\n",
            "Epoch: 0221 (0.0765s) loss_train: 1.4069 acc_train: 0.9296 loss_val: 1.5045 acc_val: 0.7750\n",
            "Epoch: 0222 (0.0768s) loss_train: 1.4070 acc_train: 0.9296 loss_val: 1.5047 acc_val: 0.7775\n",
            "Epoch: 0223 (0.0758s) loss_train: 1.4069 acc_train: 0.9269 loss_val: 1.5049 acc_val: 0.7750\n",
            "Epoch: 0224 (0.0758s) loss_train: 1.4068 acc_train: 0.9278 loss_val: 1.5049 acc_val: 0.7750\n",
            "Epoch: 0225 (0.0764s) loss_train: 1.4063 acc_train: 0.9278 loss_val: 1.5044 acc_val: 0.7750\n",
            "Epoch: 0226 (0.0767s) loss_train: 1.4056 acc_train: 0.9269 loss_val: 1.5037 acc_val: 0.7725\n",
            "Epoch: 0227 (0.0764s) loss_train: 1.4053 acc_train: 0.9260 loss_val: 1.5033 acc_val: 0.7775\n",
            "Epoch: 0228 (0.0754s) loss_train: 1.4050 acc_train: 0.9242 loss_val: 1.5030 acc_val: 0.7750\n",
            "Epoch: 0229 (0.0758s) loss_train: 1.4048 acc_train: 0.9251 loss_val: 1.5026 acc_val: 0.7750\n",
            "Epoch: 0230 (0.0767s) loss_train: 1.4046 acc_train: 0.9278 loss_val: 1.5023 acc_val: 0.7775\n",
            "Epoch: 0231 (0.0759s) loss_train: 1.4045 acc_train: 0.9269 loss_val: 1.5019 acc_val: 0.7750\n",
            "Epoch: 0232 (0.0757s) loss_train: 1.4041 acc_train: 0.9278 loss_val: 1.5013 acc_val: 0.7850\n",
            "Epoch: 0233 (0.0766s) loss_train: 1.4038 acc_train: 0.9287 loss_val: 1.5009 acc_val: 0.7850\n",
            "Epoch: 0234 (0.0759s) loss_train: 1.4039 acc_train: 0.9278 loss_val: 1.5010 acc_val: 0.7825\n",
            "Epoch: 0235 (0.0763s) loss_train: 1.4036 acc_train: 0.9215 loss_val: 1.5006 acc_val: 0.7825\n",
            "Epoch: 0236 (0.0757s) loss_train: 1.4032 acc_train: 0.9224 loss_val: 1.5002 acc_val: 0.7850\n",
            "Epoch: 0237 (0.0760s) loss_train: 1.4026 acc_train: 0.9233 loss_val: 1.4997 acc_val: 0.7900\n",
            "Epoch: 0238 (0.0763s) loss_train: 1.4020 acc_train: 0.9224 loss_val: 1.4992 acc_val: 0.7850\n",
            "Epoch: 0239 (0.0748s) loss_train: 1.4013 acc_train: 0.9233 loss_val: 1.4986 acc_val: 0.7825\n",
            "Epoch: 0240 (0.0751s) loss_train: 1.4006 acc_train: 0.9215 loss_val: 1.4981 acc_val: 0.7825\n",
            "Epoch: 0241 (0.0754s) loss_train: 1.4005 acc_train: 0.9215 loss_val: 1.4980 acc_val: 0.7800\n",
            "Epoch: 0242 (0.0748s) loss_train: 1.4010 acc_train: 0.9206 loss_val: 1.4983 acc_val: 0.7825\n",
            "Epoch: 0243 (0.0763s) loss_train: 1.4013 acc_train: 0.9215 loss_val: 1.4986 acc_val: 0.7850\n",
            "Epoch: 0244 (0.0756s) loss_train: 1.4017 acc_train: 0.9215 loss_val: 1.4989 acc_val: 0.7825\n",
            "Epoch: 0245 (0.0762s) loss_train: 1.4029 acc_train: 0.9224 loss_val: 1.4998 acc_val: 0.7850\n",
            "Epoch: 0246 (0.0756s) loss_train: 1.4042 acc_train: 0.9224 loss_val: 1.5008 acc_val: 0.7825\n",
            "Epoch: 0247 (0.0763s) loss_train: 1.4055 acc_train: 0.9224 loss_val: 1.5018 acc_val: 0.7800\n",
            "Epoch: 0248 (0.0766s) loss_train: 1.4067 acc_train: 0.9242 loss_val: 1.5029 acc_val: 0.7850\n",
            "Epoch: 0249 (0.0757s) loss_train: 1.4078 acc_train: 0.9233 loss_val: 1.5035 acc_val: 0.7825\n",
            "Epoch: 0250 (0.0760s) loss_train: 1.4093 acc_train: 0.9215 loss_val: 1.5045 acc_val: 0.7850\n",
            "Epoch: 0251 (0.0766s) loss_train: 1.4110 acc_train: 0.9224 loss_val: 1.5057 acc_val: 0.7825\n",
            "Epoch: 0252 (0.0767s) loss_train: 1.4119 acc_train: 0.9233 loss_val: 1.5064 acc_val: 0.7850\n",
            "Epoch: 0253 (0.0771s) loss_train: 1.4123 acc_train: 0.9233 loss_val: 1.5067 acc_val: 0.7850\n",
            "Epoch: 0254 (0.0765s) loss_train: 1.4124 acc_train: 0.9224 loss_val: 1.5067 acc_val: 0.7850\n",
            "Epoch: 0255 (0.0758s) loss_train: 1.4119 acc_train: 0.9233 loss_val: 1.5062 acc_val: 0.7850\n",
            "Epoch: 0256 (0.0764s) loss_train: 1.4115 acc_train: 0.9233 loss_val: 1.5059 acc_val: 0.7825\n",
            "Epoch: 0257 (0.0765s) loss_train: 1.4111 acc_train: 0.9224 loss_val: 1.5056 acc_val: 0.7850\n",
            "Epoch: 0258 (0.0760s) loss_train: 1.4107 acc_train: 0.9233 loss_val: 1.5052 acc_val: 0.7850\n",
            "Epoch: 0259 (0.0757s) loss_train: 1.4103 acc_train: 0.9206 loss_val: 1.5049 acc_val: 0.7850\n",
            "Epoch: 0260 (0.0769s) loss_train: 1.4101 acc_train: 0.9215 loss_val: 1.5047 acc_val: 0.7875\n",
            "Epoch: 0261 (0.0761s) loss_train: 1.4097 acc_train: 0.9179 loss_val: 1.5043 acc_val: 0.7900\n",
            "Epoch: 0262 (0.0757s) loss_train: 1.4093 acc_train: 0.9197 loss_val: 1.5040 acc_val: 0.7875\n",
            "Epoch: 0263 (0.0760s) loss_train: 1.4086 acc_train: 0.9197 loss_val: 1.5036 acc_val: 0.7825\n",
            "Epoch: 0264 (0.0765s) loss_train: 1.4081 acc_train: 0.9224 loss_val: 1.5031 acc_val: 0.7775\n",
            "Epoch: 0265 (0.0765s) loss_train: 1.4081 acc_train: 0.9215 loss_val: 1.5030 acc_val: 0.7800\n",
            "Epoch: 0266 (0.0766s) loss_train: 1.4079 acc_train: 0.9206 loss_val: 1.5029 acc_val: 0.7800\n",
            "Epoch: 0267 (0.0769s) loss_train: 1.4074 acc_train: 0.9206 loss_val: 1.5025 acc_val: 0.7825\n",
            "Epoch: 0268 (0.0761s) loss_train: 1.4072 acc_train: 0.9206 loss_val: 1.5026 acc_val: 0.7850\n",
            "Epoch: 0269 (0.0774s) loss_train: 1.4068 acc_train: 0.9197 loss_val: 1.5023 acc_val: 0.7825\n",
            "Epoch: 0270 (0.0764s) loss_train: 1.4063 acc_train: 0.9197 loss_val: 1.5021 acc_val: 0.7800\n",
            "Epoch: 0271 (0.0758s) loss_train: 1.4056 acc_train: 0.9197 loss_val: 1.5018 acc_val: 0.7800\n",
            "Epoch: 0272 (0.0764s) loss_train: 1.4052 acc_train: 0.9188 loss_val: 1.5018 acc_val: 0.7800\n",
            "Epoch: 0273 (0.0763s) loss_train: 1.4049 acc_train: 0.9206 loss_val: 1.5021 acc_val: 0.7800\n",
            "Epoch: 0274 (0.0762s) loss_train: 1.4045 acc_train: 0.9206 loss_val: 1.5023 acc_val: 0.7800\n",
            "Epoch: 0275 (0.0763s) loss_train: 1.4045 acc_train: 0.9188 loss_val: 1.5027 acc_val: 0.7800\n",
            "Epoch: 0276 (0.0762s) loss_train: 1.4047 acc_train: 0.9206 loss_val: 1.5034 acc_val: 0.7800\n",
            "Epoch: 0277 (0.0763s) loss_train: 1.4050 acc_train: 0.9197 loss_val: 1.5042 acc_val: 0.7725\n",
            "Epoch: 0278 (0.0769s) loss_train: 1.4057 acc_train: 0.9197 loss_val: 1.5051 acc_val: 0.7775\n",
            "Epoch: 0279 (0.0763s) loss_train: 1.4063 acc_train: 0.9197 loss_val: 1.5059 acc_val: 0.7750\n",
            "Epoch: 0280 (0.0764s) loss_train: 1.4065 acc_train: 0.9215 loss_val: 1.5064 acc_val: 0.7750\n",
            "Epoch: 0281 (0.0763s) loss_train: 1.4065 acc_train: 0.9215 loss_val: 1.5065 acc_val: 0.7725\n",
            "Epoch: 0282 (0.0776s) loss_train: 1.4067 acc_train: 0.9233 loss_val: 1.5068 acc_val: 0.7725\n",
            "Epoch: 0283 (0.0771s) loss_train: 1.4070 acc_train: 0.9224 loss_val: 1.5071 acc_val: 0.7725\n",
            "Epoch: 0284 (0.0757s) loss_train: 1.4076 acc_train: 0.9233 loss_val: 1.5075 acc_val: 0.7775\n",
            "Epoch: 0285 (0.0763s) loss_train: 1.4085 acc_train: 0.9242 loss_val: 1.5080 acc_val: 0.7800\n",
            "Epoch: 0286 (0.0766s) loss_train: 1.4095 acc_train: 0.9251 loss_val: 1.5088 acc_val: 0.7800\n",
            "Epoch: 0287 (0.0772s) loss_train: 1.4107 acc_train: 0.9242 loss_val: 1.5097 acc_val: 0.7825\n",
            "Epoch: 0288 (0.0762s) loss_train: 1.4119 acc_train: 0.9233 loss_val: 1.5108 acc_val: 0.7800\n",
            "Epoch: 0289 (0.0758s) loss_train: 1.4132 acc_train: 0.9224 loss_val: 1.5121 acc_val: 0.7825\n",
            "Epoch: 0290 (0.0765s) loss_train: 1.4148 acc_train: 0.9224 loss_val: 1.5134 acc_val: 0.7825\n",
            "Epoch: 0291 (0.0764s) loss_train: 1.4165 acc_train: 0.9251 loss_val: 1.5148 acc_val: 0.7800\n",
            "Epoch: 0292 (0.0762s) loss_train: 1.4174 acc_train: 0.9251 loss_val: 1.5158 acc_val: 0.7800\n",
            "Epoch: 0293 (0.0764s) loss_train: 1.4182 acc_train: 0.9260 loss_val: 1.5167 acc_val: 0.7825\n",
            "Epoch: 0294 (0.0767s) loss_train: 1.4187 acc_train: 0.9260 loss_val: 1.5173 acc_val: 0.7850\n",
            "Epoch: 0295 (0.0770s) loss_train: 1.4189 acc_train: 0.9251 loss_val: 1.5177 acc_val: 0.7800\n",
            "Epoch: 0296 (0.0772s) loss_train: 1.4188 acc_train: 0.9287 loss_val: 1.5177 acc_val: 0.7775\n",
            "Epoch: 0297 (0.0762s) loss_train: 1.4184 acc_train: 0.9287 loss_val: 1.5174 acc_val: 0.7750\n",
            "Epoch: 0298 (0.0761s) loss_train: 1.4180 acc_train: 0.9278 loss_val: 1.5172 acc_val: 0.7725\n",
            "Epoch: 0299 (0.0764s) loss_train: 1.4172 acc_train: 0.9260 loss_val: 1.5166 acc_val: 0.7725\n",
            "Epoch: 0300 (0.0768s) loss_train: 1.4165 acc_train: 0.9251 loss_val: 1.5160 acc_val: 0.7700\n",
            "Optimization Finished!\n",
            "Total time elapsed: 26.9694s\n",
            "Loading 241th epoch\n",
            "Test set results: loss 1.4712 accuracy 0.8208\n"
          ]
        }
      ],
      "source": [
        "# demonstrate train_and_evaluate function on cora\n",
        "# (This is for the demonstration, you can run it. However running this cell will add chance to exceed the 8 minutes limits depends on the device )\n",
        "best_model, loss_values, acc_values, loss_test, acc_test = train_and_evaluate(GAT_cora, (features, adj_mat), labels, idx_train, idx_val, args['epochs'], args['patience'], args['print'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZoE3MGp_ikc"
      },
      "outputs": [],
      "source": [
        "# (This is for the demonstration, you can run it. However running this cell will add chance to exceed the 8 minutes limits depends on the device )\n",
        "import torch\n",
        "import os\n",
        "import requests\n",
        "import tarfile\n",
        "import glob\n",
        "# Set the parameters directly\n",
        "args = {\n",
        "    'seed': 42,  # 9\n",
        "    'no_cuda': False,\n",
        "    'no_mps': False,\n",
        "    'hidden_dim': 64,\n",
        "    'num_heads': 8,\n",
        "    'concat_heads': False,\n",
        "    'dropout_p': 0.6,\n",
        "    'lr': 0.005,\n",
        "    'l2': 5e-4,\n",
        "    'epochs': 200,\n",
        "    'patience': 100,\n",
        "    'print': True,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-dO1rSi9GZO",
        "outputId": "5c92ee08-8098-403a-97e4-8cc3fa56e5ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0001 (0.1442s) loss_train: 1.5972 acc_train: 0.7794 loss_val: 1.6414 acc_val: 0.7100\n",
            "Epoch: 0002 (0.1302s) loss_train: 1.6003 acc_train: 0.7771 loss_val: 1.6440 acc_val: 0.7050\n",
            "Epoch: 0003 (0.1203s) loss_train: 1.6019 acc_train: 0.7713 loss_val: 1.6452 acc_val: 0.7050\n",
            "Epoch: 0004 (0.1167s) loss_train: 1.6017 acc_train: 0.7690 loss_val: 1.6450 acc_val: 0.7050\n",
            "Epoch: 0005 (0.1158s) loss_train: 1.6003 acc_train: 0.7695 loss_val: 1.6439 acc_val: 0.7100\n",
            "Epoch: 0006 (0.1174s) loss_train: 1.5985 acc_train: 0.7713 loss_val: 1.6428 acc_val: 0.7125\n",
            "Epoch: 0007 (0.1152s) loss_train: 1.5962 acc_train: 0.7713 loss_val: 1.6413 acc_val: 0.7175\n",
            "Epoch: 0008 (0.1154s) loss_train: 1.5936 acc_train: 0.7736 loss_val: 1.6393 acc_val: 0.7200\n",
            "Epoch: 0009 (0.1152s) loss_train: 1.5912 acc_train: 0.7748 loss_val: 1.6369 acc_val: 0.7175\n",
            "Epoch: 0010 (0.1155s) loss_train: 1.5884 acc_train: 0.7759 loss_val: 1.6344 acc_val: 0.7150\n",
            "Epoch: 0011 (0.1164s) loss_train: 1.5859 acc_train: 0.7782 loss_val: 1.6322 acc_val: 0.7225\n",
            "Epoch: 0012 (0.1207s) loss_train: 1.5830 acc_train: 0.7817 loss_val: 1.6298 acc_val: 0.7275\n",
            "Epoch: 0013 (0.1153s) loss_train: 1.5801 acc_train: 0.7834 loss_val: 1.6276 acc_val: 0.7225\n",
            "Epoch: 0014 (0.1159s) loss_train: 1.5775 acc_train: 0.7858 loss_val: 1.6257 acc_val: 0.7175\n",
            "Epoch: 0015 (0.1154s) loss_train: 1.5757 acc_train: 0.7863 loss_val: 1.6244 acc_val: 0.7225\n",
            "Epoch: 0016 (0.1152s) loss_train: 1.5741 acc_train: 0.7858 loss_val: 1.6234 acc_val: 0.7200\n",
            "Epoch: 0017 (0.1158s) loss_train: 1.5726 acc_train: 0.7863 loss_val: 1.6224 acc_val: 0.7225\n",
            "Epoch: 0018 (0.1163s) loss_train: 1.5715 acc_train: 0.7898 loss_val: 1.6218 acc_val: 0.7300\n",
            "Epoch: 0019 (0.1168s) loss_train: 1.5707 acc_train: 0.7927 loss_val: 1.6214 acc_val: 0.7325\n",
            "Epoch: 0020 (0.1158s) loss_train: 1.5699 acc_train: 0.7939 loss_val: 1.6209 acc_val: 0.7375\n",
            "Epoch: 0021 (0.1155s) loss_train: 1.5691 acc_train: 0.7962 loss_val: 1.6205 acc_val: 0.7375\n",
            "Epoch: 0022 (0.1161s) loss_train: 1.5683 acc_train: 0.7950 loss_val: 1.6200 acc_val: 0.7325\n",
            "Epoch: 0023 (0.1154s) loss_train: 1.5674 acc_train: 0.7950 loss_val: 1.6193 acc_val: 0.7275\n",
            "Epoch: 0024 (0.1155s) loss_train: 1.5665 acc_train: 0.7973 loss_val: 1.6184 acc_val: 0.7275\n",
            "Epoch: 0025 (0.1162s) loss_train: 1.5657 acc_train: 0.7997 loss_val: 1.6174 acc_val: 0.7200\n",
            "Epoch: 0026 (0.1154s) loss_train: 1.5651 acc_train: 0.7997 loss_val: 1.6169 acc_val: 0.7200\n",
            "Epoch: 0027 (0.1155s) loss_train: 1.5645 acc_train: 0.7991 loss_val: 1.6165 acc_val: 0.7175\n",
            "Epoch: 0028 (0.1156s) loss_train: 1.5638 acc_train: 0.7997 loss_val: 1.6160 acc_val: 0.7250\n",
            "Epoch: 0029 (0.1154s) loss_train: 1.5636 acc_train: 0.8002 loss_val: 1.6159 acc_val: 0.7250\n",
            "Epoch: 0030 (0.1157s) loss_train: 1.5633 acc_train: 0.7997 loss_val: 1.6158 acc_val: 0.7250\n",
            "Epoch: 0031 (0.1157s) loss_train: 1.5632 acc_train: 0.8020 loss_val: 1.6160 acc_val: 0.7250\n",
            "Epoch: 0032 (0.1166s) loss_train: 1.5634 acc_train: 0.8043 loss_val: 1.6164 acc_val: 0.7325\n",
            "Epoch: 0033 (0.1176s) loss_train: 1.5637 acc_train: 0.8049 loss_val: 1.6169 acc_val: 0.7350\n",
            "Epoch: 0034 (0.1174s) loss_train: 1.5641 acc_train: 0.8066 loss_val: 1.6175 acc_val: 0.7350\n",
            "Epoch: 0035 (0.1175s) loss_train: 1.5644 acc_train: 0.8089 loss_val: 1.6178 acc_val: 0.7375\n",
            "Epoch: 0036 (0.1171s) loss_train: 1.5643 acc_train: 0.8066 loss_val: 1.6178 acc_val: 0.7375\n",
            "Epoch: 0037 (0.1176s) loss_train: 1.5641 acc_train: 0.8060 loss_val: 1.6178 acc_val: 0.7350\n",
            "Epoch: 0038 (0.1201s) loss_train: 1.5638 acc_train: 0.8025 loss_val: 1.6177 acc_val: 0.7350\n",
            "Epoch: 0039 (0.1181s) loss_train: 1.5636 acc_train: 0.8008 loss_val: 1.6176 acc_val: 0.7400\n",
            "Epoch: 0040 (0.1154s) loss_train: 1.5634 acc_train: 0.8020 loss_val: 1.6176 acc_val: 0.7350\n",
            "Epoch: 0041 (0.1179s) loss_train: 1.5632 acc_train: 0.8020 loss_val: 1.6175 acc_val: 0.7375\n",
            "Epoch: 0042 (0.1171s) loss_train: 1.5628 acc_train: 0.8014 loss_val: 1.6172 acc_val: 0.7375\n",
            "Epoch: 0043 (0.1176s) loss_train: 1.5622 acc_train: 0.8031 loss_val: 1.6167 acc_val: 0.7375\n",
            "Epoch: 0044 (0.1165s) loss_train: 1.5615 acc_train: 0.8025 loss_val: 1.6160 acc_val: 0.7275\n",
            "Epoch: 0045 (0.1172s) loss_train: 1.5611 acc_train: 0.8014 loss_val: 1.6155 acc_val: 0.7225\n",
            "Epoch: 0046 (0.1160s) loss_train: 1.5607 acc_train: 0.8002 loss_val: 1.6150 acc_val: 0.7225\n",
            "Epoch: 0047 (0.1159s) loss_train: 1.5603 acc_train: 0.8014 loss_val: 1.6146 acc_val: 0.7250\n",
            "Epoch: 0048 (0.1156s) loss_train: 1.5598 acc_train: 0.8025 loss_val: 1.6140 acc_val: 0.7225\n",
            "Epoch: 0049 (0.1158s) loss_train: 1.5592 acc_train: 0.8014 loss_val: 1.6132 acc_val: 0.7225\n",
            "Epoch: 0050 (0.1159s) loss_train: 1.5585 acc_train: 0.8037 loss_val: 1.6124 acc_val: 0.7275\n",
            "Epoch: 0051 (0.1159s) loss_train: 1.5580 acc_train: 0.8025 loss_val: 1.6116 acc_val: 0.7225\n",
            "Epoch: 0052 (0.1155s) loss_train: 1.5572 acc_train: 0.7991 loss_val: 1.6110 acc_val: 0.7275\n",
            "Epoch: 0053 (0.1159s) loss_train: 1.5571 acc_train: 0.7991 loss_val: 1.6107 acc_val: 0.7275\n",
            "Epoch: 0054 (0.1159s) loss_train: 1.5569 acc_train: 0.8020 loss_val: 1.6105 acc_val: 0.7275\n",
            "Epoch: 0055 (0.1155s) loss_train: 1.5567 acc_train: 0.8031 loss_val: 1.6102 acc_val: 0.7275\n",
            "Epoch: 0056 (0.1169s) loss_train: 1.5566 acc_train: 0.8043 loss_val: 1.6100 acc_val: 0.7275\n",
            "Epoch: 0057 (0.1153s) loss_train: 1.5564 acc_train: 0.8049 loss_val: 1.6100 acc_val: 0.7250\n",
            "Epoch: 0058 (0.1161s) loss_train: 1.5563 acc_train: 0.8078 loss_val: 1.6100 acc_val: 0.7225\n",
            "Epoch: 0059 (0.1177s) loss_train: 1.5561 acc_train: 0.8066 loss_val: 1.6099 acc_val: 0.7175\n",
            "Epoch: 0060 (0.1159s) loss_train: 1.5558 acc_train: 0.8049 loss_val: 1.6099 acc_val: 0.7200\n",
            "Epoch: 0061 (0.1159s) loss_train: 1.5554 acc_train: 0.8054 loss_val: 1.6098 acc_val: 0.7225\n",
            "Epoch: 0062 (0.1159s) loss_train: 1.5548 acc_train: 0.8020 loss_val: 1.6094 acc_val: 0.7200\n",
            "Epoch: 0063 (0.1159s) loss_train: 1.5541 acc_train: 0.8014 loss_val: 1.6090 acc_val: 0.7225\n",
            "Epoch: 0064 (0.1156s) loss_train: 1.5534 acc_train: 0.8008 loss_val: 1.6084 acc_val: 0.7200\n",
            "Epoch: 0065 (0.1160s) loss_train: 1.5529 acc_train: 0.8014 loss_val: 1.6080 acc_val: 0.7200\n",
            "Epoch: 0066 (0.1159s) loss_train: 1.5524 acc_train: 0.8020 loss_val: 1.6075 acc_val: 0.7200\n",
            "Epoch: 0067 (0.1159s) loss_train: 1.5519 acc_train: 0.8020 loss_val: 1.6070 acc_val: 0.7225\n",
            "Epoch: 0068 (0.1158s) loss_train: 1.5515 acc_train: 0.8025 loss_val: 1.6065 acc_val: 0.7300\n",
            "Epoch: 0069 (0.1158s) loss_train: 1.5507 acc_train: 0.8025 loss_val: 1.6057 acc_val: 0.7350\n",
            "Epoch: 0070 (0.1158s) loss_train: 1.5500 acc_train: 0.8020 loss_val: 1.6046 acc_val: 0.7350\n",
            "Epoch: 0071 (0.1160s) loss_train: 1.5494 acc_train: 0.7985 loss_val: 1.6039 acc_val: 0.7325\n",
            "Epoch: 0072 (0.1158s) loss_train: 1.5486 acc_train: 0.7973 loss_val: 1.6030 acc_val: 0.7325\n",
            "Epoch: 0073 (0.1159s) loss_train: 1.5483 acc_train: 0.7985 loss_val: 1.6025 acc_val: 0.7325\n",
            "Epoch: 0074 (0.1156s) loss_train: 1.5480 acc_train: 0.8020 loss_val: 1.6021 acc_val: 0.7350\n",
            "Epoch: 0075 (0.1185s) loss_train: 1.5477 acc_train: 0.8054 loss_val: 1.6017 acc_val: 0.7400\n",
            "Epoch: 0076 (0.1157s) loss_train: 1.5476 acc_train: 0.8049 loss_val: 1.6017 acc_val: 0.7425\n",
            "Epoch: 0077 (0.1162s) loss_train: 1.5477 acc_train: 0.8060 loss_val: 1.6019 acc_val: 0.7375\n",
            "Epoch: 0078 (0.1168s) loss_train: 1.5481 acc_train: 0.8054 loss_val: 1.6022 acc_val: 0.7400\n",
            "Epoch: 0079 (0.1198s) loss_train: 1.5491 acc_train: 0.8083 loss_val: 1.6029 acc_val: 0.7325\n",
            "Epoch: 0080 (0.1173s) loss_train: 1.5499 acc_train: 0.8066 loss_val: 1.6037 acc_val: 0.7250\n",
            "Epoch: 0081 (0.1183s) loss_train: 1.5503 acc_train: 0.8072 loss_val: 1.6042 acc_val: 0.7275\n",
            "Epoch: 0082 (0.1168s) loss_train: 1.5511 acc_train: 0.8060 loss_val: 1.6051 acc_val: 0.7275\n",
            "Epoch: 0083 (0.1175s) loss_train: 1.5518 acc_train: 0.8054 loss_val: 1.6061 acc_val: 0.7325\n",
            "Epoch: 0084 (0.1230s) loss_train: 1.5520 acc_train: 0.8072 loss_val: 1.6067 acc_val: 0.7325\n",
            "Epoch: 0085 (0.1186s) loss_train: 1.5522 acc_train: 0.8066 loss_val: 1.6072 acc_val: 0.7325\n",
            "Epoch: 0086 (0.1174s) loss_train: 1.5522 acc_train: 0.8078 loss_val: 1.6077 acc_val: 0.7375\n",
            "Epoch: 0087 (0.1174s) loss_train: 1.5521 acc_train: 0.8060 loss_val: 1.6079 acc_val: 0.7425\n",
            "Epoch: 0088 (0.1177s) loss_train: 1.5518 acc_train: 0.8083 loss_val: 1.6079 acc_val: 0.7425\n",
            "Epoch: 0089 (0.1177s) loss_train: 1.5516 acc_train: 0.8072 loss_val: 1.6078 acc_val: 0.7400\n",
            "Epoch: 0090 (0.1177s) loss_train: 1.5513 acc_train: 0.8072 loss_val: 1.6080 acc_val: 0.7400\n",
            "Epoch: 0091 (0.1174s) loss_train: 1.5508 acc_train: 0.8060 loss_val: 1.6079 acc_val: 0.7400\n",
            "Epoch: 0092 (0.1171s) loss_train: 1.5504 acc_train: 0.8049 loss_val: 1.6078 acc_val: 0.7400\n",
            "Epoch: 0093 (0.1194s) loss_train: 1.5499 acc_train: 0.8060 loss_val: 1.6077 acc_val: 0.7350\n",
            "Epoch: 0094 (0.1179s) loss_train: 1.5495 acc_train: 0.8043 loss_val: 1.6076 acc_val: 0.7350\n",
            "Epoch: 0095 (0.1187s) loss_train: 1.5492 acc_train: 0.8054 loss_val: 1.6075 acc_val: 0.7325\n",
            "Epoch: 0096 (0.1178s) loss_train: 1.5489 acc_train: 0.8037 loss_val: 1.6074 acc_val: 0.7300\n",
            "Epoch: 0097 (0.1172s) loss_train: 1.5489 acc_train: 0.8049 loss_val: 1.6075 acc_val: 0.7275\n",
            "Epoch: 0098 (0.1180s) loss_train: 1.5487 acc_train: 0.8037 loss_val: 1.6074 acc_val: 0.7250\n",
            "Epoch: 0099 (0.1177s) loss_train: 1.5481 acc_train: 0.8060 loss_val: 1.6070 acc_val: 0.7225\n",
            "Epoch: 0100 (0.1177s) loss_train: 1.5478 acc_train: 0.8043 loss_val: 1.6066 acc_val: 0.7225\n",
            "Epoch: 0101 (0.1197s) loss_train: 1.5479 acc_train: 0.8020 loss_val: 1.6066 acc_val: 0.7200\n",
            "Epoch: 0102 (0.1187s) loss_train: 1.5480 acc_train: 0.8031 loss_val: 1.6067 acc_val: 0.7225\n",
            "Epoch: 0103 (0.1193s) loss_train: 1.5481 acc_train: 0.8031 loss_val: 1.6067 acc_val: 0.7275\n",
            "Epoch: 0104 (0.1185s) loss_train: 1.5482 acc_train: 0.8025 loss_val: 1.6068 acc_val: 0.7275\n",
            "Epoch: 0105 (0.1184s) loss_train: 1.5485 acc_train: 0.8072 loss_val: 1.6069 acc_val: 0.7275\n",
            "Epoch: 0106 (0.1175s) loss_train: 1.5483 acc_train: 0.8078 loss_val: 1.6064 acc_val: 0.7300\n",
            "Epoch: 0107 (0.1177s) loss_train: 1.5479 acc_train: 0.8072 loss_val: 1.6059 acc_val: 0.7325\n",
            "Epoch: 0108 (0.1186s) loss_train: 1.5472 acc_train: 0.8083 loss_val: 1.6051 acc_val: 0.7225\n",
            "Epoch: 0109 (0.1182s) loss_train: 1.5464 acc_train: 0.8066 loss_val: 1.6044 acc_val: 0.7225\n",
            "Epoch: 0110 (0.1187s) loss_train: 1.5461 acc_train: 0.8089 loss_val: 1.6040 acc_val: 0.7225\n",
            "Epoch: 0111 (0.1193s) loss_train: 1.5460 acc_train: 0.8095 loss_val: 1.6039 acc_val: 0.7275\n",
            "Epoch: 0112 (0.1186s) loss_train: 1.5457 acc_train: 0.8066 loss_val: 1.6036 acc_val: 0.7250\n",
            "Epoch: 0113 (0.1179s) loss_train: 1.5455 acc_train: 0.8072 loss_val: 1.6034 acc_val: 0.7250\n",
            "Epoch: 0114 (0.1172s) loss_train: 1.5454 acc_train: 0.8089 loss_val: 1.6033 acc_val: 0.7225\n",
            "Epoch: 0115 (0.1181s) loss_train: 1.5454 acc_train: 0.8118 loss_val: 1.6033 acc_val: 0.7225\n",
            "Epoch: 0116 (0.1186s) loss_train: 1.5458 acc_train: 0.8124 loss_val: 1.6036 acc_val: 0.7225\n",
            "Epoch: 0117 (0.1185s) loss_train: 1.5464 acc_train: 0.8141 loss_val: 1.6043 acc_val: 0.7225\n",
            "Epoch: 0118 (0.1183s) loss_train: 1.5473 acc_train: 0.8124 loss_val: 1.6052 acc_val: 0.7250\n",
            "Epoch: 0119 (0.1188s) loss_train: 1.5480 acc_train: 0.8135 loss_val: 1.6058 acc_val: 0.7250\n",
            "Epoch: 0120 (0.1180s) loss_train: 1.5485 acc_train: 0.8135 loss_val: 1.6062 acc_val: 0.7250\n",
            "Epoch: 0121 (0.1172s) loss_train: 1.5492 acc_train: 0.8141 loss_val: 1.6066 acc_val: 0.7250\n",
            "Epoch: 0122 (0.1191s) loss_train: 1.5501 acc_train: 0.8141 loss_val: 1.6072 acc_val: 0.7250\n",
            "Epoch: 0123 (0.1180s) loss_train: 1.5512 acc_train: 0.8147 loss_val: 1.6078 acc_val: 0.7275\n",
            "Epoch: 0124 (0.1182s) loss_train: 1.5520 acc_train: 0.8147 loss_val: 1.6083 acc_val: 0.7300\n",
            "Epoch: 0125 (0.1183s) loss_train: 1.5524 acc_train: 0.8159 loss_val: 1.6084 acc_val: 0.7275\n",
            "Epoch: 0126 (0.1181s) loss_train: 1.5528 acc_train: 0.8147 loss_val: 1.6087 acc_val: 0.7250\n",
            "Epoch: 0127 (0.1180s) loss_train: 1.5532 acc_train: 0.8130 loss_val: 1.6090 acc_val: 0.7275\n",
            "Epoch: 0128 (0.1190s) loss_train: 1.5533 acc_train: 0.8118 loss_val: 1.6093 acc_val: 0.7250\n",
            "Epoch: 0129 (0.1186s) loss_train: 1.5532 acc_train: 0.8107 loss_val: 1.6093 acc_val: 0.7225\n",
            "Epoch: 0130 (0.1181s) loss_train: 1.5533 acc_train: 0.8101 loss_val: 1.6097 acc_val: 0.7250\n",
            "Epoch: 0131 (0.1182s) loss_train: 1.5532 acc_train: 0.8101 loss_val: 1.6099 acc_val: 0.7250\n",
            "Epoch: 0132 (0.1174s) loss_train: 1.5530 acc_train: 0.8089 loss_val: 1.6101 acc_val: 0.7225\n",
            "Epoch: 0133 (0.1179s) loss_train: 1.5530 acc_train: 0.8101 loss_val: 1.6105 acc_val: 0.7250\n",
            "Epoch: 0134 (0.1176s) loss_train: 1.5533 acc_train: 0.8101 loss_val: 1.6109 acc_val: 0.7225\n",
            "Epoch: 0135 (0.1188s) loss_train: 1.5534 acc_train: 0.8135 loss_val: 1.6110 acc_val: 0.7175\n",
            "Epoch: 0136 (0.1179s) loss_train: 1.5531 acc_train: 0.8141 loss_val: 1.6112 acc_val: 0.7150\n",
            "Epoch: 0137 (0.1187s) loss_train: 1.5527 acc_train: 0.8130 loss_val: 1.6112 acc_val: 0.7150\n",
            "Epoch: 0138 (0.1188s) loss_train: 1.5522 acc_train: 0.8124 loss_val: 1.6110 acc_val: 0.7075\n",
            "Epoch: 0139 (0.1181s) loss_train: 1.5518 acc_train: 0.8095 loss_val: 1.6110 acc_val: 0.7125\n",
            "Epoch: 0140 (0.1183s) loss_train: 1.5511 acc_train: 0.8072 loss_val: 1.6107 acc_val: 0.7100\n",
            "Epoch: 0141 (0.1179s) loss_train: 1.5500 acc_train: 0.8072 loss_val: 1.6096 acc_val: 0.7175\n",
            "Epoch: 0142 (0.1184s) loss_train: 1.5490 acc_train: 0.8066 loss_val: 1.6087 acc_val: 0.7150\n",
            "Epoch: 0143 (0.1177s) loss_train: 1.5482 acc_train: 0.8083 loss_val: 1.6079 acc_val: 0.7150\n",
            "Epoch: 0144 (0.1180s) loss_train: 1.5474 acc_train: 0.8083 loss_val: 1.6072 acc_val: 0.7175\n",
            "Epoch: 0145 (0.1185s) loss_train: 1.5464 acc_train: 0.8095 loss_val: 1.6061 acc_val: 0.7175\n",
            "Epoch: 0146 (0.1182s) loss_train: 1.5452 acc_train: 0.8089 loss_val: 1.6050 acc_val: 0.7225\n",
            "Epoch: 0147 (0.1176s) loss_train: 1.5443 acc_train: 0.8078 loss_val: 1.6040 acc_val: 0.7250\n",
            "Epoch: 0148 (0.1179s) loss_train: 1.5435 acc_train: 0.8049 loss_val: 1.6033 acc_val: 0.7275\n",
            "Epoch: 0149 (0.1180s) loss_train: 1.5431 acc_train: 0.8043 loss_val: 1.6031 acc_val: 0.7275\n",
            "Epoch: 0150 (0.1183s) loss_train: 1.5430 acc_train: 0.8049 loss_val: 1.6032 acc_val: 0.7250\n",
            "Epoch: 0151 (0.1188s) loss_train: 1.5433 acc_train: 0.8060 loss_val: 1.6034 acc_val: 0.7275\n",
            "Epoch: 0152 (0.1175s) loss_train: 1.5439 acc_train: 0.8078 loss_val: 1.6038 acc_val: 0.7300\n",
            "Epoch: 0153 (0.1178s) loss_train: 1.5448 acc_train: 0.8066 loss_val: 1.6044 acc_val: 0.7250\n",
            "Epoch: 0154 (0.1188s) loss_train: 1.5462 acc_train: 0.8095 loss_val: 1.6057 acc_val: 0.7350\n",
            "Epoch: 0155 (0.1184s) loss_train: 1.5479 acc_train: 0.8066 loss_val: 1.6073 acc_val: 0.7300\n",
            "Epoch: 0156 (0.1182s) loss_train: 1.5499 acc_train: 0.8066 loss_val: 1.6090 acc_val: 0.7300\n",
            "Epoch: 0157 (0.1180s) loss_train: 1.5516 acc_train: 0.8089 loss_val: 1.6105 acc_val: 0.7300\n",
            "Epoch: 0158 (0.1180s) loss_train: 1.5530 acc_train: 0.8089 loss_val: 1.6117 acc_val: 0.7300\n",
            "Epoch: 0159 (0.1179s) loss_train: 1.5541 acc_train: 0.8101 loss_val: 1.6125 acc_val: 0.7275\n",
            "Epoch: 0160 (0.1168s) loss_train: 1.5550 acc_train: 0.8112 loss_val: 1.6132 acc_val: 0.7275\n",
            "Epoch: 0161 (0.1188s) loss_train: 1.5556 acc_train: 0.8095 loss_val: 1.6136 acc_val: 0.7250\n",
            "Epoch: 0162 (0.1185s) loss_train: 1.5558 acc_train: 0.8066 loss_val: 1.6141 acc_val: 0.7200\n",
            "Epoch: 0163 (0.1177s) loss_train: 1.5561 acc_train: 0.8054 loss_val: 1.6146 acc_val: 0.7200\n",
            "Epoch: 0164 (0.1180s) loss_train: 1.5563 acc_train: 0.8066 loss_val: 1.6150 acc_val: 0.7250\n",
            "Epoch: 0165 (0.1182s) loss_train: 1.5563 acc_train: 0.8060 loss_val: 1.6152 acc_val: 0.7250\n",
            "Epoch: 0166 (0.1173s) loss_train: 1.5561 acc_train: 0.8072 loss_val: 1.6152 acc_val: 0.7250\n",
            "Epoch: 0167 (0.1187s) loss_train: 1.5561 acc_train: 0.8078 loss_val: 1.6152 acc_val: 0.7200\n",
            "Epoch: 0168 (0.1177s) loss_train: 1.5560 acc_train: 0.8089 loss_val: 1.6151 acc_val: 0.7200\n",
            "Epoch: 0169 (0.1178s) loss_train: 1.5557 acc_train: 0.8089 loss_val: 1.6147 acc_val: 0.7175\n",
            "Epoch: 0170 (0.1185s) loss_train: 1.5555 acc_train: 0.8107 loss_val: 1.6144 acc_val: 0.7125\n",
            "Epoch: 0171 (0.1180s) loss_train: 1.5553 acc_train: 0.8095 loss_val: 1.6137 acc_val: 0.7150\n",
            "Epoch: 0172 (0.1181s) loss_train: 1.5554 acc_train: 0.8083 loss_val: 1.6131 acc_val: 0.7125\n",
            "Epoch: 0173 (0.1180s) loss_train: 1.5556 acc_train: 0.8107 loss_val: 1.6129 acc_val: 0.7100\n",
            "Epoch: 0174 (0.1190s) loss_train: 1.5558 acc_train: 0.8112 loss_val: 1.6127 acc_val: 0.7075\n",
            "Epoch: 0175 (0.1182s) loss_train: 1.5560 acc_train: 0.8130 loss_val: 1.6126 acc_val: 0.7050\n",
            "Epoch: 0176 (0.1173s) loss_train: 1.5558 acc_train: 0.8124 loss_val: 1.6122 acc_val: 0.7025\n",
            "Early stopping...\n",
            "Best Val Loss: 1.6017, Best Val Acc: 0.7425\n",
            "Optimization Finished!\n",
            "Total time elapsed: 28.1098s\n",
            "Loading 76th epoch\n",
            "Test set results: loss 1.6022 accuracy 0.7208\n"
          ]
        }
      ],
      "source": [
        "# test train_and_evaluate function on citeseer\n",
        "# (This is for the  demonstration, you can run it. However running this cell will exceed the 8 minutes limits )\n",
        "best_model, loss_values, acc_values, loss_test, acc_test = train_and_evaluate(GAT_citeseer, (features, adj_mat), labels, idx_train, idx_val, args['epochs'], args['patience'], args['print'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-UoHpk11IBL"
      },
      "source": [
        "For inductive learning GAT:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrZymBgCEfGn",
        "outputId": "e0f6525b-f13a-4e3c-ad4b-c17dcde59410"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model...\n",
            "Epoch: 0001 | time: 5.4065s | Train loss: 0.7525 | Train micro F1: 0.4247 | Val loss: 0.5892 | Val micro F1: 0.4316\n",
            "\n",
            "Total time elapsed: 5.5106s  |  Average time elapsed: 5.5106s\n",
            "\n",
            "Best model performance @ Epoch 0001: \n",
            "Test loss: 0.5942  |  Test micro F1: 0.4330\n"
          ]
        }
      ],
      "source": [
        "# demonstration of train function for GAT PPI dataset\n",
        "ppi_train_params_demo = {\n",
        "  \"lr\": 5e-3,\n",
        "  \"weight_decay\": 0,\n",
        "  \"epochs\": 1,\n",
        "  \"patience\": 100,\n",
        "  \"model_name\": 'gat_ppi_demo'\n",
        "}\n",
        "best_model,_ ,_ ,_ ,_ = training_loop(model, ppi_train_params_demo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef_Lt6OBIBy3",
        "outputId": "9060b1d5-dba9-4cf6-c7d8-a2b4cb4b0b73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model...\n",
            "Epoch: 0001 | time: 3.4501s | Train loss: 0.7459 | Train micro F1: 0.4315 | Val loss: 0.5772 | Val micro F1: 0.4321\n",
            "Epoch: 0002 | time: 3.3759s | Train loss: 0.5651 | Train micro F1: 0.4604 | Val loss: 0.5357 | Val micro F1: 0.5081\n",
            "Epoch: 0003 | time: 3.8666s | Train loss: 0.5293 | Train micro F1: 0.4915 | Val loss: 0.5040 | Val micro F1: 0.5050\n",
            "Epoch: 0004 | time: 3.3955s | Train loss: 0.5094 | Train micro F1: 0.5162 | Val loss: 0.4932 | Val micro F1: 0.4869\n",
            "Epoch: 0005 | time: 3.3813s | Train loss: 0.5001 | Train micro F1: 0.5267 | Val loss: 0.4862 | Val micro F1: 0.4938\n",
            "Epoch: 0006 | time: 3.7261s | Train loss: 0.4899 | Train micro F1: 0.5410 | Val loss: 0.4776 | Val micro F1: 0.5501\n",
            "Epoch: 0007 | time: 3.6333s | Train loss: 0.4810 | Train micro F1: 0.5633 | Val loss: 0.4705 | Val micro F1: 0.5435\n",
            "Epoch: 0008 | time: 3.3986s | Train loss: 0.4705 | Train micro F1: 0.5753 | Val loss: 0.4633 | Val micro F1: 0.5884\n",
            "Epoch: 0009 | time: 3.4399s | Train loss: 0.4811 | Train micro F1: 0.5630 | Val loss: 0.4713 | Val micro F1: 0.5805\n",
            "Epoch: 0010 | time: 3.8965s | Train loss: 0.4659 | Train micro F1: 0.5857 | Val loss: 0.4566 | Val micro F1: 0.5858\n",
            "Epoch: 0011 | time: 3.4402s | Train loss: 0.4511 | Train micro F1: 0.6082 | Val loss: 0.4449 | Val micro F1: 0.5773\n",
            "Epoch: 0012 | time: 3.4483s | Train loss: 0.4393 | Train micro F1: 0.6228 | Val loss: 0.4352 | Val micro F1: 0.5826\n",
            "Epoch: 0013 | time: 3.7041s | Train loss: 0.4265 | Train micro F1: 0.6379 | Val loss: 0.4254 | Val micro F1: 0.6279\n",
            "Epoch: 0014 | time: 3.7747s | Train loss: 0.4205 | Train micro F1: 0.6516 | Val loss: 0.4360 | Val micro F1: 0.5553\n",
            "Epoch: 0015 | time: 3.4657s | Train loss: 0.4292 | Train micro F1: 0.6371 | Val loss: 0.4210 | Val micro F1: 0.6179\n",
            "Epoch: 0016 | time: 3.4442s | Train loss: 0.4120 | Train micro F1: 0.6562 | Val loss: 0.4082 | Val micro F1: 0.6324\n",
            "Epoch: 0017 | time: 3.9184s | Train loss: 0.3888 | Train micro F1: 0.6872 | Val loss: 0.3899 | Val micro F1: 0.6692\n",
            "Epoch: 0018 | time: 3.7559s | Train loss: 0.3729 | Train micro F1: 0.7051 | Val loss: 0.3759 | Val micro F1: 0.6901\n",
            "Epoch: 0019 | time: 3.4591s | Train loss: 0.3579 | Train micro F1: 0.7214 | Val loss: 0.3635 | Val micro F1: 0.7007\n",
            "Epoch: 0020 | time: 3.7289s | Train loss: 0.3531 | Train micro F1: 0.7262 | Val loss: 0.3605 | Val micro F1: 0.6910\n",
            "Epoch: 0021 | time: 3.7536s | Train loss: 0.3463 | Train micro F1: 0.7321 | Val loss: 0.3512 | Val micro F1: 0.7332\n",
            "Epoch: 0022 | time: 3.4465s | Train loss: 0.3326 | Train micro F1: 0.7494 | Val loss: 0.3430 | Val micro F1: 0.7320\n",
            "Epoch: 0023 | time: 3.4670s | Train loss: 0.3184 | Train micro F1: 0.7620 | Val loss: 0.3402 | Val micro F1: 0.7514\n",
            "Epoch: 0024 | time: 4.5712s | Train loss: 0.3124 | Train micro F1: 0.7697 | Val loss: 0.3251 | Val micro F1: 0.7358\n",
            "Epoch: 0025 | time: 3.4634s | Train loss: 0.3087 | Train micro F1: 0.7713 | Val loss: 0.3343 | Val micro F1: 0.7599\n",
            "Epoch: 0026 | time: 3.4521s | Train loss: 0.3043 | Train micro F1: 0.7780 | Val loss: 0.3129 | Val micro F1: 0.7604\n",
            "Epoch: 0027 | time: 3.7153s | Train loss: 0.2799 | Train micro F1: 0.7998 | Val loss: 0.2977 | Val micro F1: 0.7796\n",
            "Epoch: 0028 | time: 3.7308s | Train loss: 0.2687 | Train micro F1: 0.8103 | Val loss: 0.2901 | Val micro F1: 0.7901\n",
            "Epoch: 0029 | time: 3.4740s | Train loss: 0.2611 | Train micro F1: 0.8167 | Val loss: 0.2843 | Val micro F1: 0.7864\n",
            "Epoch: 0030 | time: 3.4866s | Train loss: 0.2730 | Train micro F1: 0.8050 | Val loss: 0.3151 | Val micro F1: 0.7453\n",
            "Epoch: 0031 | time: 3.9698s | Train loss: 0.2809 | Train micro F1: 0.7984 | Val loss: 0.2988 | Val micro F1: 0.7885\n",
            "Epoch: 0032 | time: 3.4638s | Train loss: 0.2593 | Train micro F1: 0.8195 | Val loss: 0.2768 | Val micro F1: 0.7959\n",
            "Epoch: 0033 | time: 3.4856s | Train loss: 0.2341 | Train micro F1: 0.8420 | Val loss: 0.2586 | Val micro F1: 0.8169\n",
            "Epoch: 0034 | time: 3.7223s | Train loss: 0.2171 | Train micro F1: 0.8566 | Val loss: 0.2496 | Val micro F1: 0.8250\n",
            "Epoch: 0035 | time: 3.8268s | Train loss: 0.2060 | Train micro F1: 0.8656 | Val loss: 0.2398 | Val micro F1: 0.8329\n",
            "Epoch: 0036 | time: 3.4804s | Train loss: 0.1968 | Train micro F1: 0.8725 | Val loss: 0.2361 | Val micro F1: 0.8352\n",
            "Epoch: 0037 | time: 3.4904s | Train loss: 0.1910 | Train micro F1: 0.8766 | Val loss: 0.2316 | Val micro F1: 0.8417\n",
            "Epoch: 0038 | time: 3.9682s | Train loss: 0.1943 | Train micro F1: 0.8729 | Val loss: 0.2443 | Val micro F1: 0.8220\n",
            "Epoch: 0039 | time: 3.4976s | Train loss: 0.1954 | Train micro F1: 0.8715 | Val loss: 0.2371 | Val micro F1: 0.8425\n",
            "Epoch: 0040 | time: 3.4760s | Train loss: 0.2189 | Train micro F1: 0.8514 | Val loss: 0.2464 | Val micro F1: 0.8338\n",
            "Epoch: 0041 | time: 4.0562s | Train loss: 0.2079 | Train micro F1: 0.8613 | Val loss: 0.2409 | Val micro F1: 0.8398\n",
            "Epoch: 0042 | time: 4.0966s | Train loss: 0.1931 | Train micro F1: 0.8737 | Val loss: 0.2296 | Val micro F1: 0.8429\n",
            "Epoch: 0043 | time: 3.4960s | Train loss: 0.1949 | Train micro F1: 0.8705 | Val loss: 0.2278 | Val micro F1: 0.8489\n",
            "Epoch: 0044 | time: 3.4783s | Train loss: 0.1819 | Train micro F1: 0.8819 | Val loss: 0.2160 | Val micro F1: 0.8585\n",
            "Epoch: 0045 | time: 3.8878s | Train loss: 0.1649 | Train micro F1: 0.8965 | Val loss: 0.2065 | Val micro F1: 0.8681\n",
            "Epoch: 0046 | time: 3.6120s | Train loss: 0.1537 | Train micro F1: 0.9050 | Val loss: 0.1970 | Val micro F1: 0.8746\n",
            "Epoch: 0047 | time: 3.7449s | Train loss: 0.1443 | Train micro F1: 0.9123 | Val loss: 0.1935 | Val micro F1: 0.8778\n",
            "Epoch: 0048 | time: 3.5520s | Train loss: 0.1369 | Train micro F1: 0.9176 | Val loss: 0.1886 | Val micro F1: 0.8811\n",
            "Epoch: 0049 | time: 3.9165s | Train loss: 0.1321 | Train micro F1: 0.9209 | Val loss: 0.1832 | Val micro F1: 0.8880\n",
            "Epoch: 0050 | time: 3.4793s | Train loss: 0.1296 | Train micro F1: 0.9223 | Val loss: 0.1854 | Val micro F1: 0.8876\n",
            "Epoch: 0051 | time: 3.4830s | Train loss: 0.1321 | Train micro F1: 0.9193 | Val loss: 0.1897 | Val micro F1: 0.8858\n",
            "Epoch: 0052 | time: 4.6763s | Train loss: 0.1345 | Train micro F1: 0.9174 | Val loss: 0.1887 | Val micro F1: 0.8801\n",
            "Epoch: 0053 | time: 3.5614s | Train loss: 0.1579 | Train micro F1: 0.8980 | Val loss: 0.2136 | Val micro F1: 0.8598\n",
            "Epoch: 0054 | time: 3.4910s | Train loss: 0.1615 | Train micro F1: 0.8969 | Val loss: 0.2116 | Val micro F1: 0.8678\n",
            "Epoch: 0055 | time: 3.5952s | Train loss: 0.1739 | Train micro F1: 0.8880 | Val loss: 0.2100 | Val micro F1: 0.8661\n",
            "Epoch: 0056 | time: 3.9449s | Train loss: 0.1467 | Train micro F1: 0.9078 | Val loss: 0.1925 | Val micro F1: 0.8824\n",
            "Epoch: 0057 | time: 3.4670s | Train loss: 0.1294 | Train micro F1: 0.9213 | Val loss: 0.1761 | Val micro F1: 0.8937\n",
            "Epoch: 0058 | time: 3.4851s | Train loss: 0.1193 | Train micro F1: 0.9286 | Val loss: 0.1681 | Val micro F1: 0.9016\n",
            "Epoch: 0059 | time: 3.9094s | Train loss: 0.1131 | Train micro F1: 0.9328 | Val loss: 0.1657 | Val micro F1: 0.9017\n",
            "Epoch: 0060 | time: 3.6264s | Train loss: 0.1106 | Train micro F1: 0.9341 | Val loss: 0.1648 | Val micro F1: 0.9027\n",
            "Epoch: 0061 | time: 3.4700s | Train loss: 0.1092 | Train micro F1: 0.9355 | Val loss: 0.1693 | Val micro F1: 0.8979\n",
            "Epoch: 0062 | time: 3.4904s | Train loss: 0.1068 | Train micro F1: 0.9368 | Val loss: 0.1658 | Val micro F1: 0.9019\n",
            "Epoch: 0063 | time: 4.0494s | Train loss: 0.1054 | Train micro F1: 0.9371 | Val loss: 0.1679 | Val micro F1: 0.9003\n",
            "Epoch: 0064 | time: 3.4886s | Train loss: 0.1017 | Train micro F1: 0.9397 | Val loss: 0.1563 | Val micro F1: 0.9100\n",
            "Epoch: 0065 | time: 3.5184s | Train loss: 0.1050 | Train micro F1: 0.9367 | Val loss: 0.1603 | Val micro F1: 0.9063\n",
            "Epoch: 0066 | time: 3.8401s | Train loss: 0.1010 | Train micro F1: 0.9399 | Val loss: 0.1606 | Val micro F1: 0.9076\n",
            "Epoch: 0067 | time: 3.6879s | Train loss: 0.1033 | Train micro F1: 0.9382 | Val loss: 0.1611 | Val micro F1: 0.9067\n",
            "Epoch: 0068 | time: 3.5010s | Train loss: 0.1023 | Train micro F1: 0.9384 | Val loss: 0.1624 | Val micro F1: 0.9092\n",
            "Epoch: 0069 | time: 3.4833s | Train loss: 0.1039 | Train micro F1: 0.9375 | Val loss: 0.1610 | Val micro F1: 0.9079\n",
            "Epoch: 0070 | time: 3.9739s | Train loss: 0.1070 | Train micro F1: 0.9348 | Val loss: 0.1679 | Val micro F1: 0.9050\n",
            "Epoch: 0071 | time: 3.5055s | Train loss: 0.1071 | Train micro F1: 0.9352 | Val loss: 0.1609 | Val micro F1: 0.9057\n",
            "Epoch: 0072 | time: 3.5118s | Train loss: 0.1056 | Train micro F1: 0.9357 | Val loss: 0.1739 | Val micro F1: 0.9019\n",
            "Epoch: 0073 | time: 3.7201s | Train loss: 0.1027 | Train micro F1: 0.9387 | Val loss: 0.1546 | Val micro F1: 0.9123\n",
            "Epoch: 0074 | time: 3.8105s | Train loss: 0.0931 | Train micro F1: 0.9449 | Val loss: 0.1515 | Val micro F1: 0.9170\n",
            "Epoch: 0075 | time: 3.4827s | Train loss: 0.0868 | Train micro F1: 0.9498 | Val loss: 0.1493 | Val micro F1: 0.9182\n",
            "Epoch: 0076 | time: 3.4738s | Train loss: 0.0831 | Train micro F1: 0.9524 | Val loss: 0.1475 | Val micro F1: 0.9198\n",
            "Epoch: 0077 | time: 4.0535s | Train loss: 0.0828 | Train micro F1: 0.9523 | Val loss: 0.1516 | Val micro F1: 0.9178\n",
            "Epoch: 0078 | time: 3.5096s | Train loss: 0.0838 | Train micro F1: 0.9514 | Val loss: 0.1487 | Val micro F1: 0.9186\n",
            "Epoch: 0079 | time: 3.4863s | Train loss: 0.0828 | Train micro F1: 0.9519 | Val loss: 0.1472 | Val micro F1: 0.9200\n",
            "Epoch: 0080 | time: 3.6867s | Train loss: 0.0816 | Train micro F1: 0.9526 | Val loss: 0.1467 | Val micro F1: 0.9188\n",
            "Epoch: 0081 | time: 4.0265s | Train loss: 0.0826 | Train micro F1: 0.9515 | Val loss: 0.1490 | Val micro F1: 0.9176\n",
            "Epoch: 0082 | time: 3.4840s | Train loss: 0.0841 | Train micro F1: 0.9502 | Val loss: 0.1530 | Val micro F1: 0.9135\n",
            "Epoch: 0083 | time: 3.9424s | Train loss: 0.0856 | Train micro F1: 0.9491 | Val loss: 0.1488 | Val micro F1: 0.9201\n",
            "Epoch: 0084 | time: 4.0770s | Train loss: 0.0992 | Train micro F1: 0.9398 | Val loss: 0.1792 | Val micro F1: 0.8964\n",
            "Epoch: 0085 | time: 3.4943s | Train loss: 0.1047 | Train micro F1: 0.9360 | Val loss: 0.1667 | Val micro F1: 0.9077\n",
            "Epoch: 0086 | time: 3.5059s | Train loss: 0.1095 | Train micro F1: 0.9345 | Val loss: 0.1605 | Val micro F1: 0.9092\n",
            "Epoch: 0087 | time: 3.7715s | Train loss: 0.0910 | Train micro F1: 0.9460 | Val loss: 0.1509 | Val micro F1: 0.9178\n",
            "Epoch: 0088 | time: 3.7803s | Train loss: 0.0841 | Train micro F1: 0.9499 | Val loss: 0.1415 | Val micro F1: 0.9243\n",
            "Epoch: 0089 | time: 3.4674s | Train loss: 0.0821 | Train micro F1: 0.9512 | Val loss: 0.1461 | Val micro F1: 0.9209\n",
            "Epoch: 0090 | time: 3.5222s | Train loss: 0.0816 | Train micro F1: 0.9513 | Val loss: 0.1440 | Val micro F1: 0.9229\n",
            "Epoch: 0091 | time: 3.9955s | Train loss: 0.0794 | Train micro F1: 0.9534 | Val loss: 0.1433 | Val micro F1: 0.9220\n",
            "Epoch: 0092 | time: 3.4863s | Train loss: 0.0759 | Train micro F1: 0.9562 | Val loss: 0.1383 | Val micro F1: 0.9260\n",
            "Epoch: 0093 | time: 3.4828s | Train loss: 0.0752 | Train micro F1: 0.9561 | Val loss: 0.1392 | Val micro F1: 0.9266\n",
            "Epoch: 0094 | time: 3.7058s | Train loss: 0.0738 | Train micro F1: 0.9570 | Val loss: 0.1417 | Val micro F1: 0.9260\n",
            "Epoch: 0095 | time: 3.8713s | Train loss: 0.0759 | Train micro F1: 0.9554 | Val loss: 0.1460 | Val micro F1: 0.9250\n",
            "Epoch: 0096 | time: 3.4744s | Train loss: 0.0839 | Train micro F1: 0.9495 | Val loss: 0.1403 | Val micro F1: 0.9269\n",
            "Epoch: 0097 | time: 3.4969s | Train loss: 0.0803 | Train micro F1: 0.9527 | Val loss: 0.1451 | Val micro F1: 0.9224\n",
            "Epoch: 0098 | time: 4.0328s | Train loss: 0.0769 | Train micro F1: 0.9550 | Val loss: 0.1443 | Val micro F1: 0.9233\n",
            "Epoch: 0099 | time: 3.5146s | Train loss: 0.0751 | Train micro F1: 0.9563 | Val loss: 0.1424 | Val micro F1: 0.9263\n",
            "Epoch: 0100 | time: 3.4863s | Train loss: 0.0706 | Train micro F1: 0.9594 | Val loss: 0.1385 | Val micro F1: 0.9301\n",
            "Epoch: 0101 | time: 3.5592s | Train loss: 0.0674 | Train micro F1: 0.9616 | Val loss: 0.1287 | Val micro F1: 0.9344\n",
            "Epoch: 0102 | time: 3.9349s | Train loss: 0.0613 | Train micro F1: 0.9656 | Val loss: 0.1234 | Val micro F1: 0.9369\n",
            "Epoch: 0103 | time: 3.4978s | Train loss: 0.0566 | Train micro F1: 0.9685 | Val loss: 0.1241 | Val micro F1: 0.9379\n",
            "Epoch: 0104 | time: 3.4776s | Train loss: 0.0584 | Train micro F1: 0.9680 | Val loss: 0.1270 | Val micro F1: 0.9371\n",
            "Epoch: 0105 | time: 3.9085s | Train loss: 0.0536 | Train micro F1: 0.9712 | Val loss: 0.1215 | Val micro F1: 0.9399\n",
            "Epoch: 0106 | time: 3.5798s | Train loss: 0.0497 | Train micro F1: 0.9735 | Val loss: 0.1179 | Val micro F1: 0.9434\n",
            "Epoch: 0107 | time: 3.4966s | Train loss: 0.0454 | Train micro F1: 0.9758 | Val loss: 0.1153 | Val micro F1: 0.9441\n",
            "Epoch: 0108 | time: 3.5762s | Train loss: 0.0431 | Train micro F1: 0.9775 | Val loss: 0.1155 | Val micro F1: 0.9448\n",
            "Epoch: 0109 | time: 3.9269s | Train loss: 0.0426 | Train micro F1: 0.9778 | Val loss: 0.1157 | Val micro F1: 0.9453\n",
            "Epoch: 0110 | time: 3.5246s | Train loss: 0.0418 | Train micro F1: 0.9779 | Val loss: 0.1198 | Val micro F1: 0.9431\n",
            "Epoch: 0111 | time: 3.5282s | Train loss: 0.0434 | Train micro F1: 0.9769 | Val loss: 0.1178 | Val micro F1: 0.9438\n",
            "Epoch: 0112 | time: 3.9292s | Train loss: 0.0448 | Train micro F1: 0.9758 | Val loss: 0.1182 | Val micro F1: 0.9438\n",
            "Epoch: 0113 | time: 3.6183s | Train loss: 0.0454 | Train micro F1: 0.9751 | Val loss: 0.1241 | Val micro F1: 0.9402\n",
            "Epoch: 0114 | time: 3.4925s | Train loss: 0.0486 | Train micro F1: 0.9729 | Val loss: 0.1249 | Val micro F1: 0.9404\n",
            "Epoch: 0115 | time: 3.4727s | Train loss: 0.0522 | Train micro F1: 0.9704 | Val loss: 0.1298 | Val micro F1: 0.9378\n",
            "Epoch: 0116 | time: 3.9794s | Train loss: 0.0529 | Train micro F1: 0.9702 | Val loss: 0.1248 | Val micro F1: 0.9408\n",
            "Epoch: 0117 | time: 3.4767s | Train loss: 0.0542 | Train micro F1: 0.9693 | Val loss: 0.1233 | Val micro F1: 0.9401\n",
            "Epoch: 0118 | time: 3.4917s | Train loss: 0.0513 | Train micro F1: 0.9713 | Val loss: 0.1225 | Val micro F1: 0.9411\n",
            "Epoch: 0119 | time: 3.7786s | Train loss: 0.0524 | Train micro F1: 0.9704 | Val loss: 0.1267 | Val micro F1: 0.9403\n",
            "Epoch: 0120 | time: 3.7849s | Train loss: 0.0482 | Train micro F1: 0.9730 | Val loss: 0.1195 | Val micro F1: 0.9438\n",
            "Epoch: 0121 | time: 3.5108s | Train loss: 0.0456 | Train micro F1: 0.9746 | Val loss: 0.1165 | Val micro F1: 0.9444\n",
            "Epoch: 0122 | time: 3.4668s | Train loss: 0.0416 | Train micro F1: 0.9774 | Val loss: 0.1180 | Val micro F1: 0.9458\n",
            "Epoch: 0123 | time: 4.0149s | Train loss: 0.0384 | Train micro F1: 0.9796 | Val loss: 0.1154 | Val micro F1: 0.9467\n",
            "Epoch: 0124 | time: 3.4824s | Train loss: 0.0353 | Train micro F1: 0.9817 | Val loss: 0.1090 | Val micro F1: 0.9504\n",
            "Epoch: 0125 | time: 3.4711s | Train loss: 0.0345 | Train micro F1: 0.9822 | Val loss: 0.1126 | Val micro F1: 0.9486\n",
            "Epoch: 0126 | time: 4.2029s | Train loss: 0.0344 | Train micro F1: 0.9825 | Val loss: 0.1142 | Val micro F1: 0.9486\n",
            "Epoch: 0127 | time: 4.0329s | Train loss: 0.0338 | Train micro F1: 0.9826 | Val loss: 0.1122 | Val micro F1: 0.9496\n",
            "Epoch: 0128 | time: 3.4829s | Train loss: 0.0336 | Train micro F1: 0.9826 | Val loss: 0.1127 | Val micro F1: 0.9500\n",
            "Epoch: 0129 | time: 3.4755s | Train loss: 0.0337 | Train micro F1: 0.9829 | Val loss: 0.1136 | Val micro F1: 0.9495\n",
            "Epoch: 0130 | time: 3.8152s | Train loss: 0.0321 | Train micro F1: 0.9836 | Val loss: 0.1148 | Val micro F1: 0.9490\n",
            "Epoch: 0131 | time: 3.6869s | Train loss: 0.0378 | Train micro F1: 0.9812 | Val loss: 0.1220 | Val micro F1: 0.9464\n",
            "Epoch: 0132 | time: 3.4899s | Train loss: 0.0391 | Train micro F1: 0.9811 | Val loss: 0.1164 | Val micro F1: 0.9481\n",
            "Epoch: 0133 | time: 3.4948s | Train loss: 0.0339 | Train micro F1: 0.9825 | Val loss: 0.1100 | Val micro F1: 0.9514\n",
            "Epoch: 0134 | time: 3.9601s | Train loss: 0.0362 | Train micro F1: 0.9807 | Val loss: 0.1147 | Val micro F1: 0.9492\n",
            "Epoch: 0135 | time: 3.5065s | Train loss: 0.0360 | Train micro F1: 0.9811 | Val loss: 0.1150 | Val micro F1: 0.9487\n",
            "Epoch: 0136 | time: 3.5297s | Train loss: 0.0372 | Train micro F1: 0.9797 | Val loss: 0.1187 | Val micro F1: 0.9469\n",
            "Epoch: 0137 | time: 3.7356s | Train loss: 0.0380 | Train micro F1: 0.9790 | Val loss: 0.1156 | Val micro F1: 0.9492\n",
            "Epoch: 0138 | time: 3.7846s | Train loss: 0.0410 | Train micro F1: 0.9773 | Val loss: 0.1210 | Val micro F1: 0.9466\n",
            "Epoch: 0139 | time: 3.4888s | Train loss: 0.0385 | Train micro F1: 0.9789 | Val loss: 0.1174 | Val micro F1: 0.9476\n",
            "Epoch: 0140 | time: 3.4775s | Train loss: 0.0372 | Train micro F1: 0.9795 | Val loss: 0.1204 | Val micro F1: 0.9468\n",
            "Epoch: 0141 | time: 4.0048s | Train loss: 0.0383 | Train micro F1: 0.9795 | Val loss: 0.1196 | Val micro F1: 0.9482\n",
            "Epoch: 0142 | time: 3.5126s | Train loss: 0.0349 | Train micro F1: 0.9815 | Val loss: 0.1162 | Val micro F1: 0.9507\n",
            "Epoch: 0143 | time: 3.4815s | Train loss: 0.0311 | Train micro F1: 0.9836 | Val loss: 0.1096 | Val micro F1: 0.9525\n",
            "Epoch: 0144 | time: 3.6745s | Train loss: 0.0299 | Train micro F1: 0.9847 | Val loss: 0.1120 | Val micro F1: 0.9526\n",
            "Epoch: 0145 | time: 3.9086s | Train loss: 0.0292 | Train micro F1: 0.9857 | Val loss: 0.1126 | Val micro F1: 0.9523\n",
            "Epoch: 0146 | time: 3.5543s | Train loss: 0.0266 | Train micro F1: 0.9866 | Val loss: 0.1079 | Val micro F1: 0.9544\n",
            "Epoch: 0147 | time: 3.5042s | Train loss: 0.0257 | Train micro F1: 0.9876 | Val loss: 0.1106 | Val micro F1: 0.9542\n",
            "Epoch: 0148 | time: 4.0660s | Train loss: 0.0244 | Train micro F1: 0.9882 | Val loss: 0.1086 | Val micro F1: 0.9555\n",
            "Epoch: 0149 | time: 3.5316s | Train loss: 0.0230 | Train micro F1: 0.9893 | Val loss: 0.1094 | Val micro F1: 0.9554\n",
            "Epoch: 0150 | time: 3.4777s | Train loss: 0.0227 | Train micro F1: 0.9892 | Val loss: 0.1107 | Val micro F1: 0.9554\n",
            "Epoch: 0151 | time: 3.6487s | Train loss: 0.0230 | Train micro F1: 0.9896 | Val loss: 0.1120 | Val micro F1: 0.9540\n",
            "Epoch: 0152 | time: 3.9495s | Train loss: 0.0222 | Train micro F1: 0.9895 | Val loss: 0.1132 | Val micro F1: 0.9544\n",
            "Epoch: 0153 | time: 3.4723s | Train loss: 0.0226 | Train micro F1: 0.9895 | Val loss: 0.1087 | Val micro F1: 0.9558\n",
            "Epoch: 0154 | time: 3.5016s | Train loss: 0.0238 | Train micro F1: 0.9885 | Val loss: 0.1113 | Val micro F1: 0.9550\n",
            "Epoch: 0155 | time: 3.9527s | Train loss: 0.0255 | Train micro F1: 0.9880 | Val loss: 0.1118 | Val micro F1: 0.9543\n",
            "Epoch: 0156 | time: 3.6201s | Train loss: 0.0246 | Train micro F1: 0.9877 | Val loss: 0.1123 | Val micro F1: 0.9548\n",
            "Epoch: 0157 | time: 3.4813s | Train loss: 0.0255 | Train micro F1: 0.9872 | Val loss: 0.1122 | Val micro F1: 0.9541\n",
            "Epoch: 0158 | time: 3.4859s | Train loss: 0.0275 | Train micro F1: 0.9856 | Val loss: 0.1156 | Val micro F1: 0.9532\n",
            "Epoch: 0159 | time: 3.9772s | Train loss: 0.0291 | Train micro F1: 0.9850 | Val loss: 0.1239 | Val micro F1: 0.9487\n",
            "Epoch: 0160 | time: 3.4803s | Train loss: 0.0320 | Train micro F1: 0.9834 | Val loss: 0.1224 | Val micro F1: 0.9503\n",
            "Epoch: 0161 | time: 3.4825s | Train loss: 0.0354 | Train micro F1: 0.9818 | Val loss: 0.1240 | Val micro F1: 0.9489\n",
            "Epoch: 0162 | time: 3.7818s | Train loss: 0.0363 | Train micro F1: 0.9805 | Val loss: 0.1221 | Val micro F1: 0.9501\n",
            "Epoch: 0163 | time: 3.7360s | Train loss: 0.0468 | Train micro F1: 0.9730 | Val loss: 0.1346 | Val micro F1: 0.9416\n",
            "Epoch: 0164 | time: 3.5095s | Train loss: 0.0475 | Train micro F1: 0.9730 | Val loss: 0.1457 | Val micro F1: 0.9361\n",
            "Epoch: 0165 | time: 3.5047s | Train loss: 0.0627 | Train micro F1: 0.9640 | Val loss: 0.1494 | Val micro F1: 0.9332\n",
            "Epoch: 0166 | time: 3.9929s | Train loss: 0.0733 | Train micro F1: 0.9574 | Val loss: 0.1737 | Val micro F1: 0.9177\n",
            "Epoch: 0167 | time: 3.4819s | Train loss: 0.0734 | Train micro F1: 0.9578 | Val loss: 0.1524 | Val micro F1: 0.9319\n",
            "Epoch: 0168 | time: 3.4910s | Train loss: 0.0940 | Train micro F1: 0.9464 | Val loss: 0.1760 | Val micro F1: 0.9169\n",
            "Epoch: 0169 | time: 3.7040s | Train loss: 0.0854 | Train micro F1: 0.9504 | Val loss: 0.1559 | Val micro F1: 0.9275\n",
            "Epoch: 0170 | time: 4.3786s | Train loss: 0.0670 | Train micro F1: 0.9614 | Val loss: 0.1411 | Val micro F1: 0.9356\n",
            "Epoch: 0171 | time: 3.5729s | Train loss: 0.0565 | Train micro F1: 0.9677 | Val loss: 0.1333 | Val micro F1: 0.9409\n",
            "Epoch: 0172 | time: 3.4707s | Train loss: 0.0475 | Train micro F1: 0.9732 | Val loss: 0.1298 | Val micro F1: 0.9436\n",
            "Epoch: 0173 | time: 3.7686s | Train loss: 0.0424 | Train micro F1: 0.9762 | Val loss: 0.1242 | Val micro F1: 0.9474\n",
            "Epoch: 0174 | time: 3.8444s | Train loss: 0.0359 | Train micro F1: 0.9806 | Val loss: 0.1179 | Val micro F1: 0.9511\n",
            "Epoch: 0175 | time: 3.5018s | Train loss: 0.0299 | Train micro F1: 0.9845 | Val loss: 0.1112 | Val micro F1: 0.9535\n",
            "Epoch: 0176 | time: 3.5252s | Train loss: 0.0254 | Train micro F1: 0.9873 | Val loss: 0.1107 | Val micro F1: 0.9559\n",
            "Epoch: 0177 | time: 4.0351s | Train loss: 0.0219 | Train micro F1: 0.9895 | Val loss: 0.1068 | Val micro F1: 0.9580\n",
            "Epoch: 0178 | time: 3.5020s | Train loss: 0.0201 | Train micro F1: 0.9907 | Val loss: 0.1060 | Val micro F1: 0.9594\n",
            "Epoch: 0179 | time: 3.5180s | Train loss: 0.0196 | Train micro F1: 0.9912 | Val loss: 0.1037 | Val micro F1: 0.9600\n",
            "Epoch: 0180 | time: 3.7214s | Train loss: 0.0195 | Train micro F1: 0.9913 | Val loss: 0.1080 | Val micro F1: 0.9582\n",
            "Epoch: 0181 | time: 3.8324s | Train loss: 0.0200 | Train micro F1: 0.9910 | Val loss: 0.1088 | Val micro F1: 0.9581\n",
            "Epoch: 0182 | time: 3.5288s | Train loss: 0.0199 | Train micro F1: 0.9914 | Val loss: 0.1089 | Val micro F1: 0.9586\n",
            "Epoch: 0183 | time: 3.4881s | Train loss: 0.0192 | Train micro F1: 0.9916 | Val loss: 0.1091 | Val micro F1: 0.9581\n",
            "Epoch: 0184 | time: 3.9782s | Train loss: 0.0191 | Train micro F1: 0.9914 | Val loss: 0.1107 | Val micro F1: 0.9583\n",
            "Epoch: 0185 | time: 3.6028s | Train loss: 0.0196 | Train micro F1: 0.9910 | Val loss: 0.1126 | Val micro F1: 0.9571\n",
            "Epoch: 0186 | time: 3.4972s | Train loss: 0.0200 | Train micro F1: 0.9907 | Val loss: 0.1131 | Val micro F1: 0.9576\n",
            "Epoch: 0187 | time: 3.5340s | Train loss: 0.0198 | Train micro F1: 0.9907 | Val loss: 0.1131 | Val micro F1: 0.9583\n",
            "Epoch: 0188 | time: 3.9408s | Train loss: 0.0207 | Train micro F1: 0.9907 | Val loss: 0.1134 | Val micro F1: 0.9570\n",
            "Epoch: 0189 | time: 3.4843s | Train loss: 0.0200 | Train micro F1: 0.9906 | Val loss: 0.1129 | Val micro F1: 0.9569\n",
            "Epoch: 0190 | time: 3.5260s | Train loss: 0.0196 | Train micro F1: 0.9908 | Val loss: 0.1143 | Val micro F1: 0.9567\n",
            "Epoch: 0191 | time: 3.8448s | Train loss: 0.0197 | Train micro F1: 0.9907 | Val loss: 0.1135 | Val micro F1: 0.9573\n",
            "Epoch: 0192 | time: 3.7059s | Train loss: 0.0200 | Train micro F1: 0.9910 | Val loss: 0.1110 | Val micro F1: 0.9577\n",
            "Epoch: 0193 | time: 3.5374s | Train loss: 0.0186 | Train micro F1: 0.9913 | Val loss: 0.1100 | Val micro F1: 0.9587\n",
            "Epoch: 0194 | time: 3.4983s | Train loss: 0.0188 | Train micro F1: 0.9913 | Val loss: 0.1124 | Val micro F1: 0.9583\n",
            "Epoch: 0195 | time: 3.9831s | Train loss: 0.0196 | Train micro F1: 0.9909 | Val loss: 0.1162 | Val micro F1: 0.9568\n",
            "Epoch: 0196 | time: 3.4963s | Train loss: 0.0204 | Train micro F1: 0.9905 | Val loss: 0.1151 | Val micro F1: 0.9566\n",
            "Epoch: 0197 | time: 3.5091s | Train loss: 0.0181 | Train micro F1: 0.9913 | Val loss: 0.1159 | Val micro F1: 0.9578\n",
            "Epoch: 0198 | time: 3.7449s | Train loss: 0.0185 | Train micro F1: 0.9914 | Val loss: 0.1121 | Val micro F1: 0.9583\n",
            "Epoch: 0199 | time: 3.8070s | Train loss: 0.0190 | Train micro F1: 0.9909 | Val loss: 0.1152 | Val micro F1: 0.9580\n",
            "Epoch: 0200 | time: 3.5378s | Train loss: 0.0213 | Train micro F1: 0.9906 | Val loss: 0.1172 | Val micro F1: 0.9567\n",
            "\n",
            "Total time elapsed: 739.8786s  |  Average time elapsed: 3.6994s\n",
            "\n",
            "Best model performance @ Epoch 0179: \n",
            "Test loss: 0.0580  |  Test micro F1: 0.9766\n"
          ]
        }
      ],
      "source": [
        "# test train_and_evaluate function on PPI dataset\n",
        "# (This is for the demonstration, you can run it. However running this cell will exceed the 8 minutes limits)\n",
        "ppi_train_params = {\n",
        "  \"lr\": 5e-3,\n",
        "  \"weight_decay\": 0,\n",
        "  \"epochs\": 200,\n",
        "  \"patience\": 100,\n",
        "  \"model_name\": 'GAT_ppi'\n",
        "}\n",
        "best_model, train_losses, train_scores, val_losses, val_scores = training_loop(model, ppi_train_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "A6QtmXMdBJxo",
        "outputId": "d46621d6-19bc-4233-846c-e48c4e413a1f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_f1_score(epoch_list, train_losses, val_scores):\n",
        "    plt.figure(figsize = [10,5])\n",
        "    plt.plot(epoch_list, train_losses, 'b', label = \"train Loss\")\n",
        "    plt.plot(epoch_list, val_scores, 'r', label = \"validation F1 score\")\n",
        "    plt.title(\"Evolution of validation F1 score and training Loss w.r.t epochs\")\n",
        "    plt.ylim([0.0, 1.0])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"F1-Score/Loss\")\n",
        "    plt.legend()\n",
        "    plt.savefig('/content/drive/My Drive/GAT_ppi_200_epoch.png')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "epoch_list = list(range(1, 201))\n",
        "plot_f1_score(epoch_list, train_losses, val_scores)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LRnMBgzx5gx"
      },
      "source": [
        "##  Evaluation\n",
        "### Metrics descriptions\n",
        "\n",
        "As mentioned in model descriptions, an early stopping strategy was used on both the cross-entropy loss and accuracy (transductive) or micro-F1 (inductive) score on the validation nodes, with a patience of 100 epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzeIaHGDx5gx"
      },
      "source": [
        "### Implementation code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJinNGC4Rg2b"
      },
      "source": [
        "For transductive learning GAT:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpR7zDMgPgas"
      },
      "outputs": [],
      "source": [
        "# demonstrate evaluate function as show in the model section\n",
        "def evaluate(model, criterion, input, target, mask):\n",
        "    # model = model.to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(*input)\n",
        "        output, target = output[mask], target[mask]\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "        acc = (output.argmax(dim=1) == target).float().sum() / len(target)\n",
        "\n",
        "    return loss.item(), acc.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUG4Bgmx8Hc_",
        "outputId": "411238c1-c82b-4a0e-9671-f6b98818cab6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set results: loss 1.6022 accuracy 0.7208\n"
          ]
        }
      ],
      "source": [
        "# demonstrate evaluate function on cora, the load model is not the best model\n",
        "state = torch.load('GAT_Cora')\n",
        "GAT_cora.load_state_dict(state['model_state_dict'])\n",
        "criterion = nn.NLLLoss()\n",
        "loss_test, acc_test = evaluate(GAT_cora, criterion, (features, adj_mat), labels, idx_test)\n",
        "print(f'Test set results: loss {loss_test:.4f} accuracy {acc_test:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhiAs-X1RlAe"
      },
      "source": [
        "For inductive learning GAT:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMC6douNPg8n"
      },
      "outputs": [],
      "source": [
        "# evaluate function as show in the model section\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "\n",
        "def evaluate(model, batch_loader):\n",
        "    model = model.to(device)\n",
        "\n",
        "    loss_fcn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    total_score = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for (batch_id, batched_graph) in enumerate(batch_loader):\n",
        "            node_features = batched_graph.x.to(device)\n",
        "            edge_index = batched_graph.edge_index.to(device)\n",
        "            labels = batched_graph.y.to(device)\n",
        "\n",
        "            logits = model(node_features, edge_index)\n",
        "            pred = (logits >= 0).float().cpu().numpy() # torch.where(logits >= 0, 1, 0)\n",
        "            loss = loss_fcn(logits, labels)\n",
        "            score = torch.tensor(f1_score(labels.cpu().numpy(), pred, average='micro'), dtype=torch.float32, device=device) #\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_score += score.item()\n",
        "\n",
        "    avg_loss = total_loss / (batch_id + 1)\n",
        "    avg_score = total_score / (batch_id + 1)\n",
        "\n",
        "    return avg_loss, avg_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zl2oOPtqx5gx",
        "outputId": "c1f04a67-0e18-4711-d177-693c9c820d13"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.057982299476861954, 0.9765520691871643)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# demonstrate evaluate function on PPI, the load model is the best model\n",
        "state = torch.load('GAT_ppi')\n",
        "model.load_state_dict(state['model_state_dict'])\n",
        "\n",
        "evaluate(model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gX6bCcZNuxmz"
      },
      "source": [
        "# Results\n",
        "\n",
        "\n",
        "###  Results\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXNm-SjfRUMo"
      },
      "source": [
        "For transductive learning:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sL0uvE34x5gy"
      },
      "source": [
        "In transductive learning, the GAT model was trained for 200 epochs over 10 runs using default hyperparameters on randomly split train/val/test data. The model achieved approximately 81.25% classification accuracy on the test split of the Cora Dataset and 72.82% classification accuracy on the test split of the Citeseer Dataset. These results are consistent with the performance reported in the original paper, supporting hypotheses 1 and 2. It is important to note that the variability in results can be attributed to the randomness of the train/val/test split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwE-6vfjLG6B"
      },
      "outputs": [],
      "source": [
        "def run_transductive_experiment_cora(iters=100):\n",
        "  losses = []\n",
        "  scores = []\n",
        "  t1 = time.time()\n",
        "  for iter in range(iters):\n",
        "    print(\"Run:\", iter+1)\n",
        "    best_model, loss_values, acc_values, loss_test, acc_test = train_and_evaluate(GAT_cora, (features, adj_mat), labels, idx_train, idx_val, args['epochs'], args['patience'], False)\n",
        "    losses.append(loss_test)\n",
        "    scores.append(acc_test)\n",
        "  losses = torch.tensor(losses)\n",
        "  scores = torch.tensor(scores)\n",
        "  loss_std, loss_mean = torch.std_mean(losses)\n",
        "  score_std, score_mean = torch.std_mean(scores)\n",
        "\n",
        "  # return (torch.std_mean(losses), torch.std_mean(scores))\n",
        "  print()\n",
        "  print(f\"Test Accuracy:  mean: {loss_mean:.4f} | std: {loss_std:.4f}\")\n",
        "  print(f\"Test Loss: mean: {score_mean:.4f} | std: {score_std:.4f}\")\n",
        "  print(f\"Total training time: {time.time()-t1:.4f} seconds | Average training time:  {(time.time()-t1)/iters:.4f} seconds\")\n",
        "  return losses, scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZBeSLXfLKHX"
      },
      "outputs": [],
      "source": [
        "# (This is for the demonstration, you can run it. However running this cell will exceed the 8 minutes limits)\n",
        "losses_cora, scores_cora = run_transductive_experiment_cora(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fD3nfrVrLLID",
        "outputId": "6c568aa4-2e03-4c7b-a2d9-00fa38482dda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GAT Cora test loss:\t\t1.4994 +/- 0.0108\n",
            "GAT Cora test accuracy: \t0.8303 +/- 0.0166\n"
          ]
        }
      ],
      "source": [
        "# this result is from cell output in pytorch_GAT_draft.ipynb\n",
        "loss_std_cora, loss_mean_cora = torch.std_mean(torch.tensor([ 1.4761, 1.4836, 1.5032, 1.5019, 1.5035, 1.5052, 1.4995, 1.5094, 1.5048, 1.5070 ]))\n",
        "score_std_cora, score_mean_cora = torch.std_mean(torch.tensor([ 0.8583, 0.8625, 0.8175, 0.8267, 0.8208, 0.8183, 0.8258, 0.8325, 0.8242, 0.8167]))\n",
        "\n",
        "print(f'GAT Cora test loss:\\t\\t{loss_mean_cora:.4f} +/- {loss_std_cora:.4f}')\n",
        "print(f'GAT Cora test accuracy: \\t{score_mean_cora:.4f} +/- {score_std_cora:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qm8woEM-3MT1"
      },
      "outputs": [],
      "source": [
        "def run_transductive_experiment_citeseer(iters=100):\n",
        "  losses = []\n",
        "  scores = []\n",
        "  for iter in range(iters):\n",
        "    best_model, loss_values, acc_values, loss_test, acc_test = train_and_evaluate(GAT_citeseer, (features, adj_mat), labels, idx_train, idx_val, args['epochs'], args['patience'], False)\n",
        "    losses.append(loss_test)\n",
        "    scores.append(acc_test)\n",
        "  losses = torch.tensor(losses)\n",
        "  scores = torch.tensor(scores)\n",
        "  # return (torch.std_mean(losses), torch.std_mean(scores))\n",
        "  return losses, scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkh73lZLEqRw"
      },
      "outputs": [],
      "source": [
        "# (This is for the demonstration, you can run it. However running this cell will exceed the 8 minutes limits)\n",
        "losses_citeseer, scores_citeseer = run_transductive_experiment_citeseer(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LSSpHgVIbpv",
        "outputId": "bf259207-c5dd-4851-8e81-072d38922f18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GAT Citeseer test loss:\t\t1.6309 +/- 0.0045\n",
            "GAT Citeseer test accuracy: \t0.7282 +/- 0.0043\n"
          ]
        }
      ],
      "source": [
        "# this result is from cell output in pytorch_GAT_draft.ipynb\n",
        "loss_std_citeseer, loss_mean_citeseer = torch.std_mean(torch.tensor([ 1.6249, 1.6336, 1.6267, 1.6395, 1.6273, 1.6290, 1.6277, 1.6322, 1.6351, 1.6327 ]))\n",
        "score_std_citeseer, score_mean_citeseer = torch.std_mean(torch.tensor([ 0.7290, 0.7330, 0.7320, 0.7230, 0.7320, 0.7200, 0.7280, 0.7300, 0.7300, 0.7250]))\n",
        "\n",
        "print(f'GAT Citeseer test loss:\\t\\t{loss_mean_citeseer:.4f} +/- {loss_std_citeseer:.4f}')\n",
        "print(f'GAT Citeseer test accuracy: \\t{score_mean_citeseer:.4f} +/- {score_std_citeseer:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "XnosmNcIJTEb",
        "outputId": "7e12afd7-66c8-4da4-9014-c7a19fd3f345"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Iteration #</th>\n",
              "      <th>GAT Cora Test Loss</th>\n",
              "      <th>GAT Cora Test Accuracy</th>\n",
              "      <th>GAT Citeseer Test Loss</th>\n",
              "      <th>GAT Citeseer Test Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.476100</td>\n",
              "      <td>0.858300</td>\n",
              "      <td>1.624900</td>\n",
              "      <td>0.729000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.483600</td>\n",
              "      <td>0.862500</td>\n",
              "      <td>1.633600</td>\n",
              "      <td>0.733000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.503200</td>\n",
              "      <td>0.817500</td>\n",
              "      <td>1.626700</td>\n",
              "      <td>0.732000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.501900</td>\n",
              "      <td>0.826700</td>\n",
              "      <td>1.639500</td>\n",
              "      <td>0.723000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.503500</td>\n",
              "      <td>0.820800</td>\n",
              "      <td>1.627300</td>\n",
              "      <td>0.732000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.505200</td>\n",
              "      <td>0.818300</td>\n",
              "      <td>1.629000</td>\n",
              "      <td>0.720000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.499500</td>\n",
              "      <td>0.825800</td>\n",
              "      <td>1.627700</td>\n",
              "      <td>0.728000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.509400</td>\n",
              "      <td>0.832500</td>\n",
              "      <td>1.632200</td>\n",
              "      <td>0.730000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.504800</td>\n",
              "      <td>0.824200</td>\n",
              "      <td>1.635100</td>\n",
              "      <td>0.730000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.507000</td>\n",
              "      <td>0.816700</td>\n",
              "      <td>1.632700</td>\n",
              "      <td>0.725000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>Mean</td>\n",
              "      <td>1.499420</td>\n",
              "      <td>0.830330</td>\n",
              "      <td>1.630870</td>\n",
              "      <td>0.728200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>Standard Deviation</td>\n",
              "      <td>0.010804</td>\n",
              "      <td>0.016597</td>\n",
              "      <td>0.004525</td>\n",
              "      <td>0.004264</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# this result is from cell output in pytorch_GAT_draft.ipynb\n",
        "from IPython.display import display, HTML\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'Iteration #' : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 'Mean', 'Standard Deviation'],\n",
        "                            'GAT Cora Test Loss' : [ 1.4761, 1.4836, 1.5032, 1.5019, 1.5035, 1.5052, 1.4995, 1.5094, 1.5048, 1.5070 , loss_mean_cora.item(), loss_std_cora.item() ],\n",
        "                            'GAT Cora Test Accuracy' : [ 0.8583, 0.8625, 0.8175, 0.8267, 0.8208, 0.8183, 0.8258, 0.8325, 0.8242, 0.8167, score_mean_cora.item(), score_std_cora.item()],\n",
        "                            'GAT Citeseer Test Loss' : [ 1.6249, 1.6336, 1.6267, 1.6395, 1.6273, 1.6290, 1.6277, 1.6322, 1.6351, 1.6327 , loss_mean_citeseer.item(), loss_std_citeseer.item() ],\n",
        "                            'GAT Citeseer Test Accuracy' : [ 0.7290, 0.7330, 0.7320, 0.7230, 0.7320, 0.7200, 0.7280, 0.7300, 0.7300, 0.7250, score_mean_citeseer.item(), score_std_citeseer.item()],\n",
        "                            })\n",
        "#display(df)\n",
        "display(HTML(df.to_html(index=False)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1EadUEPRaVe"
      },
      "source": [
        "For inductive learning:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iGO7ws5x5gy"
      },
      "source": [
        "In the case of inductive learning, after 10 runs of training for 200 epochs, the GAT model achieved an average of 97.51% +/- 0.16% classification micro F1 score on the test split, further supporting hypothesis 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFVbMBtXx5g2"
      },
      "outputs": [],
      "source": [
        "# function to run inductive experiment\n",
        "def run_inductive_experiment(iters=10):\n",
        "  losses = []\n",
        "  scores = []\n",
        "  t1 = time.time()\n",
        "  for iter in range(iters):\n",
        "    print(\"Run:\", iter+1)\n",
        "    best_model,_ ,_ ,_ ,_ = training_loop(model, ppi_train_params, verbose=False)\n",
        "    loss, score = evaluate(best_model, test_loader)\n",
        "    losses.append(loss)\n",
        "    scores.append(score)\n",
        "  losses = torch.tensor(losses)\n",
        "  scores = torch.tensor(scores)\n",
        "\n",
        "  print()\n",
        "  print(f\"Test micro F1 score:  mean: {loss_mean:.4f} | std: {loss_std:.4f}\")\n",
        "  print(f\"Test Loss: mean: {score_mean:.4f} | std: {score_std:.4f}\")\n",
        "  print(f\"Total training time: {time.time()-t1:.4f} seconds | Average training time:  {(time.time()-t1)/iters:.4f} seconds\")\n",
        "  return (torch.std_mean(losses), torch.std_mean(scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CA1VXeKt1k9f"
      },
      "outputs": [],
      "source": [
        "# (This is for the demonstration, you can run it. However running this cell will exceed the 8 minutes limits )\n",
        "loss_ci, score_ci = run_inductive_experiment(10)\n",
        "loss_std, loss_mean = loss_ci\n",
        "score_std, score_mean = score_ci\n",
        "\n",
        "print(f'loss:\\t\\t{loss_mean:.4f} +/- {loss_std:.4f}')\n",
        "print(f'micro F1 score: {score_mean:.4f} +/- {score_std:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgcUBu1I1p-b",
        "outputId": "51af431e-4668-453b-bfa6-e7d7f96170db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test loss:\t\t0.0629 +/- 0.0038\n",
            "test micro F1 score: \t0.9751 +/- 0.0016\n"
          ]
        }
      ],
      "source": [
        "# this result is from cell output in pytorch_GAT_draft.ipynb\n",
        "loss_std, loss_mean = torch.std_mean(torch.tensor([ 0.0619, 0.0638, 0.0563, 0.0717, 0.0623, 0.0619, 0.0622, 0.0632, 0.0618, 0.0639 ]))\n",
        "score_std, score_mean = torch.std_mean(torch.tensor([ 0.9765, 0.9735, 0.9773, 0.9724, 0.9751, 0.9761, 0.9745, 0.9770, 0.9740, 0.9751]))\n",
        "\n",
        "print(f'test loss:\\t\\t{loss_mean:.4f} +/- {loss_std:.4f}')\n",
        "print(f'test micro F1 score: \\t{score_mean:.4f} +/- {score_std:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "pZ8LEimV1y_I",
        "outputId": "68f3a62e-3cb8-4ca0-cd65-ce7287bda665"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Iteration #</th>\n",
              "      <th>GAT PPI Test Loss</th>\n",
              "      <th>GAT PPI Test micro F1 Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.061900</td>\n",
              "      <td>0.976500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.063800</td>\n",
              "      <td>0.973500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.056300</td>\n",
              "      <td>0.977300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.071700</td>\n",
              "      <td>0.972400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.062300</td>\n",
              "      <td>0.975100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.061900</td>\n",
              "      <td>0.976100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.062200</td>\n",
              "      <td>0.974500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.063200</td>\n",
              "      <td>0.977000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.061800</td>\n",
              "      <td>0.974000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.063900</td>\n",
              "      <td>0.975100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>Mean</td>\n",
              "      <td>0.062900</td>\n",
              "      <td>0.975150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>Standard Deviation</td>\n",
              "      <td>0.003756</td>\n",
              "      <td>0.001592</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# this result is from cell output in pytorch_GAT_draft.ipynb\n",
        "from IPython.display import display, HTML\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'Iteration #' : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 'Mean', 'Standard Deviation'],\n",
        "                   'GAT PPI Test Loss' : [ 0.0619, 0.0638, 0.0563, 0.0717, 0.0623, 0.0619, 0.0622, 0.0632, 0.0618, 0.0639, loss_mean.item(), loss_std.item() ],\n",
        "                   'GAT PPI Test micro F1 Score' : [0.9765, 0.9735, 0.9773, 0.9724, 0.9751, 0.9761, 0.9745, 0.9770, 0.9740, 0.9751, score_mean.item(), score_std.item()],})\n",
        "#display(df)\n",
        "display(HTML(df.to_html(index=False)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1Y3exoDGIBq"
      },
      "source": [
        "###  Analyses\n",
        "\n",
        "#### Model comparison\n",
        "In comparing transductive GAT with transductive GCN(in Pytorch GCN.ipynb), it is observed that GAT outperforms GCN in terms of accuracy (83.03% vs. 80.74%) but takes longer to train (28s vs. 10s). This tradeoff can lead to scalability issues when dealing with large graphs.\n",
        "\n",
        "It's important to carefully consider the tradeoffs between accuracy and training time when selecting a model, especially in scenarios involving large graphs.\n",
        "\n",
        "The reported results align closely with the obtained performance, with GAT achieving 83.03% on Cora and achieving 72.82% on Citeseer. The slight variation can be attributed to additional preprocessing steps in GCN, as well as differences in the size of the training, validation, and test sets.\n",
        "\n",
        "\n",
        "\n",
        "###  Plans\n",
        "\n",
        "\n",
        "Future plans include conducting an Ablation Study focusing on the model parameter that counts the number of patience as mentioned in the paper. Additionally, the comparison of GAT with GCN in a transductive learning setting is underway, with the coding part completed and the report pending finalization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EAWAy_LwHlV"
      },
      "source": [
        "##  Ablation Study：Model comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOdhGrbwwG71"
      },
      "outputs": [],
      "source": [
        "# compare you model with others\n",
        "# you don't need to re-run all other experiments, instead, you can directly refer the metrics/numbers in the paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH75TNU71eRH"
      },
      "source": [
        "# Discussion\n",
        "\n",
        "In this section,you should discuss your work and make future plan. The discussion should address the following questions:\n",
        "  * Make assessment that the paper is reproducible or not.\n",
        "  * Explain why it is not reproducible if your results are kind negative.\n",
        "  * Describe “What was easy” and “What was difficult” during the reproduction.\n",
        "  * Make suggestions to the author or other reproducers on how to improve the reproducibility.\n",
        "  * What will you do in next phase.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2VDXo5F4Frm"
      },
      "outputs": [],
      "source": [
        "# no code is required for this section\n",
        "'''\n",
        "if you want to use an image outside this notebook for explanaition,\n",
        "you can read and plot it here like the Scope of Reproducibility\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHMI2chl9omn"
      },
      "source": [
        "# References\n",
        "\n",
        "1.   Sun, J, [paper title], [journal title], [year], [volume]:[issue], doi: [doi link to paper]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmVuzQ724HbO"
      },
      "source": [
        "# Feel free to add new sections"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}