{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXwBbkhrumbn",
        "outputId": "b185ac58-660d-4ab7-fb75-9cf4aba84f1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHPeE-GkurBw",
        "outputId": "094d7864-e7b6-48f7-9da2-14f218ae0c6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ]
        }
      ],
      "source": [
        "# change the working directory to the Drive root\n",
        "%cd /content/drive/My\\ Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa4u35CZ_5rm",
        "outputId": "e9dc5c28-9e35-49d1-e10e-89b9f631db69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.5.2-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.9.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.4.0)\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxVTF2Qreqbi"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "def glorot(value, name=None):\n",
        "    \"\"\"Glorot & Bengio (AISTATS 2010) init.\"\"\"\n",
        "    init_range = np.sqrt(6.0 / (value.size(-2) + value.size(-1)))\n",
        "    initial = value.data.uniform_(-init_range, init_range)\n",
        "    return nn.Parameter(initial, requires_grad=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgVQlo2v_d-x"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.utils import softmax, scatter\n",
        "\n",
        "\n",
        "class GATLayer(nn.Module):\n",
        "\n",
        "  def __init__(self,\n",
        "               D_in: int,\n",
        "               D_out: int,\n",
        "               num_heads: int = 1,\n",
        "               act=F.elu,\n",
        "               dropout: float = 0.0,\n",
        "               reduce='none',\n",
        "               skip=False):\n",
        "    super().__init__()\n",
        "    self.D_in = D_in\n",
        "    self.D_out = D_out\n",
        "    self.N_h = num_heads\n",
        "    self.act = act\n",
        "\n",
        "    self.W = nn.Parameter(torch.zeros((num_heads, D_out, D_in)))\n",
        "    self.W_skip = nn.Parameter(torch.zeros((num_heads, D_out, D_in)))\n",
        "    self.A_src = nn.Parameter(torch.zeros((num_heads, D_out, 1)))\n",
        "    self.A_tgt = nn.Parameter(torch.zeros((num_heads, D_out, 1)))\n",
        "\n",
        "    self.reduce = reduce\n",
        "    self.dropout = dropout\n",
        "    self.skip = skip\n",
        "\n",
        "    self.reset_parameters()\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    glorot(self.W)\n",
        "    glorot(self.W_skip)\n",
        "    glorot(self.A_src)\n",
        "    glorot(self.A_tgt)\n",
        "\n",
        "\n",
        "  def forward(self, H_in: torch.tensor, edge_index: torch.tensor) -> torch.tensor:\n",
        "    #edge_src = edge_index[:1].t().squeeze()\n",
        "    #edge_tgt = edge_index[1:].t().squeeze()\n",
        "    edge_src, edge_tgt = edge_index\n",
        "\n",
        "    N, _ = H_in.shape\n",
        "    W = self.W\n",
        "    W_skip = self.W_skip\n",
        "\n",
        "    A_src = self.A_src\n",
        "    A_tgt = self.A_tgt\n",
        "    D_in = self.D_in\n",
        "    D_out = self.D_out\n",
        "    N_h = self.N_h\n",
        "    act = self.act\n",
        "    dropout = self.dropout\n",
        "    training = self.training\n",
        "    skip = self.skip\n",
        "\n",
        "    W = W.view((N_h, D_out, D_in))\n",
        "    W_skip = W.view((N_h, D_out, D_in))\n",
        "\n",
        "    A_src = A_src.view((N_h, D_out))\n",
        "    A_tgt = A_tgt.view((N_h, D_out))\n",
        "\n",
        "    H_in = F.dropout(H_in, dropout, training)\n",
        "\n",
        "    # H_w = torch.einsum('ij, nkj -> nik', H_in, W)  # (N_h, |V|, D_out)\n",
        "    # H_w = torch.matmul(H_in, W.transpose(1, 2)) # works too!\n",
        "    H_w = torch.matmul(H_in, W.permute(0, 2, 1)) # works too!\n",
        "\n",
        "    H_w = F.dropout(H_w, dropout, training)  # (N_h, |V|, D_out)\n",
        "\n",
        "    # H_w_src = torch.index_select(H_w, 1, edge_src)  # (N_h, |E|, D_out)\n",
        "    # H_w_tgt = torch.index_select(H_w, 1, edge_tgt)  # (N_h, |E|, D_out)\n",
        "    H_w_src = H_w[:, edge_src]\n",
        "    H_w_tgt = H_w[:, edge_tgt]\n",
        "\n",
        "    # E_pre_src = torch.einsum('nij, nj -> ni', H_w_src, A_src)  # (N_h, |E|, 1), a_src^T Whi\n",
        "    # E_pre_tgt = torch.einsum('nij, nj -> ni', H_w_tgt, A_tgt)  # (N_h, |E|, 1), a_tgt^T Whj\n",
        "    E_pre_src = torch.matmul(H_w_src, A_src.unsqueeze(-1)).squeeze()\n",
        "    E_pre_tgt = torch.matmul(H_w_tgt, A_tgt.unsqueeze(-1)).squeeze()\n",
        "\n",
        "    E_pre = E_pre_src + E_pre_tgt  # (N_h, |E|, 1), a^T [Whi || Whj]\n",
        "    E = F.leaky_relu(E_pre, negative_slope=0.2)  # (N_h, |E|, 1), LeakyRelu(a^T [Whi || Whj])\n",
        "\n",
        "    alpha_scores = softmax(E, edge_tgt, dim=1).view((N_h, *edge_src.shape, 1))  # (N_h, |E|, 1)\n",
        "    # Manually implement softmax\n",
        "    # alpha_scores = torch.exp(E - torch.max(E, dim=1, keepdim=True)[0])\n",
        "\n",
        "    alpha_scores = F.dropout(alpha_scores, dropout, training)\n",
        "    Alpha = alpha_scores.repeat(1, 1, D_out)  # (N_h, |E|, D_out)\n",
        "\n",
        "    self.attention_scores = alpha_scores\n",
        "\n",
        "    H_out_pre = scatter(Alpha * H_w_src, edge_tgt, dim=1, reduce='sum')  # (N_h, |V|, D_out)\n",
        "\n",
        "    if skip:\n",
        "      H_skip_to_add = torch.zeros_like(H_out_pre)\n",
        "      if D_in != D_out:\n",
        "        # H_skip : (|V|, D_in)\n",
        "        # W_skip : (N_h, D_out, D_in)\n",
        "        # H_skip_add : (N_h, |V|, D_out)\n",
        "        # H_skip_to_add = torch.einsum('ij, nkj -> nik', H_in, W_skip)\n",
        "        H_skip_to_add = torch.matmul(H_in, W_skip.transpose(1, 2))\n",
        "      else:\n",
        "        # H_skip_add : (N_h, |V|, D_out)\n",
        "        H_skip_to_add = H_in.repeat(N_h, 1, 1)\n",
        "\n",
        "      H_out_pre += H_skip_to_add\n",
        "\n",
        "    if self.reduce == 'none':\n",
        "      H_out = act(H_out_pre)  # (N_h, |V|, D_out)\n",
        "      assert (H_out.shape == (N_h, N, D_out))\n",
        "      return H_out\n",
        "    elif self.reduce == 'concat':\n",
        "      H_out = act(H_out_pre)\n",
        "      H_out_per_head = torch.tensor_split(H_out, N_h)\n",
        "      H_out_cat = torch.cat(H_out_per_head, dim=-1).squeeze()\n",
        "      self.embeddings = H_out_cat\n",
        "      assert (H_out_cat.shape == (N, N_h * D_out))\n",
        "      return H_out_cat\n",
        "    else:\n",
        "      H_out_pre_avg = torch.mean(H_out_pre, dim=0)\n",
        "      H_out = act(H_out_pre_avg)\n",
        "      self.embeddings = H_out\n",
        "      assert (H_out.shape == (N, D_out))\n",
        "      return H_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2napXIhAeFT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dCmFDQTtCZH"
      },
      "outputs": [],
      "source": [
        "# evaluate\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "\n",
        "def evaluate(model, batch_loader):\n",
        "    model = model.to(device)\n",
        "\n",
        "    loss_fcn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    total_score = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for (batch_id, batched_graph) in enumerate(batch_loader):\n",
        "            node_features = batched_graph.x.to(device)\n",
        "            edge_index = batched_graph.edge_index.to(device)\n",
        "            labels = batched_graph.y.to(device)\n",
        "\n",
        "            logits = model(node_features, edge_index)\n",
        "            pred = (logits >= 0).float().cpu().numpy() # torch.where(logits >= 0, 1, 0)\n",
        "            loss = loss_fcn(logits, labels)\n",
        "            score = torch.tensor(f1_score(labels.cpu().numpy(), pred, average='micro'), dtype=torch.float32, device=device) #\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_score += score.item()\n",
        "\n",
        "    avg_loss = total_loss / (batch_id + 1)\n",
        "    avg_score = total_score / (batch_id + 1)\n",
        "\n",
        "    return avg_loss, avg_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjpgG1uL_bOP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "class GAT_PPI(nn.Module):\n",
        "\n",
        "  def __init__(self, dim_in: int, num_classes: int):\n",
        "    super().__init__()\n",
        "    self.model_name = 'GAT_PPI'\n",
        "    # assumes self loops were added by dataset transform\n",
        "    self.layers = nn.ModuleList([\n",
        "        GATLayer(dim_in, 256, 4, act=F.elu, reduce='concat'),\n",
        "        GATLayer(1024, 256, 4, act=F.elu, reduce='concat', skip=True),\n",
        "        GATLayer(1024, num_classes, 6, act=nn.Identity(), reduce='avg', skip=True)\n",
        "    ])\n",
        "    self.reset_parameters()\n",
        "\n",
        "  def forward(self, X: torch.tensor, edge_index: torch.tensor) -> torch.tensor:\n",
        "    out = X\n",
        "    for layer in self.layers:\n",
        "      out = layer(out, edge_index)\n",
        "    return out\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    for layer in self.layers:\n",
        "      layer.reset_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KT6bfaX_srH3"
      },
      "outputs": [],
      "source": [
        "# train\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def train(model, params: dict, verbose: bool = True) -> torch.nn.Module:\n",
        "    print('training model {}'.format(params['model_name']))\n",
        "    print(model)\n",
        "    print('training...')\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
        "    model.reset_parameters()\n",
        "    batch_criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    best_f1 = -1\n",
        "    best_loss = pow(10, 9)\n",
        "    patience = params['patience']\n",
        "    underperformed = 0\n",
        "\n",
        "    train_losses, train_scores = [], []\n",
        "    val_losses, val_scores = [], []\n",
        "\n",
        "    best_model_state_dict = None\n",
        "    best_optimizer_state_dict = None\n",
        "    best_epoch = 0\n",
        "\n",
        "    for epoch in range(params['epochs']):\n",
        "        if underperformed >= patience:\n",
        "            print('early stopping...')\n",
        "            break\n",
        "\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        total_train_score = 0\n",
        "\n",
        "        for (batch_ix, batch) in enumerate(train_loader):\n",
        "            node_features, edge_index, labels = (batch.x.to(device), batch.edge_index.to(device), batch.y.to(device))\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(node_features, edge_index)\n",
        "            pred = torch.where(logits >= 0, 1, 0)\n",
        "            loss = batch_criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "            pred_np = pred.cpu().numpy()\n",
        "            labels_np = labels.cpu().numpy()\n",
        "            f1 = f1_score(labels_np, pred_np, average='micro')\n",
        "            total_train_score += torch.tensor(f1, dtype=torch.float32, device=device).item()\n",
        "\n",
        "        avg_train_loss = total_train_loss / (batch_ix + 1)\n",
        "        avg_train_score = total_train_score / (batch_ix + 1)\n",
        "\n",
        "        val_loss, val_score = evaluate(model, val_loader)\n",
        "\n",
        "        train_losses.append(avg_train_loss)\n",
        "        train_scores.append(avg_train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        val_scores.append(val_score)\n",
        "\n",
        "        if (val_score > best_f1 or val_loss < best_loss):\n",
        "            best_f1 = max(best_f1, val_score)\n",
        "            best_loss = min(best_loss, val_loss)\n",
        "            underperformed = 0\n",
        "            was_best_so_far = True\n",
        "            best_model_state_dict = model.state_dict()\n",
        "            best_optimizer_state_dict = optimizer.state_dict()\n",
        "            best_epoch = epoch\n",
        "        else:\n",
        "            underperformed += 1\n",
        "            was_best_so_far = False\n",
        "\n",
        "        if was_best_so_far:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': best_model_state_dict,\n",
        "                'optimizer_state_dict': best_optimizer_state_dict,\n",
        "            }, params['model_name'])\n",
        "\n",
        "        if verbose:\n",
        "            print('epoch {:05d}'.format(epoch + 1))\n",
        "            print('\\t{}_loss: {:.4f} | {}_micro_f1: {:.4f}'.format('train', avg_train_loss,'train', avg_train_score))\n",
        "            print('\\t{}_loss: {:.4f} | {}_micro_f1: {:.4f}'.format('val', val_loss,'val', val_score))\n",
        "\n",
        "    state = torch.load(params['model_name'])\n",
        "    model.load_state_dict(state['model_state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    best_epoch, best_model = state['epoch'], model\n",
        "    best_val_loss, best_val_score = evaluate(model, val_loader)\n",
        "    best_test_loss, best_test_score = evaluate(model, test_loader)\n",
        "\n",
        "    print('best model performance @ epoch {:05d}: '.format(best_epoch))\n",
        "    print('\\t{}_loss: {:.4f} | {}_micro_f1: {:.4f}'.format('val', best_val_loss, 'val', best_val_score))\n",
        "    print('\\t{}_loss: {:.4f} | {}_micro_f1: {:.4f}'.format('test', best_test_loss,'test', best_test_score))\n",
        "\n",
        "    return best_model, train_losses, train_scores, val_losses, val_scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Whow_SbCHZyl",
        "outputId": "c01fb700-b47f-4e26-c87e-a2166159b19b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f0d86047130>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiMEPM-4Hqjs"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.datasets import PPI\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.transforms import AddSelfLoops\n",
        "\n",
        "train_dataset = PPI(root='', split='train', transform=AddSelfLoops())\n",
        "val_dataset = PPI(root='', split='val', transform=AddSelfLoops())\n",
        "test_dataset = PPI(root='', split='test', transform=AddSelfLoops())\n",
        "\n",
        "num_features = 50\n",
        "num_labels = 121\n",
        "train_loader = DataLoader(train_dataset, batch_size=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHRpch3OH9zs"
      },
      "outputs": [],
      "source": [
        "model = GAT_PPI(num_features, num_labels)\n",
        "# evaluate(model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTg5kzakyV4s"
      },
      "source": [
        "Demonstate the evaluation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ebVDlN7wwu6",
        "outputId": "671dee05-97e8-4115-8a03-57cdd31c8035"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.7045281529426575, 0.3901699185371399)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate(model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSpeXusLyfnN"
      },
      "source": [
        "Demonstate the train function:\n",
        "\n",
        "- If you run this using Google Hardware accelerator T4 GPU, then just set the epochs to 2 or 3, , from my testing it will cost less than 2 mintues.\n",
        "- If you run this using Google Hardware accelerator CPU, then just set the epochs to 1, from my testing it will cost aproxmately 8 mintues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "_deSisDCx84p",
        "outputId": "21bec81b-fef0-4b56-f48f-fb20adfcef63"
      },
      "outputs": [],
      "source": [
        "ppi_train_params_demo = {\n",
        "  \"lr\": 5e-3,\n",
        "  \"weight_decay\": 0,\n",
        "  \"epochs\": 3,\n",
        "  \"patience\": 100,\n",
        "  \"model_name\": model.model_name\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "903QTKmVypjN",
        "outputId": "95dcc7bc-0e31-49c9-e21e-9be5bd9d38a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training model GAT_PPI\n",
            "GAT_PPI(\n",
            "  (layers): ModuleList(\n",
            "    (0-1): 2 x GATLayer()\n",
            "    (2): GATLayer(\n",
            "      (act): Identity()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "training...\n",
            "epoch 00001\n",
            "\ttrain_loss: 0.7525 | train_micro_f1: 0.4247\n",
            "\tval_loss: 0.5892 | val_micro_f1: 0.4316\n",
            "epoch 00002\n",
            "\ttrain_loss: 0.5561 | train_micro_f1: 0.4739\n",
            "\tval_loss: 0.5164 | val_micro_f1: 0.5011\n",
            "epoch 00003\n",
            "\ttrain_loss: 0.5177 | train_micro_f1: 0.4984\n",
            "\tval_loss: 0.5001 | val_micro_f1: 0.4951\n",
            "best model performance @ epoch 00002: \n",
            "\tval_loss: 0.5001 | val_micro_f1: 0.4951\n",
            "\ttest_loss: 0.5004 | test_micro_f1: 0.5033\n"
          ]
        }
      ],
      "source": [
        "best_model,_ ,_ ,_ ,_ = train(model, ppi_train_params_demo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGUj-w16IS6z"
      },
      "outputs": [],
      "source": [
        "ppi_train_params = {\n",
        "  \"lr\": 5e-3,\n",
        "  \"weight_decay\": 0,\n",
        "  \"epochs\": 200,\n",
        "  \"patience\": 100,\n",
        "  \"model_name\": model.model_name\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WdD_b56dH8k",
        "outputId": "ddd22568-f40b-4c2c-8d62-dc0cc52112e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training model GAT_PPI\n",
            "GAT_PPI(\n",
            "  (layers): ModuleList(\n",
            "    (0-1): 2 x GATLayer()\n",
            "    (2): GATLayer(\n",
            "      (act): Identity()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "training...\n",
            "epoch 00001\n",
            "\ttrain_loss: 0.7107 | train_micro_f1: 0.4231\n",
            "\tval_loss: 0.5889 | val_micro_f1: 0.4715\n",
            "epoch 00002\n",
            "\ttrain_loss: 0.5649 | train_micro_f1: 0.4704\n",
            "\tval_loss: 0.5278 | val_micro_f1: 0.4548\n",
            "epoch 00003\n",
            "\ttrain_loss: 0.5263 | train_micro_f1: 0.4894\n",
            "\tval_loss: 0.5038 | val_micro_f1: 0.4694\n",
            "epoch 00004\n",
            "\ttrain_loss: 0.5089 | train_micro_f1: 0.5098\n",
            "\tval_loss: 0.4918 | val_micro_f1: 0.5014\n",
            "epoch 00005\n",
            "\ttrain_loss: 0.4954 | train_micro_f1: 0.5326\n",
            "\tval_loss: 0.4842 | val_micro_f1: 0.5005\n",
            "epoch 00006\n",
            "\ttrain_loss: 0.4843 | train_micro_f1: 0.5526\n",
            "\tval_loss: 0.4717 | val_micro_f1: 0.5428\n",
            "epoch 00007\n",
            "\ttrain_loss: 0.4784 | train_micro_f1: 0.5651\n",
            "\tval_loss: 0.4690 | val_micro_f1: 0.5764\n",
            "epoch 00008\n",
            "\ttrain_loss: 0.4661 | train_micro_f1: 0.5865\n",
            "\tval_loss: 0.4620 | val_micro_f1: 0.5312\n",
            "epoch 00009\n",
            "\ttrain_loss: 0.4614 | train_micro_f1: 0.5891\n",
            "\tval_loss: 0.4518 | val_micro_f1: 0.5982\n",
            "epoch 00010\n",
            "\ttrain_loss: 0.4546 | train_micro_f1: 0.6045\n",
            "\tval_loss: 0.4460 | val_micro_f1: 0.5668\n",
            "epoch 00011\n",
            "\ttrain_loss: 0.4443 | train_micro_f1: 0.6171\n",
            "\tval_loss: 0.4410 | val_micro_f1: 0.5706\n",
            "epoch 00012\n",
            "\ttrain_loss: 0.4341 | train_micro_f1: 0.6289\n",
            "\tval_loss: 0.4293 | val_micro_f1: 0.5928\n",
            "epoch 00013\n",
            "\ttrain_loss: 0.4209 | train_micro_f1: 0.6431\n",
            "\tval_loss: 0.4158 | val_micro_f1: 0.6505\n",
            "epoch 00014\n",
            "\ttrain_loss: 0.4094 | train_micro_f1: 0.6613\n",
            "\tval_loss: 0.4067 | val_micro_f1: 0.6578\n",
            "epoch 00015\n",
            "\ttrain_loss: 0.3932 | train_micro_f1: 0.6810\n",
            "\tval_loss: 0.3935 | val_micro_f1: 0.6670\n",
            "epoch 00016\n",
            "\ttrain_loss: 0.3822 | train_micro_f1: 0.6956\n",
            "\tval_loss: 0.3911 | val_micro_f1: 0.6443\n",
            "epoch 00017\n",
            "\ttrain_loss: 0.3777 | train_micro_f1: 0.6944\n",
            "\tval_loss: 0.3935 | val_micro_f1: 0.7041\n",
            "epoch 00018\n",
            "\ttrain_loss: 0.3826 | train_micro_f1: 0.6956\n",
            "\tval_loss: 0.3804 | val_micro_f1: 0.7039\n",
            "epoch 00019\n",
            "\ttrain_loss: 0.3635 | train_micro_f1: 0.7187\n",
            "\tval_loss: 0.3643 | val_micro_f1: 0.6997\n",
            "epoch 00020\n",
            "\ttrain_loss: 0.3433 | train_micro_f1: 0.7371\n",
            "\tval_loss: 0.3502 | val_micro_f1: 0.7127\n",
            "epoch 00021\n",
            "\ttrain_loss: 0.3304 | train_micro_f1: 0.7502\n",
            "\tval_loss: 0.3400 | val_micro_f1: 0.7241\n",
            "epoch 00022\n",
            "\ttrain_loss: 0.3249 | train_micro_f1: 0.7566\n",
            "\tval_loss: 0.3400 | val_micro_f1: 0.7251\n",
            "epoch 00023\n",
            "\ttrain_loss: 0.3136 | train_micro_f1: 0.7691\n",
            "\tval_loss: 0.3250 | val_micro_f1: 0.7419\n",
            "epoch 00024\n",
            "\ttrain_loss: 0.2941 | train_micro_f1: 0.7874\n",
            "\tval_loss: 0.3081 | val_micro_f1: 0.7748\n",
            "epoch 00025\n",
            "\ttrain_loss: 0.2894 | train_micro_f1: 0.7913\n",
            "\tval_loss: 0.3092 | val_micro_f1: 0.7476\n",
            "epoch 00026\n",
            "\ttrain_loss: 0.2766 | train_micro_f1: 0.8034\n",
            "\tval_loss: 0.2926 | val_micro_f1: 0.7916\n",
            "epoch 00027\n",
            "\ttrain_loss: 0.2618 | train_micro_f1: 0.8181\n",
            "\tval_loss: 0.2823 | val_micro_f1: 0.7944\n",
            "epoch 00028\n",
            "\ttrain_loss: 0.2474 | train_micro_f1: 0.8311\n",
            "\tval_loss: 0.2714 | val_micro_f1: 0.8062\n",
            "epoch 00029\n",
            "\ttrain_loss: 0.2347 | train_micro_f1: 0.8422\n",
            "\tval_loss: 0.2657 | val_micro_f1: 0.8107\n",
            "epoch 00030\n",
            "\ttrain_loss: 0.2290 | train_micro_f1: 0.8450\n",
            "\tval_loss: 0.2806 | val_micro_f1: 0.8126\n",
            "epoch 00031\n",
            "\ttrain_loss: 0.2522 | train_micro_f1: 0.8240\n",
            "\tval_loss: 0.3046 | val_micro_f1: 0.7536\n",
            "epoch 00032\n",
            "\ttrain_loss: 0.2704 | train_micro_f1: 0.8084\n",
            "\tval_loss: 0.2760 | val_micro_f1: 0.8051\n",
            "epoch 00033\n",
            "\ttrain_loss: 0.2374 | train_micro_f1: 0.8380\n",
            "\tval_loss: 0.2623 | val_micro_f1: 0.8161\n",
            "epoch 00034\n",
            "\ttrain_loss: 0.2157 | train_micro_f1: 0.8575\n",
            "\tval_loss: 0.2462 | val_micro_f1: 0.8335\n",
            "epoch 00035\n",
            "\ttrain_loss: 0.2045 | train_micro_f1: 0.8662\n",
            "\tval_loss: 0.2389 | val_micro_f1: 0.8372\n",
            "epoch 00036\n",
            "\ttrain_loss: 0.2005 | train_micro_f1: 0.8684\n",
            "\tval_loss: 0.2358 | val_micro_f1: 0.8397\n",
            "epoch 00037\n",
            "\ttrain_loss: 0.1924 | train_micro_f1: 0.8754\n",
            "\tval_loss: 0.2312 | val_micro_f1: 0.8465\n",
            "epoch 00038\n",
            "\ttrain_loss: 0.1867 | train_micro_f1: 0.8797\n",
            "\tval_loss: 0.2237 | val_micro_f1: 0.8551\n",
            "epoch 00039\n",
            "\ttrain_loss: 0.1788 | train_micro_f1: 0.8859\n",
            "\tval_loss: 0.2158 | val_micro_f1: 0.8624\n",
            "epoch 00040\n",
            "\ttrain_loss: 0.1798 | train_micro_f1: 0.8839\n",
            "\tval_loss: 0.2244 | val_micro_f1: 0.8564\n",
            "epoch 00041\n",
            "\ttrain_loss: 0.1744 | train_micro_f1: 0.8891\n",
            "\tval_loss: 0.2162 | val_micro_f1: 0.8631\n",
            "epoch 00042\n",
            "\ttrain_loss: 0.1738 | train_micro_f1: 0.8883\n",
            "\tval_loss: 0.2117 | val_micro_f1: 0.8651\n",
            "epoch 00043\n",
            "\ttrain_loss: 0.1853 | train_micro_f1: 0.8790\n",
            "\tval_loss: 0.2315 | val_micro_f1: 0.8382\n",
            "epoch 00044\n",
            "\ttrain_loss: 0.2140 | train_micro_f1: 0.8560\n",
            "\tval_loss: 0.2513 | val_micro_f1: 0.8308\n",
            "epoch 00045\n",
            "\ttrain_loss: 0.1996 | train_micro_f1: 0.8689\n",
            "\tval_loss: 0.2261 | val_micro_f1: 0.8482\n",
            "epoch 00046\n",
            "\ttrain_loss: 0.1718 | train_micro_f1: 0.8911\n",
            "\tval_loss: 0.2027 | val_micro_f1: 0.8694\n",
            "epoch 00047\n",
            "\ttrain_loss: 0.1549 | train_micro_f1: 0.9037\n",
            "\tval_loss: 0.1936 | val_micro_f1: 0.8771\n",
            "epoch 00048\n",
            "\ttrain_loss: 0.1438 | train_micro_f1: 0.9119\n",
            "\tval_loss: 0.1873 | val_micro_f1: 0.8847\n",
            "epoch 00049\n",
            "\ttrain_loss: 0.1373 | train_micro_f1: 0.9164\n",
            "\tval_loss: 0.1849 | val_micro_f1: 0.8870\n",
            "epoch 00050\n",
            "\ttrain_loss: 0.1353 | train_micro_f1: 0.9174\n",
            "\tval_loss: 0.1878 | val_micro_f1: 0.8846\n",
            "epoch 00051\n",
            "\ttrain_loss: 0.1368 | train_micro_f1: 0.9160\n",
            "\tval_loss: 0.1864 | val_micro_f1: 0.8860\n",
            "epoch 00052\n",
            "\ttrain_loss: 0.1427 | train_micro_f1: 0.9112\n",
            "\tval_loss: 0.1913 | val_micro_f1: 0.8798\n",
            "epoch 00053\n",
            "\ttrain_loss: 0.1422 | train_micro_f1: 0.9115\n",
            "\tval_loss: 0.1857 | val_micro_f1: 0.8851\n",
            "epoch 00054\n",
            "\ttrain_loss: 0.1322 | train_micro_f1: 0.9194\n",
            "\tval_loss: 0.1765 | val_micro_f1: 0.8929\n",
            "epoch 00055\n",
            "\ttrain_loss: 0.1223 | train_micro_f1: 0.9265\n",
            "\tval_loss: 0.1670 | val_micro_f1: 0.9015\n",
            "epoch 00056\n",
            "\ttrain_loss: 0.1150 | train_micro_f1: 0.9319\n",
            "\tval_loss: 0.1658 | val_micro_f1: 0.9022\n",
            "epoch 00057\n",
            "\ttrain_loss: 0.1103 | train_micro_f1: 0.9350\n",
            "\tval_loss: 0.1645 | val_micro_f1: 0.9042\n",
            "epoch 00058\n",
            "\ttrain_loss: 0.1072 | train_micro_f1: 0.9373\n",
            "\tval_loss: 0.1601 | val_micro_f1: 0.9076\n",
            "epoch 00059\n",
            "\ttrain_loss: 0.1088 | train_micro_f1: 0.9354\n",
            "\tval_loss: 0.1649 | val_micro_f1: 0.9059\n",
            "epoch 00060\n",
            "\ttrain_loss: 0.1070 | train_micro_f1: 0.9369\n",
            "\tval_loss: 0.1608 | val_micro_f1: 0.9083\n",
            "epoch 00061\n",
            "\ttrain_loss: 0.1098 | train_micro_f1: 0.9346\n",
            "\tval_loss: 0.1601 | val_micro_f1: 0.9086\n",
            "epoch 00062\n",
            "\ttrain_loss: 0.1095 | train_micro_f1: 0.9345\n",
            "\tval_loss: 0.1605 | val_micro_f1: 0.9085\n",
            "epoch 00063\n",
            "\ttrain_loss: 0.1089 | train_micro_f1: 0.9347\n",
            "\tval_loss: 0.1641 | val_micro_f1: 0.9059\n",
            "epoch 00064\n",
            "\ttrain_loss: 0.1053 | train_micro_f1: 0.9372\n",
            "\tval_loss: 0.1593 | val_micro_f1: 0.9086\n",
            "epoch 00065\n",
            "\ttrain_loss: 0.1039 | train_micro_f1: 0.9380\n",
            "\tval_loss: 0.1576 | val_micro_f1: 0.9108\n",
            "epoch 00066\n",
            "\ttrain_loss: 0.1037 | train_micro_f1: 0.9378\n",
            "\tval_loss: 0.1577 | val_micro_f1: 0.9103\n",
            "epoch 00067\n",
            "\ttrain_loss: 0.1018 | train_micro_f1: 0.9390\n",
            "\tval_loss: 0.1583 | val_micro_f1: 0.9120\n",
            "epoch 00068\n",
            "\ttrain_loss: 0.1055 | train_micro_f1: 0.9361\n",
            "\tval_loss: 0.1650 | val_micro_f1: 0.9022\n",
            "epoch 00069\n",
            "\ttrain_loss: 0.1135 | train_micro_f1: 0.9298\n",
            "\tval_loss: 0.1689 | val_micro_f1: 0.9039\n",
            "epoch 00070\n",
            "\ttrain_loss: 0.1205 | train_micro_f1: 0.9251\n",
            "\tval_loss: 0.1673 | val_micro_f1: 0.9005\n",
            "epoch 00071\n",
            "\ttrain_loss: 0.1140 | train_micro_f1: 0.9302\n",
            "\tval_loss: 0.1672 | val_micro_f1: 0.9024\n",
            "epoch 00072\n",
            "\ttrain_loss: 0.1037 | train_micro_f1: 0.9385\n",
            "\tval_loss: 0.1577 | val_micro_f1: 0.9098\n",
            "epoch 00073\n",
            "\ttrain_loss: 0.0959 | train_micro_f1: 0.9433\n",
            "\tval_loss: 0.1508 | val_micro_f1: 0.9157\n",
            "epoch 00074\n",
            "\ttrain_loss: 0.0937 | train_micro_f1: 0.9446\n",
            "\tval_loss: 0.1507 | val_micro_f1: 0.9137\n",
            "epoch 00075\n",
            "\ttrain_loss: 0.0954 | train_micro_f1: 0.9429\n",
            "\tval_loss: 0.1545 | val_micro_f1: 0.9125\n",
            "epoch 00076\n",
            "\ttrain_loss: 0.0995 | train_micro_f1: 0.9400\n",
            "\tval_loss: 0.1535 | val_micro_f1: 0.9158\n",
            "epoch 00077\n",
            "\ttrain_loss: 0.1027 | train_micro_f1: 0.9381\n",
            "\tval_loss: 0.1592 | val_micro_f1: 0.9119\n",
            "epoch 00078\n",
            "\ttrain_loss: 0.0980 | train_micro_f1: 0.9412\n",
            "\tval_loss: 0.1577 | val_micro_f1: 0.9138\n",
            "epoch 00079\n",
            "\ttrain_loss: 0.0965 | train_micro_f1: 0.9429\n",
            "\tval_loss: 0.1564 | val_micro_f1: 0.9142\n",
            "epoch 00080\n",
            "\ttrain_loss: 0.0982 | train_micro_f1: 0.9411\n",
            "\tval_loss: 0.1555 | val_micro_f1: 0.9153\n",
            "epoch 00081\n",
            "\ttrain_loss: 0.0944 | train_micro_f1: 0.9438\n",
            "\tval_loss: 0.1485 | val_micro_f1: 0.9188\n",
            "epoch 00082\n",
            "\ttrain_loss: 0.0913 | train_micro_f1: 0.9458\n",
            "\tval_loss: 0.1536 | val_micro_f1: 0.9148\n",
            "epoch 00083\n",
            "\ttrain_loss: 0.0907 | train_micro_f1: 0.9461\n",
            "\tval_loss: 0.1559 | val_micro_f1: 0.9129\n",
            "epoch 00084\n",
            "\ttrain_loss: 0.0949 | train_micro_f1: 0.9430\n",
            "\tval_loss: 0.1468 | val_micro_f1: 0.9196\n",
            "epoch 00085\n",
            "\ttrain_loss: 0.0936 | train_micro_f1: 0.9440\n",
            "\tval_loss: 0.1540 | val_micro_f1: 0.9161\n",
            "epoch 00086\n",
            "\ttrain_loss: 0.0903 | train_micro_f1: 0.9469\n",
            "\tval_loss: 0.1520 | val_micro_f1: 0.9185\n",
            "epoch 00087\n",
            "\ttrain_loss: 0.0855 | train_micro_f1: 0.9501\n",
            "\tval_loss: 0.1416 | val_micro_f1: 0.9244\n",
            "epoch 00088\n",
            "\ttrain_loss: 0.0756 | train_micro_f1: 0.9567\n",
            "\tval_loss: 0.1343 | val_micro_f1: 0.9306\n",
            "epoch 00089\n",
            "\ttrain_loss: 0.0684 | train_micro_f1: 0.9616\n",
            "\tval_loss: 0.1272 | val_micro_f1: 0.9344\n",
            "epoch 00090\n",
            "\ttrain_loss: 0.0652 | train_micro_f1: 0.9642\n",
            "\tval_loss: 0.1264 | val_micro_f1: 0.9359\n",
            "epoch 00091\n",
            "\ttrain_loss: 0.0637 | train_micro_f1: 0.9644\n",
            "\tval_loss: 0.1268 | val_micro_f1: 0.9356\n",
            "epoch 00092\n",
            "\ttrain_loss: 0.0661 | train_micro_f1: 0.9625\n",
            "\tval_loss: 0.1297 | val_micro_f1: 0.9344\n",
            "epoch 00093\n",
            "\ttrain_loss: 0.0680 | train_micro_f1: 0.9609\n",
            "\tval_loss: 0.1373 | val_micro_f1: 0.9294\n",
            "epoch 00094\n",
            "\ttrain_loss: 0.0721 | train_micro_f1: 0.9586\n",
            "\tval_loss: 0.1338 | val_micro_f1: 0.9315\n",
            "epoch 00095\n",
            "\ttrain_loss: 0.0730 | train_micro_f1: 0.9574\n",
            "\tval_loss: 0.1356 | val_micro_f1: 0.9290\n",
            "epoch 00096\n",
            "\ttrain_loss: 0.0777 | train_micro_f1: 0.9543\n",
            "\tval_loss: 0.1377 | val_micro_f1: 0.9273\n",
            "epoch 00097\n",
            "\ttrain_loss: 0.0757 | train_micro_f1: 0.9552\n",
            "\tval_loss: 0.1408 | val_micro_f1: 0.9272\n",
            "epoch 00098\n",
            "\ttrain_loss: 0.0716 | train_micro_f1: 0.9584\n",
            "\tval_loss: 0.1331 | val_micro_f1: 0.9323\n",
            "epoch 00099\n",
            "\ttrain_loss: 0.0642 | train_micro_f1: 0.9634\n",
            "\tval_loss: 0.1283 | val_micro_f1: 0.9349\n",
            "epoch 00100\n",
            "\ttrain_loss: 0.0610 | train_micro_f1: 0.9655\n",
            "\tval_loss: 0.1260 | val_micro_f1: 0.9363\n",
            "epoch 00101\n",
            "\ttrain_loss: 0.0612 | train_micro_f1: 0.9655\n",
            "\tval_loss: 0.1275 | val_micro_f1: 0.9364\n",
            "epoch 00102\n",
            "\ttrain_loss: 0.0589 | train_micro_f1: 0.9670\n",
            "\tval_loss: 0.1248 | val_micro_f1: 0.9383\n",
            "epoch 00103\n",
            "\ttrain_loss: 0.0557 | train_micro_f1: 0.9689\n",
            "\tval_loss: 0.1226 | val_micro_f1: 0.9390\n",
            "epoch 00104\n",
            "\ttrain_loss: 0.0556 | train_micro_f1: 0.9689\n",
            "\tval_loss: 0.1215 | val_micro_f1: 0.9394\n",
            "epoch 00105\n",
            "\ttrain_loss: 0.0526 | train_micro_f1: 0.9707\n",
            "\tval_loss: 0.1216 | val_micro_f1: 0.9405\n",
            "epoch 00106\n",
            "\ttrain_loss: 0.0514 | train_micro_f1: 0.9717\n",
            "\tval_loss: 0.1180 | val_micro_f1: 0.9427\n",
            "epoch 00107\n",
            "\ttrain_loss: 0.0501 | train_micro_f1: 0.9726\n",
            "\tval_loss: 0.1163 | val_micro_f1: 0.9437\n",
            "epoch 00108\n",
            "\ttrain_loss: 0.0474 | train_micro_f1: 0.9744\n",
            "\tval_loss: 0.1160 | val_micro_f1: 0.9452\n",
            "epoch 00109\n",
            "\ttrain_loss: 0.0452 | train_micro_f1: 0.9758\n",
            "\tval_loss: 0.1151 | val_micro_f1: 0.9459\n",
            "epoch 00110\n",
            "\ttrain_loss: 0.0425 | train_micro_f1: 0.9774\n",
            "\tval_loss: 0.1147 | val_micro_f1: 0.9465\n",
            "epoch 00111\n",
            "\ttrain_loss: 0.0413 | train_micro_f1: 0.9781\n",
            "\tval_loss: 0.1139 | val_micro_f1: 0.9471\n",
            "epoch 00112\n",
            "\ttrain_loss: 0.0412 | train_micro_f1: 0.9785\n",
            "\tval_loss: 0.1143 | val_micro_f1: 0.9466\n",
            "epoch 00113\n",
            "\ttrain_loss: 0.0433 | train_micro_f1: 0.9769\n",
            "\tval_loss: 0.1163 | val_micro_f1: 0.9463\n",
            "epoch 00114\n",
            "\ttrain_loss: 0.0441 | train_micro_f1: 0.9765\n",
            "\tval_loss: 0.1182 | val_micro_f1: 0.9454\n",
            "epoch 00115\n",
            "\ttrain_loss: 0.0443 | train_micro_f1: 0.9763\n",
            "\tval_loss: 0.1148 | val_micro_f1: 0.9469\n",
            "epoch 00116\n",
            "\ttrain_loss: 0.0420 | train_micro_f1: 0.9776\n",
            "\tval_loss: 0.1160 | val_micro_f1: 0.9460\n",
            "epoch 00117\n",
            "\ttrain_loss: 0.0423 | train_micro_f1: 0.9770\n",
            "\tval_loss: 0.1167 | val_micro_f1: 0.9456\n",
            "epoch 00118\n",
            "\ttrain_loss: 0.0433 | train_micro_f1: 0.9764\n",
            "\tval_loss: 0.1156 | val_micro_f1: 0.9465\n",
            "epoch 00119\n",
            "\ttrain_loss: 0.0436 | train_micro_f1: 0.9766\n",
            "\tval_loss: 0.1173 | val_micro_f1: 0.9459\n",
            "epoch 00120\n",
            "\ttrain_loss: 0.0430 | train_micro_f1: 0.9768\n",
            "\tval_loss: 0.1174 | val_micro_f1: 0.9454\n",
            "epoch 00121\n",
            "\ttrain_loss: 0.0440 | train_micro_f1: 0.9756\n",
            "\tval_loss: 0.1210 | val_micro_f1: 0.9436\n",
            "epoch 00122\n",
            "\ttrain_loss: 0.0461 | train_micro_f1: 0.9738\n",
            "\tval_loss: 0.1214 | val_micro_f1: 0.9432\n",
            "epoch 00123\n",
            "\ttrain_loss: 0.0471 | train_micro_f1: 0.9732\n",
            "\tval_loss: 0.1213 | val_micro_f1: 0.9441\n",
            "epoch 00124\n",
            "\ttrain_loss: 0.0526 | train_micro_f1: 0.9702\n",
            "\tval_loss: 0.1276 | val_micro_f1: 0.9402\n",
            "epoch 00125\n",
            "\ttrain_loss: 0.0525 | train_micro_f1: 0.9703\n",
            "\tval_loss: 0.1194 | val_micro_f1: 0.9449\n",
            "epoch 00126\n",
            "\ttrain_loss: 0.0513 | train_micro_f1: 0.9705\n",
            "\tval_loss: 0.1224 | val_micro_f1: 0.9442\n",
            "epoch 00127\n",
            "\ttrain_loss: 0.0500 | train_micro_f1: 0.9715\n",
            "\tval_loss: 0.1258 | val_micro_f1: 0.9421\n",
            "epoch 00128\n",
            "\ttrain_loss: 0.0483 | train_micro_f1: 0.9727\n",
            "\tval_loss: 0.1191 | val_micro_f1: 0.9450\n",
            "epoch 00129\n",
            "\ttrain_loss: 0.0469 | train_micro_f1: 0.9740\n",
            "\tval_loss: 0.1201 | val_micro_f1: 0.9449\n",
            "epoch 00130\n",
            "\ttrain_loss: 0.0439 | train_micro_f1: 0.9757\n",
            "\tval_loss: 0.1173 | val_micro_f1: 0.9467\n",
            "epoch 00131\n",
            "\ttrain_loss: 0.0418 | train_micro_f1: 0.9773\n",
            "\tval_loss: 0.1187 | val_micro_f1: 0.9457\n",
            "epoch 00132\n",
            "\ttrain_loss: 0.0403 | train_micro_f1: 0.9783\n",
            "\tval_loss: 0.1178 | val_micro_f1: 0.9473\n",
            "epoch 00133\n",
            "\ttrain_loss: 0.0401 | train_micro_f1: 0.9783\n",
            "\tval_loss: 0.1161 | val_micro_f1: 0.9492\n",
            "epoch 00134\n",
            "\ttrain_loss: 0.0408 | train_micro_f1: 0.9778\n",
            "\tval_loss: 0.1168 | val_micro_f1: 0.9472\n",
            "epoch 00135\n",
            "\ttrain_loss: 0.0438 | train_micro_f1: 0.9760\n",
            "\tval_loss: 0.1152 | val_micro_f1: 0.9479\n",
            "epoch 00136\n",
            "\ttrain_loss: 0.0439 | train_micro_f1: 0.9758\n",
            "\tval_loss: 0.1214 | val_micro_f1: 0.9452\n",
            "epoch 00137\n",
            "\ttrain_loss: 0.0450 | train_micro_f1: 0.9755\n",
            "\tval_loss: 0.1193 | val_micro_f1: 0.9480\n",
            "epoch 00138\n",
            "\ttrain_loss: 0.0411 | train_micro_f1: 0.9775\n",
            "\tval_loss: 0.1164 | val_micro_f1: 0.9488\n",
            "epoch 00139\n",
            "\ttrain_loss: 0.0387 | train_micro_f1: 0.9789\n",
            "\tval_loss: 0.1174 | val_micro_f1: 0.9480\n",
            "epoch 00140\n",
            "\ttrain_loss: 0.0360 | train_micro_f1: 0.9804\n",
            "\tval_loss: 0.1141 | val_micro_f1: 0.9497\n",
            "epoch 00141\n",
            "\ttrain_loss: 0.0352 | train_micro_f1: 0.9809\n",
            "\tval_loss: 0.1135 | val_micro_f1: 0.9509\n",
            "epoch 00142\n",
            "\ttrain_loss: 0.0366 | train_micro_f1: 0.9806\n",
            "\tval_loss: 0.1174 | val_micro_f1: 0.9503\n",
            "epoch 00143\n",
            "\ttrain_loss: 0.0355 | train_micro_f1: 0.9815\n",
            "\tval_loss: 0.1149 | val_micro_f1: 0.9509\n",
            "epoch 00144\n",
            "\ttrain_loss: 0.0342 | train_micro_f1: 0.9822\n",
            "\tval_loss: 0.1149 | val_micro_f1: 0.9513\n",
            "epoch 00145\n",
            "\ttrain_loss: 0.0322 | train_micro_f1: 0.9832\n",
            "\tval_loss: 0.1146 | val_micro_f1: 0.9513\n",
            "epoch 00146\n",
            "\ttrain_loss: 0.0321 | train_micro_f1: 0.9832\n",
            "\tval_loss: 0.1135 | val_micro_f1: 0.9519\n",
            "epoch 00147\n",
            "\ttrain_loss: 0.0308 | train_micro_f1: 0.9844\n",
            "\tval_loss: 0.1105 | val_micro_f1: 0.9532\n",
            "epoch 00148\n",
            "\ttrain_loss: 0.0286 | train_micro_f1: 0.9853\n",
            "\tval_loss: 0.1118 | val_micro_f1: 0.9533\n",
            "epoch 00149\n",
            "\ttrain_loss: 0.0300 | train_micro_f1: 0.9844\n",
            "\tval_loss: 0.1115 | val_micro_f1: 0.9533\n",
            "epoch 00150\n",
            "\ttrain_loss: 0.0295 | train_micro_f1: 0.9850\n",
            "\tval_loss: 0.1124 | val_micro_f1: 0.9534\n",
            "epoch 00151\n",
            "\ttrain_loss: 0.0296 | train_micro_f1: 0.9848\n",
            "\tval_loss: 0.1153 | val_micro_f1: 0.9528\n",
            "epoch 00152\n",
            "\ttrain_loss: 0.0304 | train_micro_f1: 0.9844\n",
            "\tval_loss: 0.1148 | val_micro_f1: 0.9518\n",
            "epoch 00153\n",
            "\ttrain_loss: 0.0293 | train_micro_f1: 0.9850\n",
            "\tval_loss: 0.1130 | val_micro_f1: 0.9529\n",
            "epoch 00154\n",
            "\ttrain_loss: 0.0284 | train_micro_f1: 0.9853\n",
            "\tval_loss: 0.1109 | val_micro_f1: 0.9539\n",
            "epoch 00155\n",
            "\ttrain_loss: 0.0282 | train_micro_f1: 0.9854\n",
            "\tval_loss: 0.1123 | val_micro_f1: 0.9543\n",
            "epoch 00156\n",
            "\ttrain_loss: 0.0279 | train_micro_f1: 0.9856\n",
            "\tval_loss: 0.1126 | val_micro_f1: 0.9545\n",
            "epoch 00157\n",
            "\ttrain_loss: 0.0279 | train_micro_f1: 0.9859\n",
            "\tval_loss: 0.1106 | val_micro_f1: 0.9548\n",
            "epoch 00158\n",
            "\ttrain_loss: 0.0281 | train_micro_f1: 0.9854\n",
            "\tval_loss: 0.1129 | val_micro_f1: 0.9542\n",
            "epoch 00159\n",
            "\ttrain_loss: 0.0288 | train_micro_f1: 0.9854\n",
            "\tval_loss: 0.1131 | val_micro_f1: 0.9548\n",
            "epoch 00160\n",
            "\ttrain_loss: 0.0281 | train_micro_f1: 0.9857\n",
            "\tval_loss: 0.1147 | val_micro_f1: 0.9530\n",
            "epoch 00161\n",
            "\ttrain_loss: 0.0277 | train_micro_f1: 0.9858\n",
            "\tval_loss: 0.1153 | val_micro_f1: 0.9536\n",
            "epoch 00162\n",
            "\ttrain_loss: 0.0283 | train_micro_f1: 0.9851\n",
            "\tval_loss: 0.1203 | val_micro_f1: 0.9519\n",
            "epoch 00163\n",
            "\ttrain_loss: 0.0281 | train_micro_f1: 0.9858\n",
            "\tval_loss: 0.1184 | val_micro_f1: 0.9531\n",
            "epoch 00164\n",
            "\ttrain_loss: 0.0264 | train_micro_f1: 0.9865\n",
            "\tval_loss: 0.1122 | val_micro_f1: 0.9560\n",
            "epoch 00165\n",
            "\ttrain_loss: 0.0271 | train_micro_f1: 0.9861\n",
            "\tval_loss: 0.1134 | val_micro_f1: 0.9549\n",
            "epoch 00166\n",
            "\ttrain_loss: 0.0266 | train_micro_f1: 0.9862\n",
            "\tval_loss: 0.1142 | val_micro_f1: 0.9545\n",
            "epoch 00167\n",
            "\ttrain_loss: 0.0261 | train_micro_f1: 0.9868\n",
            "\tval_loss: 0.1119 | val_micro_f1: 0.9553\n",
            "epoch 00168\n",
            "\ttrain_loss: 0.0253 | train_micro_f1: 0.9871\n",
            "\tval_loss: 0.1093 | val_micro_f1: 0.9567\n",
            "epoch 00169\n",
            "\ttrain_loss: 0.0264 | train_micro_f1: 0.9865\n",
            "\tval_loss: 0.1167 | val_micro_f1: 0.9543\n",
            "epoch 00170\n",
            "\ttrain_loss: 0.0253 | train_micro_f1: 0.9866\n",
            "\tval_loss: 0.1133 | val_micro_f1: 0.9564\n",
            "epoch 00171\n",
            "\ttrain_loss: 0.0249 | train_micro_f1: 0.9874\n",
            "\tval_loss: 0.1124 | val_micro_f1: 0.9561\n",
            "epoch 00172\n",
            "\ttrain_loss: 0.0244 | train_micro_f1: 0.9877\n",
            "\tval_loss: 0.1165 | val_micro_f1: 0.9552\n",
            "epoch 00173\n",
            "\ttrain_loss: 0.0236 | train_micro_f1: 0.9881\n",
            "\tval_loss: 0.1123 | val_micro_f1: 0.9567\n",
            "epoch 00174\n",
            "\ttrain_loss: 0.0233 | train_micro_f1: 0.9884\n",
            "\tval_loss: 0.1128 | val_micro_f1: 0.9559\n",
            "epoch 00175\n",
            "\ttrain_loss: 0.0228 | train_micro_f1: 0.9892\n",
            "\tval_loss: 0.1098 | val_micro_f1: 0.9585\n",
            "epoch 00176\n",
            "\ttrain_loss: 0.0215 | train_micro_f1: 0.9894\n",
            "\tval_loss: 0.1141 | val_micro_f1: 0.9565\n",
            "epoch 00177\n",
            "\ttrain_loss: 0.0214 | train_micro_f1: 0.9894\n",
            "\tval_loss: 0.1110 | val_micro_f1: 0.9579\n",
            "epoch 00178\n",
            "\ttrain_loss: 0.0209 | train_micro_f1: 0.9897\n",
            "\tval_loss: 0.1115 | val_micro_f1: 0.9579\n",
            "epoch 00179\n",
            "\ttrain_loss: 0.0240 | train_micro_f1: 0.9885\n",
            "\tval_loss: 0.1119 | val_micro_f1: 0.9568\n",
            "epoch 00180\n",
            "\ttrain_loss: 0.0230 | train_micro_f1: 0.9883\n",
            "\tval_loss: 0.1158 | val_micro_f1: 0.9552\n",
            "epoch 00181\n",
            "\ttrain_loss: 0.0239 | train_micro_f1: 0.9878\n",
            "\tval_loss: 0.1126 | val_micro_f1: 0.9572\n",
            "epoch 00182\n",
            "\ttrain_loss: 0.0250 | train_micro_f1: 0.9875\n",
            "\tval_loss: 0.1162 | val_micro_f1: 0.9554\n",
            "epoch 00183\n",
            "\ttrain_loss: 0.0248 | train_micro_f1: 0.9873\n",
            "\tval_loss: 0.1162 | val_micro_f1: 0.9550\n",
            "epoch 00184\n",
            "\ttrain_loss: 0.0241 | train_micro_f1: 0.9875\n",
            "\tval_loss: 0.1145 | val_micro_f1: 0.9559\n",
            "epoch 00185\n",
            "\ttrain_loss: 0.0239 | train_micro_f1: 0.9879\n",
            "\tval_loss: 0.1141 | val_micro_f1: 0.9569\n",
            "epoch 00186\n",
            "\ttrain_loss: 0.0249 | train_micro_f1: 0.9875\n",
            "\tval_loss: 0.1155 | val_micro_f1: 0.9561\n",
            "epoch 00187\n",
            "\ttrain_loss: 0.0239 | train_micro_f1: 0.9876\n",
            "\tval_loss: 0.1157 | val_micro_f1: 0.9556\n",
            "epoch 00188\n",
            "\ttrain_loss: 0.0241 | train_micro_f1: 0.9876\n",
            "\tval_loss: 0.1153 | val_micro_f1: 0.9564\n",
            "epoch 00189\n",
            "\ttrain_loss: 0.0252 | train_micro_f1: 0.9876\n",
            "\tval_loss: 0.1161 | val_micro_f1: 0.9558\n",
            "epoch 00190\n",
            "\ttrain_loss: 0.0255 | train_micro_f1: 0.9865\n",
            "\tval_loss: 0.1168 | val_micro_f1: 0.9558\n",
            "epoch 00191\n",
            "\ttrain_loss: 0.0264 | train_micro_f1: 0.9858\n",
            "\tval_loss: 0.1199 | val_micro_f1: 0.9545\n",
            "epoch 00192\n",
            "\ttrain_loss: 0.0287 | train_micro_f1: 0.9849\n",
            "\tval_loss: 0.1166 | val_micro_f1: 0.9556\n",
            "epoch 00193\n",
            "\ttrain_loss: 0.0298 | train_micro_f1: 0.9838\n",
            "\tval_loss: 0.1199 | val_micro_f1: 0.9537\n",
            "epoch 00194\n",
            "\ttrain_loss: 0.0365 | train_micro_f1: 0.9794\n",
            "\tval_loss: 0.1175 | val_micro_f1: 0.9548\n",
            "epoch 00195\n",
            "\ttrain_loss: 0.0427 | train_micro_f1: 0.9763\n",
            "\tval_loss: 0.1344 | val_micro_f1: 0.9463\n",
            "epoch 00196\n",
            "\ttrain_loss: 0.0461 | train_micro_f1: 0.9734\n",
            "\tval_loss: 0.1379 | val_micro_f1: 0.9424\n",
            "epoch 00197\n",
            "\ttrain_loss: 0.0684 | train_micro_f1: 0.9616\n",
            "\tval_loss: 0.1974 | val_micro_f1: 0.9141\n",
            "epoch 00198\n",
            "\ttrain_loss: 0.0920 | train_micro_f1: 0.9497\n",
            "\tval_loss: 0.1672 | val_micro_f1: 0.9276\n",
            "epoch 00199\n",
            "\ttrain_loss: 0.0924 | train_micro_f1: 0.9498\n",
            "\tval_loss: 0.1629 | val_micro_f1: 0.9279\n",
            "epoch 00200\n",
            "\ttrain_loss: 0.0737 | train_micro_f1: 0.9585\n",
            "\tval_loss: 0.1489 | val_micro_f1: 0.9373\n",
            "best model performance @ epoch 00174: \n",
            "\tval_loss: 0.1098 | val_micro_f1: 0.9585\n",
            "\ttest_loss: 0.0670 | test_micro_f1: 0.9734\n"
          ]
        }
      ],
      "source": [
        "best_model, train_losses, train_scores, val_losses, val_scores = train(model, ppi_train_params)\n",
        "# best_model = model\n",
        "# model_saver.download_best_model_state()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "NJ-Hvg0q3aGp",
        "outputId": "a6e316a6-7c3f-44e7-8e74-c4e34d6100bb"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#train_losses, train_scores, val_losses, val_scores\n",
        "def plot_f1_score(epoch_list, train_losses, val_scores):\n",
        "    plt.figure(figsize = [10,5])\n",
        "    plt.plot(epoch_list, train_losses, 'b', label = \"train loss\")\n",
        "    plt.plot(epoch_list, val_scores, 'r', label = \"validation score\")\n",
        "    plt.title(\"Evolution of validation F1 score and training loss w.r.t epochs\")\n",
        "    plt.ylim([0.0, 1.0])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"F1-Score\")\n",
        "    plt.legend()\n",
        "    plt.savefig('/content/drive/My Drive/GAT_ppi_200_epoch.png')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "epoch_list = list(range(1, 201))\n",
        "plot_f1_score(epoch_list, train_losses, val_scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rZ5Vyw9ITv0"
      },
      "outputs": [],
      "source": [
        "def run_inductive_experiment(iters=10):\n",
        "  losses = []\n",
        "  scores = []\n",
        "  for iter in range(iters):\n",
        "    best_model,_ ,_ ,_ ,_ = train(model, ppi_train_params, verbose=False)\n",
        "    loss, score = evaluate(best_model, test_loader)\n",
        "    losses.append(loss)\n",
        "    scores.append(score)\n",
        "  losses = torch.tensor(losses)\n",
        "  scores = torch.tensor(scores)\n",
        "  return (torch.std_mean(losses), torch.std_mean(scores))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWXFihKkM8hA"
      },
      "source": [
        "Colab's free version works on a dynamic usage limit, which is not fixed and size is not documented anywhere, that is the reason free version is not a guaranteed and unlimited resources.\n",
        "Basically, the overall usage limits and timeout periods, maximum VM lifetime, GPU types available, and other factors vary over time. Colab does not publish these limits, in part because they can (and sometimes do) vary quickly.\n",
        "\n",
        "GPUs and TPUs are sometimes prioritized for users who use Colab interactively rather than for long-running computations, or for users who have recently used less resources in Colab. As a result, users who use Colab for long-running computations, or users who have recently used more resources in Colab, are more likely to run into usage limits and have their access to GPUs and TPUs temporarily restricted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71mWHGQb2np7"
      },
      "source": [
        "It suppose to directly run in 10 iterations at one go. However, Due to the usage limits in Google colab, it never let me run 10 iterations on one go, it always stop at aroung # 5, 6 iteration and it really require at least 12+ GPU. I decided to run it 2 iterations for 5 times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Lh6k_hctASz"
      },
      "outputs": [],
      "source": [
        "loss_ci, score_ci = run_inductive_experiment(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kFV7keH25O6"
      },
      "source": [
        "Demonstate the experiment function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exwQVBvmIYA4",
        "outputId": "2bd39215-6a84-4c92-ad44-01b7b503f2d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training model GAT_PPI\n",
            "GAT_PPI(\n",
            "  (layers): ModuleList(\n",
            "    (0-1): 2 x GATLayer()\n",
            "    (2): GATLayer(\n",
            "      (act): Identity()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "training...\n",
            "best model performance @ epoch 00183: \n",
            "\tval_loss: 0.1116 | val_micro_f1: 0.9589\n",
            "\ttest_loss: 0.0619 | test_micro_f1: 0.9765\n",
            "training model GAT_PPI\n",
            "GAT_PPI(\n",
            "  (layers): ModuleList(\n",
            "    (0-1): 2 x GATLayer()\n",
            "    (2): GATLayer(\n",
            "      (act): Identity()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "training...\n",
            "best model performance @ epoch 00178: \n",
            "\tval_loss: 0.1121 | val_micro_f1: 0.9563\n",
            "\ttest_loss: 0.0638 | test_micro_f1: 0.9735\n"
          ]
        }
      ],
      "source": [
        "loss_ci_0, score_ci_0 = run_inductive_experiment(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wm55k5Ohhnrv",
        "outputId": "3434d7fd-ea81-48e9-906a-e702a8c3199e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training model GAT_PPI\n",
            "GAT_PPI(\n",
            "  (layers): ModuleList(\n",
            "    (0-1): 2 x GATLayer()\n",
            "    (2): GATLayer(\n",
            "      (act): Identity()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "training...\n",
            "best model performance @ epoch 00182: \n",
            "\tval_loss: 0.0994 | val_micro_f1: 0.9627\n",
            "\ttest_loss: 0.0563 | test_micro_f1: 0.9773\n",
            "training model GAT_PPI\n",
            "GAT_PPI(\n",
            "  (layers): ModuleList(\n",
            "    (0-1): 2 x GATLayer()\n",
            "    (2): GATLayer(\n",
            "      (act): Identity()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "training...\n",
            "best model performance @ epoch 00192: \n",
            "\tval_loss: 0.1055 | val_micro_f1: 0.9593\n",
            "\ttest_loss: 0.0717 | test_micro_f1: 0.9724\n"
          ]
        }
      ],
      "source": [
        "loss_ci_1, score_ci_1 = run_inductive_experiment(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6jzXk_t13kN",
        "outputId": "4bf2a213-9c13-435d-bdfd-0ebef7ac2424"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training model GAT_PPI\n",
            "GAT_PPI(\n",
            "  (layers): ModuleList(\n",
            "    (0-1): 2 x GATLayer()\n",
            "    (2): GATLayer(\n",
            "      (act): Identity()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "training...\n",
            "best model performance @ epoch 00173: \n",
            "\tval_loss: 0.1063 | val_micro_f1: 0.9602\n",
            "\ttest_loss: 0.0623 | test_micro_f1: 0.9751\n",
            "training model GAT_PPI\n",
            "GAT_PPI(\n",
            "  (layers): ModuleList(\n",
            "    (0-1): 2 x GATLayer()\n",
            "    (2): GATLayer(\n",
            "      (act): Identity()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "training...\n",
            "best model performance @ epoch 00197: \n",
            "\tval_loss: 0.1047 | val_micro_f1: 0.9626\n",
            "\ttest_loss: 0.0619 | test_micro_f1: 0.9761\n"
          ]
        }
      ],
      "source": [
        "loss_ci_2, score_ci_2 = run_inductive_experiment(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Os79LV7W17G0",
        "outputId": "99b18f05-a729-4ae3-c60c-a44b1dae7541"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training model GAT_PPI\n",
            "GAT_PPI(\n",
            "  (layers): ModuleList(\n",
            "    (0-1): 2 x GATLayer()\n",
            "    (2): GATLayer(\n",
            "      (act): Identity()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "training...\n",
            "best model performance @ epoch 00172: \n",
            "\tval_loss: 0.1069 | val_micro_f1: 0.9577\n",
            "\ttest_loss: 0.0622 | test_micro_f1: 0.9745\n",
            "training model GAT_PPI\n",
            "GAT_PPI(\n",
            "  (layers): ModuleList(\n",
            "    (0-1): 2 x GATLayer()\n",
            "    (2): GATLayer(\n",
            "      (act): Identity()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "training...\n",
            "best model performance @ epoch 00197: \n",
            "\tval_loss: 0.1104 | val_micro_f1: 0.9608\n",
            "\ttest_loss: 0.0632 | test_micro_f1: 0.9770\n"
          ]
        }
      ],
      "source": [
        "loss_ci_3, score_ci_3 = run_inductive_experiment(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0fOFQ6X19uT",
        "outputId": "7c98aa20-5466-45f8-d3b1-6bae2f593716"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training model GAT_PPI\n",
            "GAT_PPI(\n",
            "  (layers): ModuleList(\n",
            "    (0-1): 2 x GATLayer()\n",
            "    (2): GATLayer(\n",
            "      (act): Identity()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "training...\n",
            "best model performance @ epoch 00170: \n",
            "\tval_loss: 0.1057 | val_micro_f1: 0.9575\n",
            "\ttest_loss: 0.0618 | test_micro_f1: 0.9740\n",
            "training model GAT_PPI\n",
            "GAT_PPI(\n",
            "  (layers): ModuleList(\n",
            "    (0-1): 2 x GATLayer()\n",
            "    (2): GATLayer(\n",
            "      (act): Identity()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "training...\n",
            "best model performance @ epoch 00199: \n",
            "\tval_loss: 0.1042 | val_micro_f1: 0.9620\n",
            "\ttest_loss: 0.0639 | test_micro_f1: 0.9751\n"
          ]
        }
      ],
      "source": [
        "loss_ci_4, score_ci_4 = run_inductive_experiment(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdZsB_sBiIv-"
      },
      "source": [
        "Below is the code for print the loss and f1 score however due to the previous running problem it never got chance to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQm1Asdoail8"
      },
      "outputs": [],
      "source": [
        "loss_std, loss_mean = loss_ci\n",
        "score_std, score_mean = score_ci\n",
        "\n",
        "print(f'loss:\\t\\t{loss_mean:.4f} +/- {loss_std:.4f}')\n",
        "print(f'micro F1 score: {score_mean:.4f} +/- {score_std:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM38TWMVlZ1k"
      },
      "source": [
        "Modify above code so it can use the 5 results together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XxXrJy_b93I",
        "outputId": "7ed33dbb-a395-4b8b-b698-888ab1e20d72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test loss:\t\t0.0629 +/- 0.0038\n",
            "test micro F1 score: \t0.9751 +/- 0.0016\n"
          ]
        }
      ],
      "source": [
        "loss_std, loss_mean = torch.std_mean(torch.tensor([ 0.0619, 0.0638, 0.0563, 0.0717, 0.0623, 0.0619, 0.0622, 0.0632, 0.0618, 0.0639 ]))\n",
        "score_std, score_mean = torch.std_mean(torch.tensor([ 0.9765, 0.9735, 0.9773, 0.9724, 0.9751, 0.9761, 0.9745, 0.9770, 0.9740, 0.9751]))\n",
        "\n",
        "print(f'test loss:\\t\\t{loss_mean:.4f} +/- {loss_std:.4f}')\n",
        "print(f'test micro F1 score: \\t{score_mean:.4f} +/- {score_std:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaiI8NO_v1R-"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, HTML\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'Iteration #' : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 'Mean', 'Standard Deviation'],\n",
        "                   'GAT PPI Test Loss' : [ 0.0619, 0.0638, 0.0563, 0.0717, 0.0623, 0.0619, 0.0622, 0.0632, 0.0618, 0.0639, loss_mean.item(), loss_std.item() ],\n",
        "                   'GAT PPI Test Accuracy' : [0.9765, 0.9735, 0.9773, 0.9724, 0.9751, 0.9761, 0.9745, 0.9770, 0.9740, 0.9751, score_mean.item(), score_std.item()]})\n",
        "#display(df)\n",
        "display(HTML(df.to_html(index=False)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ke9xS-5JIh71"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "################################\n",
        "###       GAT LAYER # 1      ###\n",
        "################################\n",
        "\n",
        "class GraphAttentionLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Graph Attention Layer (GAT) as described in the paper `\"Graph Attention Networks\" <https://arxiv.org/pdf/1710.10903.pdf>`.\n",
        "\n",
        "        This operation can be mathematically described as:\n",
        "\n",
        "            e_ij = a(W h_i, W h_j)\n",
        "            α_ij = softmax_j(e_ij) = exp(e_ij) / Σ_k(exp(e_ik))\n",
        "            h_i' = σ(Σ_j(α_ij W h_j))\n",
        "\n",
        "            where h_i and h_j are the feature vectors of nodes i and j respectively, W is a learnable weight matrix,\n",
        "            a is an attention mechanism that computes the attention coefficients e_ij, and σ is an activation function.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features: int, out_features: int, n_heads: int, concat: bool = False, dropout: float = 0.4, leaky_relu_slope: float = 0.2):\n",
        "        super(GraphAttentionLayer, self).__init__()\n",
        "\n",
        "        self.n_heads = n_heads # Number of attention heads\n",
        "        self.concat = concat # wether to concatenate the final attention heads\n",
        "        self.dropout = dropout # Dropout rate\n",
        "\n",
        "        if concat: # concatenating the attention heads\n",
        "            self.out_features = out_features # Number of output features per node\n",
        "            assert out_features % n_heads == 0 # Ensure that out_features is a multiple of n_heads\n",
        "            self.n_hidden = out_features // n_heads\n",
        "        else: # averaging output over the attention heads (Used in the main paper)\n",
        "            self.n_hidden = out_features\n",
        "\n",
        "        #  A shared linear transformation, parametrized by a weight matrix W is applied to every node\n",
        "        #  Initialize the weight matrix W\n",
        "        self.W = nn.Parameter(torch.empty(size=(in_features, self.n_hidden * n_heads)))\n",
        "\n",
        "        # Initialize the attention weights a\n",
        "        self.a = nn.Parameter(torch.empty(size=(n_heads, 2 * self.n_hidden, 1)))\n",
        "\n",
        "        self.leakyrelu = nn.LeakyReLU(leaky_relu_slope) # LeakyReLU activation function\n",
        "        self.softmax = nn.Softmax(dim=1) # softmax activation function to the attention coefficients\n",
        "\n",
        "        self.reset_parameters() # Reset the parameters\n",
        "\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        \"\"\"\n",
        "        Reinitialize learnable parameters.\n",
        "        \"\"\"\n",
        "        nn.init.xavier_normal_(self.W)\n",
        "        nn.init.xavier_normal_(self.a)\n",
        "\n",
        "\n",
        "    def forward(self,  h: torch.Tensor, adj_mat: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Performs a graph attention layer operation.\n",
        "\n",
        "        Args:\n",
        "            h (torch.Tensor): Input tensor representing node features.\n",
        "            adj_mat (torch.Tensor): Adjacency matrix representing graph structure.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor after the graph convolution operation.\n",
        "        \"\"\"\n",
        "        n_nodes = h.shape[0]\n",
        "\n",
        "        # Apply linear transformation to node feature -> W h\n",
        "        # output shape (n_nodes, n_hidden * n_heads)\n",
        "        h_transformed = torch.mm(h, self.W)\n",
        "        h_transformed = F.dropout(h_transformed, self.dropout, training=self.training)\n",
        "\n",
        "        # splitting the heads by reshaping the tensor and putting heads dim first\n",
        "        # output shape (n_heads, n_nodes, n_hidden)\n",
        "        h_transformed = h_transformed.view(n_nodes, self.n_heads, self.n_hidden).permute(1, 0, 2)\n",
        "\n",
        "        # getting the attention scores\n",
        "        # output shape (n_heads, n_nodes, n_nodes)\n",
        "        # e = self._get_attention_scores(h_transformed) ORGINIAL\n",
        "        source_scores = torch.matmul(h_transformed, self.a[:, :self.n_hidden, :])\n",
        "        target_scores = torch.matmul(h_transformed, self.a[:, self.n_hidden:, :])\n",
        "\n",
        "        # broadcast add\n",
        "        # (n_heads, n_nodes, 1) + (n_heads, 1, n_nodes) = (n_heads, n_nodes, n_nodes)\n",
        "        e = source_scores + target_scores.mT\n",
        "        e = self.leakyrelu(e)\n",
        "\n",
        "        # Set the attention score for non-existent edges to -9e15 (MASKING NON-EXISTENT EDGES)\n",
        "        connectivity_mask = -9e16 * torch.ones_like(e)\n",
        "        e = torch.where(adj_mat > 0, e, connectivity_mask) # masked attention scores\n",
        "\n",
        "        # attention coefficients are computed as a softmax over the rows\n",
        "        # for each column j in the attention score matrix e\n",
        "        attention = F.softmax(e, dim=-1)\n",
        "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
        "\n",
        "        # final node embeddings are computed as a weighted average of the features of its neighbors\n",
        "        h_prime = torch.matmul(attention, h_transformed)\n",
        "\n",
        "        # concatenating/averaging the attention heads\n",
        "        # output shape (n_nodes, out_features)\n",
        "        if self.concat:\n",
        "            h_prime = h_prime.permute(1, 0, 2).contiguous().view(n_nodes, self.out_features)\n",
        "        else:\n",
        "            h_prime = h_prime.mean(dim=0)\n",
        "\n",
        "        return h_prime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "N0OIE116Ktyz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "################################\n",
        "###    GAT NETWORK MODULE    ###\n",
        "################################\n",
        "\n",
        "class GAT(nn.Module):\n",
        "    \"\"\"\n",
        "    Graph Attention Network (GAT) as described in the paper `\"Graph Attention Networks\" <https://arxiv.org/pdf/1710.10903.pdf>`.\n",
        "    Consists of a 2-layer stack of Graph Attention Layers (GATs). The fist GAT Layer is followed by an ELU activation.\n",
        "    And the second (final) layer is a GAT layer with a single attention head and softmax activation function.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "        in_features,\n",
        "        n_hidden,\n",
        "        n_heads,\n",
        "        num_classes,\n",
        "        concat=False,\n",
        "        dropout=0.4,\n",
        "        leaky_relu_slope=0.2):\n",
        "        \"\"\" Initializes the GAT model.\n",
        "\n",
        "        Args:\n",
        "            in_features (int): number of input features per node.\n",
        "            n_hidden (int): output size of the first Graph Attention Layer.\n",
        "            n_heads (int): number of attention heads in the first Graph Attention Layer.\n",
        "            num_classes (int): number of classes to predict for each node.\n",
        "            concat (bool, optional): Wether to concatinate attention heads or take an average over them for the\n",
        "                output of the first Graph Attention Layer. Defaults to False.\n",
        "            dropout (float, optional): dropout rate. Defaults to 0.4.\n",
        "            leaky_relu_slope (float, optional): alpha (slope) of the leaky relu activation. Defaults to 0.2.\n",
        "        \"\"\"\n",
        "\n",
        "        super(GAT, self).__init__()\n",
        "\n",
        "        # Define the Graph Attention layers\n",
        "        self.gat1 = GraphAttentionLayer(\n",
        "            in_features=in_features, out_features=n_hidden, n_heads=n_heads,\n",
        "            concat=concat, dropout=dropout, leaky_relu_slope=leaky_relu_slope\n",
        "            )\n",
        "\n",
        "        self.gat2 = GraphAttentionLayer(\n",
        "            in_features=n_hidden, out_features=num_classes, n_heads=1,\n",
        "            concat=False, dropout=dropout, leaky_relu_slope=leaky_relu_slope\n",
        "            )\n",
        "\n",
        "\n",
        "    '''\n",
        "    def reset_parameters(self):\n",
        "        self.gat1.reset_parameters()\n",
        "        self.gat2.reset_parameters()\n",
        "    '''\n",
        "\n",
        "    def forward(self, input_tensor: torch.Tensor , adj_mat: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Performs a forward pass through the network.\n",
        "\n",
        "        Args:\n",
        "            input_tensor (torch.Tensor): Input tensor representing node features.\n",
        "            adj_mat (torch.Tensor): Adjacency matrix representing graph structure.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor after the forward pass.\n",
        "        \"\"\"\n",
        "\n",
        "        # Apply the first Graph Attention layer\n",
        "        x = self.gat1(input_tensor, adj_mat)\n",
        "        x = F.elu(x) # Apply ELU activation function to the output of the first layer\n",
        "\n",
        "        # Apply the second Graph Attention layer\n",
        "        x = self.gat2(x, adj_mat)\n",
        "\n",
        "        return F.log_softmax(x, dim=1) # Apply log softmax activation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "e4hQ_XTcIik1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import requests\n",
        "import tarfile\n",
        "import numpy as np\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "\n",
        "################################\n",
        "### LOADING THE CORA DATASET ###\n",
        "################################\n",
        "def preprocess_index(cites_data, content_data):\n",
        "\n",
        "    # Get unique IDs from both files\n",
        "    content_ids = np.unique(content_data[:, 0])\n",
        "    cites_ids = np.unique(np.concatenate((cites_data[:, 0], cites_data[:, 1])))\n",
        "\n",
        "    # Create ID mappings\n",
        "    content_id_mapping = dict(zip(content_ids, range(len(content_ids))))\n",
        "    cites_id_mapping = {id: content_id_mapping.get(id, len(content_id_mapping) + idx) for idx, id in enumerate(np.setdiff1d(cites_ids, content_ids))}\n",
        "    len_diff = len(cites_id_mapping)\n",
        "    cites_id_mapping.update(content_id_mapping)\n",
        "\n",
        "    # Update IDs in .cites file\n",
        "    cites_data[:, 0] = [cites_id_mapping[id] for id in cites_data[:, 0]]\n",
        "    cites_data[:, 1] = [cites_id_mapping[id] for id in cites_data[:, 1]]\n",
        "\n",
        "    # Update IDs in .content file\n",
        "    content_data[:, 0] = [content_id_mapping[id] for id in content_data[:, 0]]\n",
        "    # Add len_diff rows to content_data\n",
        "    additional_rows = np.zeros((len_diff, content_data.shape[1]),  dtype=content_data.dtype)\n",
        "    additional_rows[:, 1:-1] = np.array([['0'] * (content_data.shape[1] - 2)] * len_diff)\n",
        "    additional_rows[:, 0] = np.arange(len(content_ids), len(content_ids) + len_diff)\n",
        "    additional_rows[:, -1] = np.array(['NONE'] * len_diff)\n",
        "\n",
        "    new_content_data = np.vstack((content_data, additional_rows))\n",
        "\n",
        "    return new_content_data, cites_data.astype(np.int64)\n",
        "\n",
        "def load_data(path='./datasets/citeseer/', device='cpu'):\n",
        "    \"\"\"\n",
        "    Loads the Cora dataset. The dataset is downloaded from https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Set the paths to the data files\n",
        "    content_path = os.path.join(path, 'citeseer.content')\n",
        "    cites_path = os.path.join(path, 'citeseer.cites')\n",
        "\n",
        "    # Load data from files\n",
        "    content_data = np.genfromtxt(content_path, dtype=np.dtype(str))\n",
        "    cites_data = np.genfromtxt(cites_path, dtype=np.dtype(str))\n",
        "\n",
        "    content_tensor, cites_tensor = preprocess_index(cites_data, content_data)\n",
        "\n",
        "    # Process features\n",
        "    features = torch.FloatTensor(content_tensor[:, 1:-1].astype(np.int32)) # Extract feature values\n",
        "    scale_vector = torch.sum(features, dim=1) # Compute sum of features for each node\n",
        "    scale_vector = 1 / scale_vector # Compute reciprocal of the sums\n",
        "    scale_vector[scale_vector == float('inf')] = 0 # Handle division by zero cases\n",
        "    scale_vector = torch.diag(scale_vector).to_sparse() # Convert the scale vector to a sparse diagonal matrix\n",
        "    features = scale_vector @ features # Scale the features using the scale vector\n",
        "\n",
        "    # Process labels\n",
        "    classes, labels = np.unique(content_tensor[:, -1], return_inverse=True) # Extract unique classes and map labels to indices\n",
        "    labels = torch.LongTensor(labels) # Convert labels to a tensor\n",
        "\n",
        "    # Process adjacency matrix\n",
        "    idx = content_tensor[:, 0].astype(np.int32) # Extract node indices\n",
        "    idx_map = {id: pos for pos, id in enumerate(idx)} # Create a dictionary to map indices to positions\n",
        "\n",
        "    # Map node indices to positions in the adjacency matrix\n",
        "    edges = np.array(\n",
        "        list(map(lambda edge: [idx_map[edge[0]], idx_map[edge[1]]],\n",
        "            cites_tensor)), dtype=np.int32)\n",
        "\n",
        "    V = len(idx) # Number of nodes\n",
        "    E = edges.shape[0] # Number of edges\n",
        "    adj_mat = torch.sparse_coo_tensor(edges.T, torch.ones(E), (V, V), dtype=torch.int64) # Create the initial adjacency matrix as a sparse tensor\n",
        "    adj_mat = torch.eye(V) + adj_mat # Add self-loops to the adjacency matrix\n",
        "\n",
        "    # return features.to_sparse().to(device), labels.to(device), adj_mat.to_sparse().to(device)\n",
        "    return features.to(device), labels.to(device), adj_mat.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "xWMZIpLDfJGr"
      },
      "outputs": [],
      "source": [
        "# 3\n",
        "import os\n",
        "import time\n",
        "import requests\n",
        "import tarfile\n",
        "import numpy as np\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "\n",
        "def train_and_evaluate(model, input, target, mask_train, mask_val, epochs, patience, print_result):\n",
        "    optimizer = Adam(model.parameters(), lr=args['lr'], weight_decay=args['l2'])\n",
        "    # model.to(device).reset_parameters()\n",
        "    # model.reset_parameters()\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    t_total = time.time()\n",
        "    loss_values = []\n",
        "    acc_values = []\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    best_val_loss = np.inf\n",
        "    curr_step = 0\n",
        "\n",
        "    best_model_state_dict = None\n",
        "    best_optimizer_state_dict = None\n",
        "\n",
        "    best = -1\n",
        "    best_epoch = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Configure the optimizer and loss function\n",
        "\n",
        "        start_t = time.time()\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(*input)\n",
        "        loss = criterion(output[mask_train], target[mask_train])  # Compute the loss using the training mask\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Evaluate the model performance on training and validation sets\n",
        "        loss_train, acc_train = evaluate(model, criterion, input, target, mask_train)\n",
        "        loss_val, acc_val = evaluate(model, criterion, input, target, mask_val)\n",
        "\n",
        "        loss_values.append(loss_val)\n",
        "        acc_values.append(acc_val)\n",
        "\n",
        "        # Print the training progress\n",
        "        if print_result:\n",
        "            print(f'Epoch: {epoch + 1:04d} ({(time.time() - start_t):.4f}s) loss_train: {loss_train:.4f} acc_train: {acc_train:.4f} loss_val: {loss_val:.4f} acc_val: {acc_val:.4f}')\n",
        "\n",
        "        if acc_val > best_val_acc or loss_val < best_val_loss:\n",
        "            best_val_acc = max(acc_val, best_val_acc)\n",
        "            best_val_loss = min(loss_val, best_val_loss)\n",
        "            best_model_state_dict = model.state_dict()\n",
        "            best_optimizer_state_dict = optimizer.state_dict()\n",
        "            best_epoch = epoch\n",
        "            curr_step = 0\n",
        "            best_so_far = True\n",
        "        else:\n",
        "            curr_step += 1\n",
        "            best_so_far = False\n",
        "\n",
        "        if best_so_far:\n",
        "            torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': best_model_state_dict,\n",
        "                    'optimizer_state_dict': best_optimizer_state_dict,\n",
        "                }, 'GAT')\n",
        "\n",
        "        if curr_step == patience:\n",
        "            print('Early stopping...')\n",
        "            print(f'Best Val Loss: {best_val_loss:.4f}, Best Val Acc: {best_val_acc:.4f}')\n",
        "            break\n",
        "\n",
        "    print(\"Optimization Finished!\")\n",
        "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "    # Restore best model\n",
        "    print('Loading {}th epoch'.format(best_epoch + 1))\n",
        "    state = torch.load('GAT')\n",
        "    model.load_state_dict(state['model_state_dict'])\n",
        "    loss_test, acc_test = evaluate(model, criterion, (features, adj_mat), labels, idx_test)\n",
        "    print(f'Test set results: loss {loss_test:.4f} accuracy {acc_test:.4f}')\n",
        "    return model, loss_values, acc_values, loss_test, acc_test\n",
        "\n",
        "\n",
        "def evaluate(model, criterion, input, target, mask):\n",
        "    # model = model.to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(*input)\n",
        "        output, target = output[mask], target[mask]\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "        acc = (output.argmax(dim=1) == target).float().sum() / len(target)\n",
        "\n",
        "    return loss.item(), acc.item()\n",
        "\n",
        "\n",
        "# Example usage\n",
        "# Initialize your model, optimizer, criterion, input, target, and masks\n",
        "# train(GAT_citeseer, optimizer, criterion, (features, adj_mat), labels, idx_train, idx_val, args['epochs'], args['patience'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVMlQ8TNI0vE",
        "outputId": "134f1220-be16-4649-92af-a8eb3ede611d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "# 2\n",
        "import torch\n",
        "import os\n",
        "import requests\n",
        "import tarfile\n",
        "import glob\n",
        "# Set the parameters directly\n",
        "args = {\n",
        "    'seed': 42, # 9\n",
        "    'no_cuda': False,\n",
        "    'no_mps': False,\n",
        "    'hidden_dim': 64,\n",
        "    'num_heads': 8,\n",
        "    'concat_heads': False,\n",
        "    'dropout_p': 0.6,\n",
        "    'lr': 0.005,\n",
        "    'l2': 5e-4,\n",
        "    'epochs': 200,  # 300\n",
        "    'patience': 100,\n",
        "    'print': True,\n",
        "    # 'val_every': 20,\n",
        "}\n",
        "\n",
        "torch.manual_seed(args['seed'])\n",
        "use_cuda = not args['no_cuda'] and torch.cuda.is_available()\n",
        "use_mps = not args['no_mps'] and torch.backends.mps.is_available()\n",
        "\n",
        "# Set the device to run on\n",
        "if use_cuda:\n",
        "    device = torch.device('cuda')\n",
        "elif use_mps:\n",
        "    device = torch.device('mps')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "print(f'Using {device} device')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWTkpQs6I56s",
        "outputId": "ccb06832-429f-4930-cb3b-3db4a90d583d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset already downloaded...\n",
            "Loading dataset...\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "data_url = 'https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz'\n",
        "path = './datasets/citeseer'\n",
        "\n",
        "if os.path.isfile(os.path.join(path, 'citeseer.content')) and os.path.isfile(os.path.join(path, 'citeseer.cites')):\n",
        "    print('Dataset already downloaded...')\n",
        "else:\n",
        "    print('Downloading dataset...')\n",
        "    with requests.get(data_url, stream=True) as tgz_file:\n",
        "        with tarfile.open(fileobj=tgz_file.raw, mode='r:gz') as tgz_object:\n",
        "            tgz_object.extractall()\n",
        "\n",
        "print('Loading dataset...')\n",
        "# Load the dataset\n",
        "features, labels, adj_mat = load_data(device=device)\n",
        "# Split the dataset into training, validation, and test sets\n",
        "idx = torch.randperm(len(labels)).to(device)\n",
        "# idx_test, idx_val, idx_train = idx[:1200], idx[1200:1600], idx[1600:]\n",
        "idx_test, idx_val, idx_train = idx[:1000], idx[1000:1500], idx[1500:]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ClQTJcuJvrJt"
      },
      "outputs": [],
      "source": [
        "# Create the model\n",
        "# The model consists of a 2-layer stack of Graph Attention Layers (GATs).\n",
        "GAT_citeseer = GAT(\n",
        "    in_features=features.shape[1],          # Number of input features per node\n",
        "    n_hidden=args['hidden_dim'],            # Output size of the first Graph Attention Layer\n",
        "    n_heads=args['num_heads'],               # Number of attention heads in the first Graph Attention Layer\n",
        "    num_classes=labels.max().item() + 1,     # Number of classes to predict for each node\n",
        "    concat=args['concat_heads'],             # Whether to concatenate attention heads\n",
        "    dropout=args['dropout_p'],                # Dropout rate\n",
        "    leaky_relu_slope=0.2                     # Alpha (slope) of the leaky ReLU activation\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8CtaO2-I83U",
        "outputId": "321287e4-ead7-446a-ce1c-863fad091404"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0001 (16.4478s) loss_train: 1.9420 acc_train: 0.5271 loss_val: 1.9424 acc_val: 0.5140\n",
            "Epoch: 0002 (13.2491s) loss_train: 1.9379 acc_train: 0.6086 loss_val: 1.9388 acc_val: 0.5640\n",
            "Epoch: 0003 (10.8165s) loss_train: 1.9339 acc_train: 0.6207 loss_val: 1.9353 acc_val: 0.5780\n",
            "Epoch: 0004 (9.6499s) loss_train: 1.9298 acc_train: 0.6278 loss_val: 1.9317 acc_val: 0.5860\n",
            "Epoch: 0005 (10.7097s) loss_train: 1.9255 acc_train: 0.6377 loss_val: 1.9279 acc_val: 0.5840\n",
            "Epoch: 0006 (10.7778s) loss_train: 1.9212 acc_train: 0.6530 loss_val: 1.9241 acc_val: 0.6000\n",
            "Epoch: 0007 (10.8381s) loss_train: 1.9170 acc_train: 0.6782 loss_val: 1.9204 acc_val: 0.6160\n",
            "Epoch: 0008 (10.5602s) loss_train: 1.9128 acc_train: 0.6875 loss_val: 1.9166 acc_val: 0.6280\n",
            "Epoch: 0009 (10.5469s) loss_train: 1.9086 acc_train: 0.6973 loss_val: 1.9129 acc_val: 0.6480\n",
            "Epoch: 0010 (11.0695s) loss_train: 1.9043 acc_train: 0.7066 loss_val: 1.9092 acc_val: 0.6480\n",
            "Epoch: 0011 (19.7382s) loss_train: 1.9001 acc_train: 0.7132 loss_val: 1.9055 acc_val: 0.6600\n",
            "Epoch: 0012 (12.2183s) loss_train: 1.8960 acc_train: 0.7143 loss_val: 1.9019 acc_val: 0.6660\n",
            "Epoch: 0013 (10.7716s) loss_train: 1.8920 acc_train: 0.7170 loss_val: 1.8983 acc_val: 0.6700\n",
            "Epoch: 0014 (10.8350s) loss_train: 1.8880 acc_train: 0.7170 loss_val: 1.8949 acc_val: 0.6720\n",
            "Epoch: 0015 (10.8610s) loss_train: 1.8841 acc_train: 0.7192 loss_val: 1.8914 acc_val: 0.6700\n",
            "Epoch: 0016 (10.8230s) loss_train: 1.8804 acc_train: 0.7236 loss_val: 1.8881 acc_val: 0.6680\n",
            "Epoch: 0017 (9.8016s) loss_train: 1.8768 acc_train: 0.7214 loss_val: 1.8848 acc_val: 0.6720\n",
            "Epoch: 0018 (11.2135s) loss_train: 1.8731 acc_train: 0.7176 loss_val: 1.8815 acc_val: 0.6740\n",
            "Epoch: 0019 (11.2215s) loss_train: 1.8697 acc_train: 0.7187 loss_val: 1.8784 acc_val: 0.6760\n",
            "Epoch: 0020 (10.9207s) loss_train: 1.8665 acc_train: 0.7219 loss_val: 1.8755 acc_val: 0.6760\n",
            "Epoch: 0021 (10.8840s) loss_train: 1.8633 acc_train: 0.7225 loss_val: 1.8726 acc_val: 0.6740\n",
            "Epoch: 0022 (10.0077s) loss_train: 1.8602 acc_train: 0.7181 loss_val: 1.8698 acc_val: 0.6800\n",
            "Epoch: 0023 (10.4716s) loss_train: 1.8572 acc_train: 0.7148 loss_val: 1.8671 acc_val: 0.6760\n",
            "Epoch: 0024 (10.8305s) loss_train: 1.8543 acc_train: 0.7143 loss_val: 1.8644 acc_val: 0.6720\n",
            "Epoch: 0025 (10.9012s) loss_train: 1.8513 acc_train: 0.7121 loss_val: 1.8617 acc_val: 0.6740\n",
            "Epoch: 0026 (10.8785s) loss_train: 1.8482 acc_train: 0.7126 loss_val: 1.8588 acc_val: 0.6840\n",
            "Epoch: 0027 (9.9563s) loss_train: 1.8452 acc_train: 0.7154 loss_val: 1.8561 acc_val: 0.6820\n",
            "Epoch: 0028 (11.1012s) loss_train: 1.8423 acc_train: 0.7143 loss_val: 1.8534 acc_val: 0.6760\n",
            "Epoch: 0029 (11.1283s) loss_train: 1.8392 acc_train: 0.7115 loss_val: 1.8505 acc_val: 0.6760\n",
            "Epoch: 0030 (10.9646s) loss_train: 1.8363 acc_train: 0.7170 loss_val: 1.8478 acc_val: 0.6760\n",
            "Epoch: 0031 (10.9105s) loss_train: 1.8333 acc_train: 0.7170 loss_val: 1.8452 acc_val: 0.6780\n",
            "Epoch: 0032 (10.1512s) loss_train: 1.8304 acc_train: 0.7170 loss_val: 1.8426 acc_val: 0.6780\n",
            "Epoch: 0033 (10.4655s) loss_train: 1.8275 acc_train: 0.7181 loss_val: 1.8400 acc_val: 0.6800\n",
            "Epoch: 0034 (10.8244s) loss_train: 1.8244 acc_train: 0.7192 loss_val: 1.8372 acc_val: 0.6820\n",
            "Epoch: 0035 (10.9021s) loss_train: 1.8213 acc_train: 0.7219 loss_val: 1.8345 acc_val: 0.6840\n",
            "Epoch: 0036 (10.8398s) loss_train: 1.8182 acc_train: 0.7214 loss_val: 1.8316 acc_val: 0.6840\n",
            "Epoch: 0037 (9.9700s) loss_train: 1.8152 acc_train: 0.7230 loss_val: 1.8289 acc_val: 0.6820\n",
            "Epoch: 0038 (10.5541s) loss_train: 1.8122 acc_train: 0.7241 loss_val: 1.8262 acc_val: 0.6860\n",
            "Epoch: 0039 (10.9108s) loss_train: 1.8092 acc_train: 0.7258 loss_val: 1.8234 acc_val: 0.6880\n",
            "Epoch: 0040 (10.8521s) loss_train: 1.8064 acc_train: 0.7296 loss_val: 1.8208 acc_val: 0.6840\n",
            "Epoch: 0041 (10.9097s) loss_train: 1.8035 acc_train: 0.7329 loss_val: 1.8181 acc_val: 0.6860\n",
            "Epoch: 0042 (9.9092s) loss_train: 1.8006 acc_train: 0.7345 loss_val: 1.8154 acc_val: 0.6840\n",
            "Epoch: 0043 (10.6406s) loss_train: 1.7978 acc_train: 0.7378 loss_val: 1.8128 acc_val: 0.6840\n",
            "Epoch: 0044 (10.9067s) loss_train: 1.7949 acc_train: 0.7433 loss_val: 1.8102 acc_val: 0.6840\n",
            "Epoch: 0045 (10.8302s) loss_train: 1.7921 acc_train: 0.7422 loss_val: 1.8077 acc_val: 0.6840\n",
            "Epoch: 0046 (10.8070s) loss_train: 1.7891 acc_train: 0.7438 loss_val: 1.8051 acc_val: 0.6840\n",
            "Epoch: 0047 (9.7667s) loss_train: 1.7861 acc_train: 0.7422 loss_val: 1.8024 acc_val: 0.6860\n",
            "Epoch: 0048 (10.7469s) loss_train: 1.7829 acc_train: 0.7406 loss_val: 1.7997 acc_val: 0.6860\n",
            "Epoch: 0049 (10.9037s) loss_train: 1.7798 acc_train: 0.7395 loss_val: 1.7970 acc_val: 0.6880\n",
            "Epoch: 0050 (10.8684s) loss_train: 1.7766 acc_train: 0.7400 loss_val: 1.7941 acc_val: 0.6840\n",
            "Epoch: 0051 (10.6566s) loss_train: 1.7735 acc_train: 0.7395 loss_val: 1.7913 acc_val: 0.6860\n",
            "Epoch: 0052 (9.9162s) loss_train: 1.7702 acc_train: 0.7417 loss_val: 1.7883 acc_val: 0.6860\n",
            "Epoch: 0053 (10.7824s) loss_train: 1.7671 acc_train: 0.7417 loss_val: 1.7854 acc_val: 0.6880\n",
            "Epoch: 0054 (10.9145s) loss_train: 1.7641 acc_train: 0.7427 loss_val: 1.7824 acc_val: 0.6900\n",
            "Epoch: 0055 (10.8312s) loss_train: 1.7611 acc_train: 0.7433 loss_val: 1.7795 acc_val: 0.6900\n",
            "Epoch: 0056 (10.5170s) loss_train: 1.7580 acc_train: 0.7422 loss_val: 1.7765 acc_val: 0.6940\n",
            "Epoch: 0057 (10.0864s) loss_train: 1.7548 acc_train: 0.7422 loss_val: 1.7734 acc_val: 0.6900\n",
            "Epoch: 0058 (10.8549s) loss_train: 1.7515 acc_train: 0.7411 loss_val: 1.7703 acc_val: 0.6940\n",
            "Epoch: 0059 (10.8040s) loss_train: 1.7480 acc_train: 0.7438 loss_val: 1.7670 acc_val: 0.6980\n",
            "Epoch: 0060 (10.7781s) loss_train: 1.7445 acc_train: 0.7466 loss_val: 1.7638 acc_val: 0.6960\n",
            "Epoch: 0061 (10.1581s) loss_train: 1.7410 acc_train: 0.7460 loss_val: 1.7606 acc_val: 0.6960\n",
            "Epoch: 0062 (10.2331s) loss_train: 1.7373 acc_train: 0.7449 loss_val: 1.7572 acc_val: 0.7000\n",
            "Epoch: 0063 (10.8654s) loss_train: 1.7337 acc_train: 0.7455 loss_val: 1.7539 acc_val: 0.6980\n",
            "Epoch: 0064 (10.9442s) loss_train: 1.7302 acc_train: 0.7422 loss_val: 1.7507 acc_val: 0.6940\n",
            "Epoch: 0065 (10.9151s) loss_train: 1.7267 acc_train: 0.7444 loss_val: 1.7476 acc_val: 0.6900\n",
            "Epoch: 0066 (10.0924s) loss_train: 1.7229 acc_train: 0.7433 loss_val: 1.7442 acc_val: 0.6900\n",
            "Epoch: 0067 (21.0944s) loss_train: 1.7194 acc_train: 0.7438 loss_val: 1.7410 acc_val: 0.6920\n",
            "Epoch: 0068 (11.7071s) loss_train: 1.7159 acc_train: 0.7438 loss_val: 1.7379 acc_val: 0.6960\n",
            "Epoch: 0069 (10.1477s) loss_train: 1.7127 acc_train: 0.7455 loss_val: 1.7352 acc_val: 0.7000\n",
            "Epoch: 0070 (10.8372s) loss_train: 1.7093 acc_train: 0.7449 loss_val: 1.7322 acc_val: 0.7000\n",
            "Epoch: 0071 (10.8410s) loss_train: 1.7061 acc_train: 0.7466 loss_val: 1.7295 acc_val: 0.7020\n",
            "Epoch: 0072 (10.8905s) loss_train: 1.7031 acc_train: 0.7482 loss_val: 1.7268 acc_val: 0.7000\n",
            "Epoch: 0073 (10.2404s) loss_train: 1.7000 acc_train: 0.7482 loss_val: 1.7241 acc_val: 0.7000\n",
            "Epoch: 0074 (10.3031s) loss_train: 1.6974 acc_train: 0.7493 loss_val: 1.7216 acc_val: 0.7020\n",
            "Epoch: 0075 (10.8148s) loss_train: 1.6951 acc_train: 0.7526 loss_val: 1.7196 acc_val: 0.7040\n",
            "Epoch: 0076 (11.2381s) loss_train: 1.6928 acc_train: 0.7548 loss_val: 1.7178 acc_val: 0.7060\n",
            "Epoch: 0077 (10.8927s) loss_train: 1.6907 acc_train: 0.7564 loss_val: 1.7160 acc_val: 0.7080\n",
            "Epoch: 0078 (10.2555s) loss_train: 1.6885 acc_train: 0.7564 loss_val: 1.7145 acc_val: 0.7000\n",
            "Epoch: 0079 (10.3568s) loss_train: 1.6867 acc_train: 0.7553 loss_val: 1.7131 acc_val: 0.7020\n",
            "Epoch: 0080 (10.7612s) loss_train: 1.6849 acc_train: 0.7570 loss_val: 1.7116 acc_val: 0.7100\n",
            "Epoch: 0081 (10.8596s) loss_train: 1.6832 acc_train: 0.7537 loss_val: 1.7101 acc_val: 0.7040\n",
            "Epoch: 0082 (10.8701s) loss_train: 1.6816 acc_train: 0.7521 loss_val: 1.7090 acc_val: 0.7080\n",
            "Epoch: 0083 (10.0466s) loss_train: 1.6802 acc_train: 0.7510 loss_val: 1.7078 acc_val: 0.7080\n",
            "Epoch: 0084 (10.5629s) loss_train: 1.6792 acc_train: 0.7482 loss_val: 1.7070 acc_val: 0.7100\n",
            "Epoch: 0085 (10.9641s) loss_train: 1.6782 acc_train: 0.7510 loss_val: 1.7063 acc_val: 0.7060\n",
            "Epoch: 0086 (10.8623s) loss_train: 1.6771 acc_train: 0.7526 loss_val: 1.7054 acc_val: 0.7020\n",
            "Epoch: 0087 (10.8632s) loss_train: 1.6761 acc_train: 0.7537 loss_val: 1.7044 acc_val: 0.7020\n",
            "Epoch: 0088 (10.0361s) loss_train: 1.6751 acc_train: 0.7592 loss_val: 1.7033 acc_val: 0.7060\n",
            "Epoch: 0089 (10.5745s) loss_train: 1.6738 acc_train: 0.7592 loss_val: 1.7020 acc_val: 0.7060\n",
            "Epoch: 0090 (10.9736s) loss_train: 1.6723 acc_train: 0.7581 loss_val: 1.7008 acc_val: 0.7080\n",
            "Epoch: 0091 (10.9126s) loss_train: 1.6706 acc_train: 0.7597 loss_val: 1.6993 acc_val: 0.7080\n",
            "Epoch: 0092 (10.8750s) loss_train: 1.6691 acc_train: 0.7592 loss_val: 1.6980 acc_val: 0.7060\n",
            "Epoch: 0093 (9.7813s) loss_train: 1.6674 acc_train: 0.7614 loss_val: 1.6967 acc_val: 0.7080\n",
            "Epoch: 0094 (10.8833s) loss_train: 1.6659 acc_train: 0.7625 loss_val: 1.6955 acc_val: 0.7080\n",
            "Epoch: 0095 (10.8885s) loss_train: 1.6640 acc_train: 0.7635 loss_val: 1.6938 acc_val: 0.7080\n",
            "Epoch: 0096 (11.0029s) loss_train: 1.6620 acc_train: 0.7641 loss_val: 1.6922 acc_val: 0.7120\n",
            "Epoch: 0097 (10.9315s) loss_train: 1.6605 acc_train: 0.7625 loss_val: 1.6909 acc_val: 0.7100\n",
            "Epoch: 0098 (9.7783s) loss_train: 1.6592 acc_train: 0.7630 loss_val: 1.6900 acc_val: 0.7060\n",
            "Epoch: 0099 (10.7836s) loss_train: 1.6582 acc_train: 0.7619 loss_val: 1.6892 acc_val: 0.7080\n",
            "Epoch: 0100 (10.8008s) loss_train: 1.6571 acc_train: 0.7614 loss_val: 1.6883 acc_val: 0.7080\n",
            "Epoch: 0101 (10.8488s) loss_train: 1.6561 acc_train: 0.7614 loss_val: 1.6874 acc_val: 0.7080\n",
            "Epoch: 0102 (10.7117s) loss_train: 1.6548 acc_train: 0.7614 loss_val: 1.6861 acc_val: 0.7100\n",
            "Epoch: 0103 (9.8246s) loss_train: 1.6534 acc_train: 0.7619 loss_val: 1.6846 acc_val: 0.7100\n",
            "Epoch: 0104 (10.8334s) loss_train: 1.6524 acc_train: 0.7635 loss_val: 1.6834 acc_val: 0.7100\n",
            "Epoch: 0105 (10.9657s) loss_train: 1.6514 acc_train: 0.7652 loss_val: 1.6821 acc_val: 0.7120\n",
            "Epoch: 0106 (10.8602s) loss_train: 1.6502 acc_train: 0.7635 loss_val: 1.6808 acc_val: 0.7080\n",
            "Epoch: 0107 (10.6786s) loss_train: 1.6488 acc_train: 0.7657 loss_val: 1.6793 acc_val: 0.7100\n",
            "Epoch: 0108 (9.9993s) loss_train: 1.6468 acc_train: 0.7674 loss_val: 1.6776 acc_val: 0.7160\n",
            "Epoch: 0109 (10.8639s) loss_train: 1.6449 acc_train: 0.7679 loss_val: 1.6763 acc_val: 0.7160\n",
            "Epoch: 0110 (10.9326s) loss_train: 1.6427 acc_train: 0.7701 loss_val: 1.6747 acc_val: 0.7180\n",
            "Epoch: 0111 (10.8674s) loss_train: 1.6402 acc_train: 0.7712 loss_val: 1.6729 acc_val: 0.7180\n",
            "Epoch: 0112 (10.5996s) loss_train: 1.6375 acc_train: 0.7723 loss_val: 1.6711 acc_val: 0.7180\n",
            "Epoch: 0113 (9.9883s) loss_train: 1.6351 acc_train: 0.7734 loss_val: 1.6694 acc_val: 0.7180\n",
            "Epoch: 0114 (10.8188s) loss_train: 1.6334 acc_train: 0.7750 loss_val: 1.6681 acc_val: 0.7180\n",
            "Epoch: 0115 (10.8370s) loss_train: 1.6323 acc_train: 0.7723 loss_val: 1.6672 acc_val: 0.7200\n",
            "Epoch: 0116 (10.8711s) loss_train: 1.6312 acc_train: 0.7707 loss_val: 1.6666 acc_val: 0.7180\n",
            "Epoch: 0117 (10.5169s) loss_train: 1.6304 acc_train: 0.7696 loss_val: 1.6662 acc_val: 0.7140\n",
            "Epoch: 0118 (10.1373s) loss_train: 1.6300 acc_train: 0.7696 loss_val: 1.6663 acc_val: 0.7140\n",
            "Epoch: 0119 (10.8256s) loss_train: 1.6292 acc_train: 0.7685 loss_val: 1.6659 acc_val: 0.7120\n",
            "Epoch: 0120 (10.8878s) loss_train: 1.6280 acc_train: 0.7674 loss_val: 1.6652 acc_val: 0.7120\n",
            "Epoch: 0121 (10.9214s) loss_train: 1.6271 acc_train: 0.7712 loss_val: 1.6649 acc_val: 0.7140\n",
            "Epoch: 0122 (10.3506s) loss_train: 1.6261 acc_train: 0.7723 loss_val: 1.6645 acc_val: 0.7140\n",
            "Epoch: 0123 (21.3745s) loss_train: 1.6253 acc_train: 0.7734 loss_val: 1.6644 acc_val: 0.7120\n",
            "Epoch: 0124 (11.8665s) loss_train: 1.6247 acc_train: 0.7723 loss_val: 1.6647 acc_val: 0.7080\n",
            "Epoch: 0125 (9.9649s) loss_train: 1.6242 acc_train: 0.7745 loss_val: 1.6651 acc_val: 0.7060\n",
            "Epoch: 0126 (10.5503s) loss_train: 1.6233 acc_train: 0.7739 loss_val: 1.6650 acc_val: 0.7040\n",
            "Epoch: 0127 (10.7664s) loss_train: 1.6227 acc_train: 0.7750 loss_val: 1.6649 acc_val: 0.7040\n",
            "Epoch: 0128 (10.8125s) loss_train: 1.6220 acc_train: 0.7756 loss_val: 1.6645 acc_val: 0.7040\n",
            "Epoch: 0129 (10.6575s) loss_train: 1.6213 acc_train: 0.7761 loss_val: 1.6639 acc_val: 0.7080\n",
            "Epoch: 0130 (9.8818s) loss_train: 1.6209 acc_train: 0.7767 loss_val: 1.6637 acc_val: 0.7100\n",
            "Epoch: 0131 (10.7676s) loss_train: 1.6205 acc_train: 0.7761 loss_val: 1.6633 acc_val: 0.7120\n",
            "Epoch: 0132 (10.9247s) loss_train: 1.6203 acc_train: 0.7750 loss_val: 1.6630 acc_val: 0.7120\n",
            "Epoch: 0133 (10.8165s) loss_train: 1.6196 acc_train: 0.7761 loss_val: 1.6623 acc_val: 0.7140\n",
            "Epoch: 0134 (10.5241s) loss_train: 1.6188 acc_train: 0.7767 loss_val: 1.6615 acc_val: 0.7100\n",
            "Epoch: 0135 (9.9646s) loss_train: 1.6172 acc_train: 0.7778 loss_val: 1.6600 acc_val: 0.7120\n",
            "Epoch: 0136 (10.7990s) loss_train: 1.6155 acc_train: 0.7789 loss_val: 1.6581 acc_val: 0.7100\n",
            "Epoch: 0137 (10.9243s) loss_train: 1.6136 acc_train: 0.7794 loss_val: 1.6560 acc_val: 0.7100\n",
            "Epoch: 0138 (10.8777s) loss_train: 1.6118 acc_train: 0.7811 loss_val: 1.6541 acc_val: 0.7100\n",
            "Epoch: 0139 (10.4716s) loss_train: 1.6103 acc_train: 0.7805 loss_val: 1.6524 acc_val: 0.7120\n",
            "Epoch: 0140 (10.1725s) loss_train: 1.6086 acc_train: 0.7800 loss_val: 1.6508 acc_val: 0.7160\n",
            "Epoch: 0141 (10.8487s) loss_train: 1.6072 acc_train: 0.7794 loss_val: 1.6494 acc_val: 0.7160\n",
            "Epoch: 0142 (10.8759s) loss_train: 1.6054 acc_train: 0.7805 loss_val: 1.6477 acc_val: 0.7160\n",
            "Epoch: 0143 (10.9200s) loss_train: 1.6038 acc_train: 0.7838 loss_val: 1.6464 acc_val: 0.7160\n",
            "Epoch: 0144 (10.4160s) loss_train: 1.6025 acc_train: 0.7833 loss_val: 1.6453 acc_val: 0.7180\n",
            "Epoch: 0145 (10.2183s) loss_train: 1.6011 acc_train: 0.7865 loss_val: 1.6442 acc_val: 0.7160\n",
            "Epoch: 0146 (10.8723s) loss_train: 1.5998 acc_train: 0.7849 loss_val: 1.6434 acc_val: 0.7160\n",
            "Epoch: 0147 (10.8929s) loss_train: 1.5991 acc_train: 0.7849 loss_val: 1.6430 acc_val: 0.7160\n",
            "Epoch: 0148 (10.8549s) loss_train: 1.5985 acc_train: 0.7838 loss_val: 1.6426 acc_val: 0.7180\n",
            "Epoch: 0149 (10.2884s) loss_train: 1.5983 acc_train: 0.7827 loss_val: 1.6427 acc_val: 0.7140\n",
            "Epoch: 0150 (10.2280s) loss_train: 1.5980 acc_train: 0.7860 loss_val: 1.6425 acc_val: 0.7120\n",
            "Epoch: 0151 (10.8384s) loss_train: 1.5980 acc_train: 0.7865 loss_val: 1.6425 acc_val: 0.7140\n",
            "Epoch: 0152 (10.9010s) loss_train: 1.5983 acc_train: 0.7876 loss_val: 1.6425 acc_val: 0.7220\n",
            "Epoch: 0153 (10.7834s) loss_train: 1.5987 acc_train: 0.7904 loss_val: 1.6425 acc_val: 0.7220\n",
            "Epoch: 0154 (9.7758s) loss_train: 1.5990 acc_train: 0.7920 loss_val: 1.6424 acc_val: 0.7220\n",
            "Epoch: 0155 (10.6896s) loss_train: 1.5994 acc_train: 0.7909 loss_val: 1.6423 acc_val: 0.7200\n",
            "Epoch: 0156 (10.8784s) loss_train: 1.5999 acc_train: 0.7893 loss_val: 1.6425 acc_val: 0.7160\n",
            "Epoch: 0157 (10.8296s) loss_train: 1.5998 acc_train: 0.7893 loss_val: 1.6424 acc_val: 0.7160\n",
            "Epoch: 0158 (10.6624s) loss_train: 1.5990 acc_train: 0.7860 loss_val: 1.6420 acc_val: 0.7160\n",
            "Epoch: 0159 (9.8914s) loss_train: 1.5983 acc_train: 0.7854 loss_val: 1.6419 acc_val: 0.7140\n",
            "Epoch: 0160 (12.3427s) loss_train: 1.5972 acc_train: 0.7871 loss_val: 1.6416 acc_val: 0.7140\n",
            "Epoch: 0161 (10.9399s) loss_train: 1.5960 acc_train: 0.7876 loss_val: 1.6412 acc_val: 0.7220\n",
            "Epoch: 0162 (10.9226s) loss_train: 1.5949 acc_train: 0.7893 loss_val: 1.6410 acc_val: 0.7200\n",
            "Epoch: 0163 (10.8936s) loss_train: 1.5937 acc_train: 0.7887 loss_val: 1.6405 acc_val: 0.7180\n",
            "Epoch: 0164 (10.2962s) loss_train: 1.5925 acc_train: 0.7882 loss_val: 1.6400 acc_val: 0.7180\n",
            "Epoch: 0165 (10.3429s) loss_train: 1.5912 acc_train: 0.7887 loss_val: 1.6396 acc_val: 0.7200\n",
            "Epoch: 0166 (10.8549s) loss_train: 1.5900 acc_train: 0.7893 loss_val: 1.6390 acc_val: 0.7160\n",
            "Epoch: 0167 (10.8734s) loss_train: 1.5889 acc_train: 0.7887 loss_val: 1.6383 acc_val: 0.7160\n",
            "Epoch: 0168 (10.8286s) loss_train: 1.5879 acc_train: 0.7843 loss_val: 1.6377 acc_val: 0.7160\n",
            "Epoch: 0169 (10.1234s) loss_train: 1.5867 acc_train: 0.7860 loss_val: 1.6367 acc_val: 0.7160\n",
            "Epoch: 0170 (10.4561s) loss_train: 1.5860 acc_train: 0.7838 loss_val: 1.6360 acc_val: 0.7160\n",
            "Epoch: 0171 (10.8662s) loss_train: 1.5856 acc_train: 0.7849 loss_val: 1.6351 acc_val: 0.7140\n",
            "Epoch: 0172 (10.8982s) loss_train: 1.5856 acc_train: 0.7833 loss_val: 1.6344 acc_val: 0.7120\n",
            "Epoch: 0173 (10.9313s) loss_train: 1.5855 acc_train: 0.7838 loss_val: 1.6337 acc_val: 0.7160\n",
            "Epoch: 0174 (10.0960s) loss_train: 1.5855 acc_train: 0.7843 loss_val: 1.6332 acc_val: 0.7160\n",
            "Epoch: 0175 (10.5674s) loss_train: 1.5858 acc_train: 0.7854 loss_val: 1.6329 acc_val: 0.7100\n",
            "Epoch: 0176 (10.8363s) loss_train: 1.5866 acc_train: 0.7865 loss_val: 1.6331 acc_val: 0.7100\n",
            "Epoch: 0177 (10.7567s) loss_train: 1.5871 acc_train: 0.7849 loss_val: 1.6331 acc_val: 0.7120\n",
            "Epoch: 0178 (10.7362s) loss_train: 1.5870 acc_train: 0.7843 loss_val: 1.6326 acc_val: 0.7120\n",
            "Epoch: 0179 (20.3310s) loss_train: 1.5868 acc_train: 0.7865 loss_val: 1.6323 acc_val: 0.7200\n",
            "Epoch: 0180 (11.9153s) loss_train: 1.5868 acc_train: 0.7882 loss_val: 1.6323 acc_val: 0.7220\n",
            "Epoch: 0181 (9.8752s) loss_train: 1.5872 acc_train: 0.7898 loss_val: 1.6326 acc_val: 0.7260\n",
            "Epoch: 0182 (10.6378s) loss_train: 1.5877 acc_train: 0.7909 loss_val: 1.6331 acc_val: 0.7280\n",
            "Epoch: 0183 (10.9466s) loss_train: 1.5883 acc_train: 0.7882 loss_val: 1.6337 acc_val: 0.7300\n",
            "Epoch: 0184 (10.8919s) loss_train: 1.5887 acc_train: 0.7893 loss_val: 1.6343 acc_val: 0.7320\n",
            "Epoch: 0185 (10.8756s) loss_train: 1.5890 acc_train: 0.7937 loss_val: 1.6349 acc_val: 0.7300\n",
            "Epoch: 0186 (9.6593s) loss_train: 1.5896 acc_train: 0.7947 loss_val: 1.6353 acc_val: 0.7320\n",
            "Epoch: 0187 (10.7187s) loss_train: 1.5900 acc_train: 0.7931 loss_val: 1.6356 acc_val: 0.7300\n",
            "Epoch: 0188 (10.8026s) loss_train: 1.5908 acc_train: 0.7937 loss_val: 1.6363 acc_val: 0.7280\n",
            "Epoch: 0189 (10.7605s) loss_train: 1.5912 acc_train: 0.7931 loss_val: 1.6369 acc_val: 0.7260\n",
            "Epoch: 0190 (10.3413s) loss_train: 1.5914 acc_train: 0.7909 loss_val: 1.6375 acc_val: 0.7240\n",
            "Epoch: 0191 (10.1594s) loss_train: 1.5916 acc_train: 0.7920 loss_val: 1.6377 acc_val: 0.7280\n",
            "Epoch: 0192 (10.7319s) loss_train: 1.5917 acc_train: 0.7931 loss_val: 1.6377 acc_val: 0.7300\n",
            "Epoch: 0193 (10.8492s) loss_train: 1.5918 acc_train: 0.7937 loss_val: 1.6377 acc_val: 0.7320\n",
            "Epoch: 0194 (22.1539s) loss_train: 1.5917 acc_train: 0.7942 loss_val: 1.6377 acc_val: 0.7320\n",
            "Epoch: 0195 (17.4600s) loss_train: 1.5914 acc_train: 0.7947 loss_val: 1.6374 acc_val: 0.7280\n",
            "Epoch: 0196 (10.0122s) loss_train: 1.5908 acc_train: 0.7958 loss_val: 1.6370 acc_val: 0.7280\n",
            "Epoch: 0197 (10.4341s) loss_train: 1.5902 acc_train: 0.7953 loss_val: 1.6367 acc_val: 0.7280\n",
            "Epoch: 0198 (10.8280s) loss_train: 1.5894 acc_train: 0.7953 loss_val: 1.6362 acc_val: 0.7280\n",
            "Epoch: 0199 (12.3582s) loss_train: 1.5878 acc_train: 0.7947 loss_val: 1.6353 acc_val: 0.7260\n",
            "Epoch: 0200 (12.2703s) loss_train: 1.5862 acc_train: 0.7986 loss_val: 1.6343 acc_val: 0.7220\n",
            "Optimization Finished!\n",
            "Total time elapsed: 2216.4519s\n",
            "Loading 184th epoch\n",
            "Test set results: loss 1.6275 accuracy 0.7290\n"
          ]
        }
      ],
      "source": [
        "# 3\n",
        "\n",
        "best_model, loss_values, acc_values, loss_test, acc_test = train_and_evaluate(GAT_citeseer, (features, adj_mat), labels, idx_train, idx_val, args['epochs'], args['patience'], args['print'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62kbcIvgI_SB",
        "outputId": "279858fe-0078-4966-da92-aa1e6b0737e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set results: loss 1.6275 accuracy 0.7290\n"
          ]
        }
      ],
      "source": [
        "#  DO NOT RUN THIS\n",
        "loss_test, acc_test = evaluate(GAT_citeseer, criterion, (features, adj_mat), labels, idx_test)\n",
        "print(f'Test set results: loss {loss_test:.4f} accuracy {acc_test:.4f}')\n",
        "# Test set results: loss 1.6275 accuracy 0.7290"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "JlagN1LzJBWz",
        "outputId": "82d70596-3e8c-4a8e-b58d-656a2f53ae41"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#train_losses, train_scores, val_losses, val_scores\n",
        "def plot_accuracy(epoch_list, train_losses, val_scores):\n",
        "    plt.figure(figsize = [10,5])\n",
        "    plt.plot(epoch_list, train_losses, 'b', label = \"validation loss\")\n",
        "    plt.plot(epoch_list, val_scores, 'r', label = \"validation accuracy\")\n",
        "    plt.title(\"Evolution of validation accuracy and loss w.r.t epochs\")\n",
        "    plt.ylim([0.0, 2.0])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.savefig('/content/drive/My Drive/GAT_citeseer_200_epoch.png')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "epoch_list = list(range(1, 201))\n",
        "plot_accuracy(epoch_list, loss_values, acc_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qm8woEM-3MT1"
      },
      "outputs": [],
      "source": [
        "def run_transductive_experiment(iters=100):\n",
        "  losses = []\n",
        "  scores = []\n",
        "  for iter in range(iters):\n",
        "    #features, labels, adj_mat = load_data(device=device)\n",
        "    #idx = torch.randperm(len(labels)).to(device)\n",
        "    #idx_test, idx_val, idx_train = idx[:1000], idx[1000:1500], idx[1500:]\n",
        "    best_model, loss_values, acc_values, loss_test, acc_test = train_and_evaluate(GAT_citeseer, (features, adj_mat), labels, idx_train, idx_val, args['epochs'], args['patience'], False)\n",
        "    # best_model,_ ,_ ,_ ,_ = train(model, ppi_train_params, verbose=False)\n",
        "    # loss, score = evaluate(best_model, test_loader)\n",
        "    losses.append(loss_test)\n",
        "    scores.append(acc_test)\n",
        "  losses = torch.tensor(losses)\n",
        "  scores = torch.tensor(scores)\n",
        "  # return (torch.std_mean(losses), torch.std_mean(scores))\n",
        "  return losses, scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZQubCe-9O12",
        "outputId": "8e3894a9-2716-455d-953f-86dfa74ecdd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping...\n",
            "Best Val Loss: 1.6196072101593018, Best Val Acc: 0.7300000190734863\n",
            "Optimization Finished!\n",
            "Total time elapsed: 1772.5080s\n",
            "Loading 63th epoch\n",
            "Test set results: loss 1.6147 accuracy 0.7230\n",
            "Optimization Finished!\n",
            "Total time elapsed: 2198.5439s\n",
            "Loading 132th epoch\n",
            "Test set results: loss 1.6199 accuracy 0.7230\n"
          ]
        }
      ],
      "source": [
        "losses, scores = run_transductive_experiment(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQKo_jWgIS62",
        "outputId": "9ccec6e9-5259-407d-f8cd-64559064c20a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimization Finished!\n",
            "Total time elapsed: 44.3754s\n",
            "Loading 194th epoch\n",
            "Test set results: loss 1.6210 accuracy 0.7310\n",
            "Early stopping...\n",
            "Best Val Loss: 1.6216, Best Val Acc: 0.7320\n",
            "Optimization Finished!\n",
            "Total time elapsed: 20.0919s\n",
            "Loading 50th epoch\n",
            "Test set results: loss 1.6128 accuracy 0.7310\n"
          ]
        }
      ],
      "source": [
        "losses_0, scores_0 = run_transductive_experiment(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzkeQ66w9Bvx",
        "outputId": "5c92c595-0d82-487b-9543-dc1f70e059a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping...\n",
            "Best Val Loss: 1.6219, Best Val Acc: 0.7380\n",
            "Optimization Finished!\n",
            "Total time elapsed: 22.7520s\n",
            "Loading 91th epoch\n",
            "Test set results: loss 1.6189 accuracy 0.7320\n",
            "Early stopping...\n",
            "Best Val Loss: 1.6299, Best Val Acc: 0.7420\n",
            "Optimization Finished!\n",
            "Total time elapsed: 11.8675s\n",
            "Loading 1th epoch\n",
            "Test set results: loss 1.6257 accuracy 0.7290\n"
          ]
        }
      ],
      "source": [
        "losses_1, scores_1 = run_transductive_experiment(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6R_a3gH9BfK",
        "outputId": "574fc253-c867-412b-e954-d80310d78279"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping...\n",
            "Best Val Loss: 1.6324, Best Val Acc: 0.7380\n",
            "Optimization Finished!\n",
            "Total time elapsed: 14.8922s\n",
            "Loading 17th epoch\n",
            "Test set results: loss 1.6221 accuracy 0.7280\n",
            "Early stopping...\n",
            "Best Val Loss: 1.6272, Best Val Acc: 0.7300\n",
            "Optimization Finished!\n",
            "Total time elapsed: 26.8779s\n",
            "Loading 92th epoch\n",
            "Test set results: loss 1.6215 accuracy 0.7260\n"
          ]
        }
      ],
      "source": [
        "losses_2, scores_2 = run_transductive_experiment(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ID35U4z99BN9",
        "outputId": "9ed0078b-f6f3-4fc5-c140-8eee195373f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping...\n",
            "Best Val Loss: 1.6324, Best Val Acc: 0.7400\n",
            "Optimization Finished!\n",
            "Total time elapsed: 17.2719s\n",
            "Loading 33th epoch\n",
            "Test set results: loss 1.6374 accuracy 0.7280\n",
            "Optimization Finished!\n",
            "Total time elapsed: 27.0879s\n",
            "Loading 115th epoch\n",
            "Test set results: loss 1.6254 accuracy 0.7230\n"
          ]
        }
      ],
      "source": [
        "losses_3, scores_3 = run_transductive_experiment(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSZkpVL69Aqm",
        "outputId": "af083fe6-28a5-4489-a875-fbf9dcbfb958"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping...\n",
            "Best Val Loss: 1.6348, Best Val Acc: 0.7360\n",
            "Optimization Finished!\n",
            "Total time elapsed: 19.4610s\n",
            "Loading 62th epoch\n",
            "Test set results: loss 1.6278 accuracy 0.7270\n",
            "Optimization Finished!\n",
            "Total time elapsed: 24.9122s\n",
            "Loading 198th epoch\n",
            "Test set results: loss 1.6332 accuracy 0.7220\n"
          ]
        }
      ],
      "source": [
        "losses_4, scores_4 = run_transductive_experiment(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTHmPNa6RhwS",
        "outputId": "d9ee0a7a-d202-4aea-e21a-5d6d8829d71f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimization Finished!\n",
            "Total time elapsed: 26.0711s\n",
            "Loading 169th epoch\n",
            "Test set results: loss 1.6277 accuracy 0.7180\n",
            "Optimization Finished!\n",
            "Total time elapsed: 25.1110s\n",
            "Loading 166th epoch\n",
            "Test set results: loss 1.6329 accuracy 0.7270\n"
          ]
        }
      ],
      "source": [
        "losses_5, scores_5 = run_transductive_experiment(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Oz2s8RERjav",
        "outputId": "289f55b4-ee69-4098-da3c-b87e6f143214"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping...\n",
            "Best Val Loss: 1.6380, Best Val Acc: 0.7360\n",
            "Optimization Finished!\n",
            "Total time elapsed: 16.2396s\n",
            "Loading 31th epoch\n",
            "Test set results: loss 1.6321 accuracy 0.7270\n",
            "Optimization Finished!\n",
            "Total time elapsed: 25.2433s\n",
            "Loading 169th epoch\n",
            "Test set results: loss 1.6329 accuracy 0.7270\n"
          ]
        }
      ],
      "source": [
        "losses_6, scores_6 = run_transductive_experiment(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVaAo2jPRjMu",
        "outputId": "ca51af55-6611-4a77-8d7f-8913630d8e2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimization Finished!\n",
            "Total time elapsed: 26.2348s\n",
            "Loading 120th epoch\n",
            "Test set results: loss 1.6286 accuracy 0.7240\n",
            "Optimization Finished!\n",
            "Total time elapsed: 24.5503s\n",
            "Loading 141th epoch\n",
            "Test set results: loss 1.6281 accuracy 0.7320\n"
          ]
        }
      ],
      "source": [
        "losses_7, scores_7 = run_transductive_experiment(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMO3iOYzRi-O",
        "outputId": "bd1d3fb7-2eb9-4fd7-8baa-47e482590562"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping...\n",
            "Best Val Loss: 1.6348, Best Val Acc: 0.7380\n",
            "Optimization Finished!\n",
            "Total time elapsed: 18.2400s\n",
            "Loading 42th epoch\n",
            "Test set results: loss 1.6341 accuracy 0.7240\n",
            "Optimization Finished!\n",
            "Total time elapsed: 25.8495s\n",
            "Loading 150th epoch\n",
            "Test set results: loss 1.6280 accuracy 0.7260\n"
          ]
        }
      ],
      "source": [
        "losses_8, scores_8 = run_transductive_experiment(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DmpoZZSRisG",
        "outputId": "313ec5c4-342b-4241-9cd3-d6046c117a05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping...\n",
            "Best Val Loss: 1.6358, Best Val Acc: 0.7360\n",
            "Optimization Finished!\n",
            "Total time elapsed: 17.0150s\n",
            "Loading 42th epoch\n",
            "Test set results: loss 1.6323 accuracy 0.7280\n",
            "Optimization Finished!\n",
            "Total time elapsed: 30.2767s\n",
            "Loading 124th epoch\n",
            "Test set results: loss 1.6283 accuracy 0.7190\n"
          ]
        }
      ],
      "source": [
        "losses_9, scores_9 = run_transductive_experiment(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9D-jovmRiUu",
        "outputId": "cc000f0a-7ed1-4b92-ad65-7b85c20e6751"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimization Finished!\n",
            "Total time elapsed: 24.1893s\n",
            "Loading 187th epoch\n",
            "Test set results: loss 1.6332 accuracy 0.7220\n",
            "Early stopping...\n",
            "Best Val Loss: 1.6354, Best Val Acc: 0.7340\n",
            "Optimization Finished!\n",
            "Total time elapsed: 19.7346s\n",
            "Loading 46th epoch\n",
            "Test set results: loss 1.6285 accuracy 0.7230\n"
          ]
        }
      ],
      "source": [
        "losses_10, scores_10 = run_transductive_experiment(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kf6cD_isqLNz",
        "outputId": "3f02794e-1e85-4009-eaee-a6943924c7cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimization Finished!\n",
            "Total time elapsed: 25.3853s\n",
            "Loading 126th epoch\n",
            "Test set results: loss 1.6249 accuracy 0.7290\n",
            "Optimization Finished!\n",
            "Total time elapsed: 25.5544s\n",
            "Loading 157th epoch\n",
            "Test set results: loss 1.6336 accuracy 0.7330\n",
            "Early stopping...\n",
            "Best Val Loss: 1.6361, Best Val Acc: 0.7360\n",
            "Optimization Finished!\n",
            "Total time elapsed: 25.0489s\n",
            "Loading 97th epoch\n",
            "Test set results: loss 1.6267 accuracy 0.7320\n",
            "Early stopping...\n",
            "Best Val Loss: 1.6338, Best Val Acc: 0.7340\n",
            "Optimization Finished!\n",
            "Total time elapsed: 24.4906s\n",
            "Loading 74th epoch\n",
            "Test set results: loss 1.6395 accuracy 0.7230\n",
            "Early stopping...\n",
            "Best Val Loss: 1.6338, Best Val Acc: 0.7340\n",
            "Optimization Finished!\n",
            "Total time elapsed: 19.9166s\n",
            "Loading 52th epoch\n",
            "Test set results: loss 1.6273 accuracy 0.7320\n",
            "Early stopping...\n",
            "Best Val Loss: 1.6350, Best Val Acc: 0.7440\n",
            "Optimization Finished!\n",
            "Total time elapsed: 25.0692s\n",
            "Loading 99th epoch\n",
            "Test set results: loss 1.6290 accuracy 0.7200\n",
            "Early stopping...\n",
            "Best Val Loss: 1.6333, Best Val Acc: 0.7380\n",
            "Optimization Finished!\n",
            "Total time elapsed: 17.4635s\n",
            "Loading 34th epoch\n",
            "Test set results: loss 1.6277 accuracy 0.7280\n",
            "Early stopping...\n",
            "Best Val Loss: 1.6370, Best Val Acc: 0.7360\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.4299s\n",
            "Loading 25th epoch\n",
            "Test set results: loss 1.6322 accuracy 0.7300\n",
            "Early stopping...\n",
            "Best Val Loss: 1.6356, Best Val Acc: 0.7320\n",
            "Optimization Finished!\n",
            "Total time elapsed: 18.8756s\n",
            "Loading 49th epoch\n",
            "Test set results: loss 1.6351 accuracy 0.7300\n",
            "Optimization Finished!\n",
            "Total time elapsed: 27.8364s\n",
            "Loading 139th epoch\n",
            "Test set results: loss 1.6327 accuracy 0.7250\n"
          ]
        }
      ],
      "source": [
        "losses, scores = run_transductive_experiment(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "DGbpZg_f8rzm",
        "outputId": "1b6a674d-e4b9-402e-e67e-54faa5ef5281"
      },
      "outputs": [],
      "source": [
        "loss_std, loss_mean = torch.std_mean(losses )\n",
        "score_std, score_mean = torch.std_mean(scores + scores_1 + scores_2 + scores_3 + scores_4 + scores_5 + scores_6 + scores_7 + scores_8 + scores_9 + scores_10)\n",
        "\n",
        "print(f'test loss:\\t\\t{loss_mean:.4f} +/- {loss_std:.4f}')\n",
        "print(f'test accuracy: \\t{score_mean:.4f} +/- {score_std:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYLFNsM8cXFZ"
      },
      "outputs": [],
      "source": [
        "losses_citeseer_100, scores_citeseer_100 = run_transductive_experiment(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvU_f-licdD8"
      },
      "outputs": [],
      "source": [
        "loss_std_citeseer_100, loss_mean_citeseer_100 = torch.std_mean(losses_citeseer_100)\n",
        "score_std_citeseer_100, score_mean_citeseer_100 = torch.std_mean(scores_citeseer_100)\n",
        "\n",
        "print(f'test loss:\\t\\t{loss_mean_citeseer_100:.4f} +/- {loss_std_citeseer_100:.4f}')\n",
        "print(f'test accuracy: \\t{score_mean_citeseer_100:.4f} +/- {score_std_citeseer_100:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "lzpHM853SGYt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import requests\n",
        "import tarfile\n",
        "import numpy as np\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "def load_data(path='./datasets/cora/', device='cpu'):\n",
        "    \"\"\"\n",
        "    Loads the Cora dataset. The dataset is downloaded from https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Set the paths to the data files\n",
        "    content_path = os.path.join(path, 'cora.content')\n",
        "    cites_path = os.path.join(path, 'cora.cites')\n",
        "\n",
        "    # Load data from files\n",
        "    content_tensor = np.genfromtxt(content_path, dtype=np.dtype(str))\n",
        "    cites_tensor = np.genfromtxt(cites_path, dtype=np.int32)\n",
        "\n",
        "\n",
        "    # content_tensor, cites_tensor = preprocess_index(cites_data, content_data)\n",
        "\n",
        "    # Process features\n",
        "    features = torch.FloatTensor(content_tensor[:, 1:-1].astype(np.int32)) # Extract feature values\n",
        "    scale_vector = torch.sum(features, dim=1) # Compute sum of features for each node\n",
        "    scale_vector = 1 / scale_vector # Compute reciprocal of the sums\n",
        "    scale_vector[scale_vector == float('inf')] = 0 # Handle division by zero cases\n",
        "    scale_vector = torch.diag(scale_vector).to_sparse() # Convert the scale vector to a sparse diagonal matrix\n",
        "    features = scale_vector @ features # Scale the features using the scale vector\n",
        "\n",
        "    # Process labels\n",
        "    classes, labels = np.unique(content_tensor[:, -1], return_inverse=True) # Extract unique classes and map labels to indices\n",
        "    labels = torch.LongTensor(labels) # Convert labels to a tensor\n",
        "\n",
        "    # Process adjacency matrix\n",
        "    idx = content_tensor[:, 0].astype(np.int32) # Extract node indices\n",
        "    idx_map = {id: pos for pos, id in enumerate(idx)} # Create a dictionary to map indices to positions\n",
        "\n",
        "    # Map node indices to positions in the adjacency matrix\n",
        "    edges = np.array(\n",
        "        list(map(lambda edge: [idx_map[edge[0]], idx_map[edge[1]]],\n",
        "            cites_tensor)), dtype=np.int32)\n",
        "\n",
        "    V = len(idx) # Number of nodes\n",
        "    E = edges.shape[0] # Number of edges\n",
        "    adj_mat = torch.sparse_coo_tensor(edges.T, torch.ones(E), (V, V), dtype=torch.int64) # Create the initial adjacency matrix as a sparse tensor\n",
        "    adj_mat = torch.eye(V) + adj_mat # Add self-loops to the adjacency matrix\n",
        "\n",
        "    # return features.to_sparse().to(device), labels.to(device), adj_mat.to_sparse().to(device)\n",
        "    return features.to(device), labels.to(device), adj_mat.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqJP12_nR61z",
        "outputId": "663c8904-8785-42d3-c0ef-129bea3ed518"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset already downloaded...\n",
            "Loading dataset...\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "data_url = 'https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz'\n",
        "path = './datasets/cora'\n",
        "\n",
        "if os.path.isfile(os.path.join(path, 'cora.content')) and os.path.isfile(os.path.join(path, 'cora.cites')):\n",
        "    print('Dataset already downloaded...')\n",
        "else:\n",
        "    print('Downloading dataset...')\n",
        "    with requests.get(data_url, stream=True) as tgz_file:\n",
        "        with tarfile.open(fileobj=tgz_file.raw, mode='r:gz') as tgz_object:\n",
        "            tgz_object.extractall()\n",
        "\n",
        "print('Loading dataset...')\n",
        "# Load the dataset\n",
        "features, labels, adj_mat = load_data(device=device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "jzfhqpHFY0Xy"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into training, validation, and test sets\n",
        "idx = torch.randperm(len(labels)).to(device)\n",
        "idx_test, idx_val, idx_train = idx[:1200], idx[1200:1600], idx[1600:]\n",
        "#idx_test, idx_val, idx_train = idx[:1000], idx[1000:1500], idx[1500:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53JRxO2KX_9H",
        "outputId": "4258ac03-4b97-4801-a7a6-7a12dc7075d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "# 2\n",
        "import torch\n",
        "import os\n",
        "import requests\n",
        "import tarfile\n",
        "import glob\n",
        "# Set the parameters directly\n",
        "args = {\n",
        "    'seed': 42,\n",
        "    'no_cuda': False,\n",
        "    'no_mps': False,\n",
        "    'hidden_dim': 64,\n",
        "    'num_heads': 8,\n",
        "    'concat_heads': False,\n",
        "    'dropout_p': 0.6,\n",
        "    'lr': 0.005,\n",
        "    'l2': 5e-4,\n",
        "    'epochs': 200,\n",
        "    'patience': 100,\n",
        "    'print': True,\n",
        "}\n",
        "\n",
        "torch.manual_seed(args['seed'])\n",
        "use_cuda = not args['no_cuda'] and torch.cuda.is_available()\n",
        "use_mps = not args['no_mps'] and torch.backends.mps.is_available()\n",
        "\n",
        "# Set the device to run on\n",
        "if use_cuda:\n",
        "    device = torch.device('cuda')\n",
        "elif use_mps:\n",
        "    device = torch.device('mps')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "print(f'Using {device} device')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "IRRiCIS_SXz-"
      },
      "outputs": [],
      "source": [
        "# Create the model\n",
        "# The model consists of a 2-layer stack of Graph Attention Layers (GATs).\n",
        "GAT_cora = GAT(\n",
        "    in_features=features.shape[1],          # Number of input features per node\n",
        "    n_hidden=args['hidden_dim'],            # Output size of the first Graph Attention Layer\n",
        "    n_heads=args['num_heads'],               # Number of attention heads in the first Graph Attention Layer\n",
        "    num_classes=labels.max().item() + 1,     # Number of classes to predict for each node\n",
        "    concat=args['concat_heads'],             # Whether to concatenate attention heads\n",
        "    dropout=args['dropout_p'],                # Dropout rate\n",
        "    leaky_relu_slope=0.2                     # Alpha (slope) of the leaky ReLU activation\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0k1HnL_VDh5",
        "outputId": "7df4e7b4-ab7f-4968-b902-fb3ad6bc6455"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0001 (0.0914s) loss_train: 1.4804 acc_train: 0.8547 loss_val: 1.4856 acc_val: 0.8450\n",
            "Epoch: 0002 (0.0906s) loss_train: 1.4794 acc_train: 0.8457 loss_val: 1.4893 acc_val: 0.8425\n",
            "Epoch: 0003 (0.0861s) loss_train: 1.4786 acc_train: 0.8475 loss_val: 1.4926 acc_val: 0.8325\n",
            "Epoch: 0004 (0.0752s) loss_train: 1.4771 acc_train: 0.8520 loss_val: 1.4953 acc_val: 0.8300\n",
            "Epoch: 0005 (0.0751s) loss_train: 1.4756 acc_train: 0.8601 loss_val: 1.4979 acc_val: 0.8300\n",
            "Epoch: 0006 (0.0759s) loss_train: 1.4739 acc_train: 0.8601 loss_val: 1.4997 acc_val: 0.8225\n",
            "Epoch: 0007 (0.0755s) loss_train: 1.4725 acc_train: 0.8628 loss_val: 1.5017 acc_val: 0.8175\n",
            "Epoch: 0008 (0.0754s) loss_train: 1.4715 acc_train: 0.8664 loss_val: 1.5036 acc_val: 0.8200\n",
            "Epoch: 0009 (0.0750s) loss_train: 1.4702 acc_train: 0.8700 loss_val: 1.5053 acc_val: 0.8125\n",
            "Epoch: 0010 (0.0755s) loss_train: 1.4686 acc_train: 0.8745 loss_val: 1.5064 acc_val: 0.8050\n",
            "Epoch: 0011 (0.0753s) loss_train: 1.4666 acc_train: 0.8782 loss_val: 1.5067 acc_val: 0.8075\n",
            "Epoch: 0012 (0.0752s) loss_train: 1.4648 acc_train: 0.8800 loss_val: 1.5071 acc_val: 0.8075\n",
            "Epoch: 0013 (0.0750s) loss_train: 1.4631 acc_train: 0.8809 loss_val: 1.5078 acc_val: 0.8125\n",
            "Epoch: 0014 (0.0750s) loss_train: 1.4615 acc_train: 0.8809 loss_val: 1.5084 acc_val: 0.8125\n",
            "Epoch: 0015 (0.0749s) loss_train: 1.4606 acc_train: 0.8845 loss_val: 1.5095 acc_val: 0.8125\n",
            "Epoch: 0016 (0.0752s) loss_train: 1.4602 acc_train: 0.8881 loss_val: 1.5110 acc_val: 0.8100\n",
            "Epoch: 0017 (0.0757s) loss_train: 1.4599 acc_train: 0.8908 loss_val: 1.5123 acc_val: 0.8125\n",
            "Epoch: 0018 (0.0752s) loss_train: 1.4589 acc_train: 0.8962 loss_val: 1.5130 acc_val: 0.8125\n",
            "Epoch: 0019 (0.0749s) loss_train: 1.4581 acc_train: 0.8971 loss_val: 1.5136 acc_val: 0.8150\n",
            "Epoch: 0020 (0.0765s) loss_train: 1.4571 acc_train: 0.8980 loss_val: 1.5140 acc_val: 0.8100\n",
            "Epoch: 0021 (0.0751s) loss_train: 1.4564 acc_train: 0.9016 loss_val: 1.5144 acc_val: 0.8050\n",
            "Epoch: 0022 (0.0749s) loss_train: 1.4552 acc_train: 0.9016 loss_val: 1.5144 acc_val: 0.8075\n",
            "Epoch: 0023 (0.0747s) loss_train: 1.4541 acc_train: 0.9034 loss_val: 1.5144 acc_val: 0.8025\n",
            "Epoch: 0024 (0.0751s) loss_train: 1.4530 acc_train: 0.9043 loss_val: 1.5145 acc_val: 0.8050\n",
            "Epoch: 0025 (0.0752s) loss_train: 1.4517 acc_train: 0.9043 loss_val: 1.5143 acc_val: 0.8025\n",
            "Epoch: 0026 (0.0748s) loss_train: 1.4504 acc_train: 0.9043 loss_val: 1.5141 acc_val: 0.8000\n",
            "Epoch: 0027 (0.0747s) loss_train: 1.4491 acc_train: 0.9052 loss_val: 1.5139 acc_val: 0.7975\n",
            "Epoch: 0028 (0.0751s) loss_train: 1.4482 acc_train: 0.9070 loss_val: 1.5140 acc_val: 0.8000\n",
            "Epoch: 0029 (0.0751s) loss_train: 1.4471 acc_train: 0.9097 loss_val: 1.5139 acc_val: 0.8025\n",
            "Epoch: 0030 (0.0751s) loss_train: 1.4461 acc_train: 0.9079 loss_val: 1.5139 acc_val: 0.8000\n",
            "Epoch: 0031 (0.0750s) loss_train: 1.4452 acc_train: 0.9079 loss_val: 1.5136 acc_val: 0.7975\n",
            "Epoch: 0032 (0.0754s) loss_train: 1.4447 acc_train: 0.9097 loss_val: 1.5136 acc_val: 0.7950\n",
            "Epoch: 0033 (0.0751s) loss_train: 1.4444 acc_train: 0.9097 loss_val: 1.5137 acc_val: 0.7925\n",
            "Epoch: 0034 (0.0749s) loss_train: 1.4442 acc_train: 0.9106 loss_val: 1.5139 acc_val: 0.7925\n",
            "Epoch: 0035 (0.0751s) loss_train: 1.4441 acc_train: 0.9143 loss_val: 1.5141 acc_val: 0.7950\n",
            "Epoch: 0036 (0.0749s) loss_train: 1.4441 acc_train: 0.9125 loss_val: 1.5144 acc_val: 0.7950\n",
            "Epoch: 0037 (0.0752s) loss_train: 1.4439 acc_train: 0.9188 loss_val: 1.5145 acc_val: 0.7950\n",
            "Epoch: 0038 (0.0753s) loss_train: 1.4441 acc_train: 0.9215 loss_val: 1.5147 acc_val: 0.8000\n",
            "Epoch: 0039 (0.0753s) loss_train: 1.4447 acc_train: 0.9233 loss_val: 1.5152 acc_val: 0.8025\n",
            "Epoch: 0040 (0.0749s) loss_train: 1.4454 acc_train: 0.9215 loss_val: 1.5159 acc_val: 0.8050\n",
            "Epoch: 0041 (0.0751s) loss_train: 1.4465 acc_train: 0.9206 loss_val: 1.5170 acc_val: 0.8050\n",
            "Epoch: 0042 (0.0761s) loss_train: 1.4474 acc_train: 0.9224 loss_val: 1.5179 acc_val: 0.8075\n",
            "Epoch: 0043 (0.0765s) loss_train: 1.4480 acc_train: 0.9233 loss_val: 1.5185 acc_val: 0.8075\n",
            "Epoch: 0044 (0.0752s) loss_train: 1.4482 acc_train: 0.9233 loss_val: 1.5189 acc_val: 0.8100\n",
            "Epoch: 0045 (0.0751s) loss_train: 1.4483 acc_train: 0.9242 loss_val: 1.5191 acc_val: 0.8150\n",
            "Epoch: 0046 (0.0751s) loss_train: 1.4482 acc_train: 0.9251 loss_val: 1.5191 acc_val: 0.8125\n",
            "Epoch: 0047 (0.0754s) loss_train: 1.4477 acc_train: 0.9233 loss_val: 1.5189 acc_val: 0.8150\n",
            "Epoch: 0048 (0.0749s) loss_train: 1.4472 acc_train: 0.9242 loss_val: 1.5189 acc_val: 0.8150\n",
            "Epoch: 0049 (0.0750s) loss_train: 1.4465 acc_train: 0.9215 loss_val: 1.5184 acc_val: 0.8100\n",
            "Epoch: 0050 (0.0749s) loss_train: 1.4456 acc_train: 0.9215 loss_val: 1.5176 acc_val: 0.8100\n",
            "Epoch: 0051 (0.0753s) loss_train: 1.4448 acc_train: 0.9206 loss_val: 1.5170 acc_val: 0.8100\n",
            "Epoch: 0052 (0.0749s) loss_train: 1.4441 acc_train: 0.9206 loss_val: 1.5166 acc_val: 0.8125\n",
            "Epoch: 0053 (0.0750s) loss_train: 1.4438 acc_train: 0.9215 loss_val: 1.5167 acc_val: 0.8150\n",
            "Epoch: 0054 (0.0751s) loss_train: 1.4438 acc_train: 0.9224 loss_val: 1.5173 acc_val: 0.8100\n",
            "Epoch: 0055 (0.0754s) loss_train: 1.4439 acc_train: 0.9233 loss_val: 1.5181 acc_val: 0.8075\n",
            "Epoch: 0056 (0.0750s) loss_train: 1.4443 acc_train: 0.9215 loss_val: 1.5190 acc_val: 0.8075\n",
            "Epoch: 0057 (0.0752s) loss_train: 1.4449 acc_train: 0.9242 loss_val: 1.5200 acc_val: 0.8125\n",
            "Epoch: 0058 (0.0764s) loss_train: 1.4448 acc_train: 0.9233 loss_val: 1.5202 acc_val: 0.8100\n",
            "Epoch: 0059 (0.0752s) loss_train: 1.4449 acc_train: 0.9251 loss_val: 1.5207 acc_val: 0.8100\n",
            "Epoch: 0060 (0.0751s) loss_train: 1.4446 acc_train: 0.9251 loss_val: 1.5206 acc_val: 0.8075\n",
            "Epoch: 0061 (0.0750s) loss_train: 1.4441 acc_train: 0.9251 loss_val: 1.5201 acc_val: 0.8075\n",
            "Epoch: 0062 (0.0750s) loss_train: 1.4438 acc_train: 0.9233 loss_val: 1.5200 acc_val: 0.8075\n",
            "Epoch: 0063 (0.0750s) loss_train: 1.4434 acc_train: 0.9233 loss_val: 1.5197 acc_val: 0.8075\n",
            "Epoch: 0064 (0.0750s) loss_train: 1.4433 acc_train: 0.9215 loss_val: 1.5198 acc_val: 0.8075\n",
            "Epoch: 0065 (0.0752s) loss_train: 1.4434 acc_train: 0.9215 loss_val: 1.5201 acc_val: 0.8075\n",
            "Epoch: 0066 (0.0754s) loss_train: 1.4436 acc_train: 0.9242 loss_val: 1.5204 acc_val: 0.8100\n",
            "Epoch: 0067 (0.0753s) loss_train: 1.4435 acc_train: 0.9242 loss_val: 1.5208 acc_val: 0.8125\n",
            "Epoch: 0068 (0.0757s) loss_train: 1.4435 acc_train: 0.9242 loss_val: 1.5212 acc_val: 0.8125\n",
            "Epoch: 0069 (0.0752s) loss_train: 1.4434 acc_train: 0.9260 loss_val: 1.5214 acc_val: 0.8125\n",
            "Epoch: 0070 (0.0750s) loss_train: 1.4427 acc_train: 0.9233 loss_val: 1.5210 acc_val: 0.8025\n",
            "Epoch: 0071 (0.0752s) loss_train: 1.4416 acc_train: 0.9233 loss_val: 1.5198 acc_val: 0.7975\n",
            "Epoch: 0072 (0.0762s) loss_train: 1.4403 acc_train: 0.9242 loss_val: 1.5184 acc_val: 0.7975\n",
            "Epoch: 0073 (0.0757s) loss_train: 1.4390 acc_train: 0.9215 loss_val: 1.5171 acc_val: 0.7975\n",
            "Epoch: 0074 (0.0759s) loss_train: 1.4381 acc_train: 0.9215 loss_val: 1.5161 acc_val: 0.7950\n",
            "Epoch: 0075 (0.0753s) loss_train: 1.4372 acc_train: 0.9170 loss_val: 1.5151 acc_val: 0.7950\n",
            "Epoch: 0076 (0.0751s) loss_train: 1.4362 acc_train: 0.9161 loss_val: 1.5142 acc_val: 0.7950\n",
            "Epoch: 0077 (0.0754s) loss_train: 1.4355 acc_train: 0.9152 loss_val: 1.5135 acc_val: 0.7975\n",
            "Epoch: 0078 (0.0751s) loss_train: 1.4352 acc_train: 0.9134 loss_val: 1.5133 acc_val: 0.7975\n",
            "Epoch: 0079 (0.0754s) loss_train: 1.4357 acc_train: 0.9170 loss_val: 1.5137 acc_val: 0.8050\n",
            "Epoch: 0080 (0.0752s) loss_train: 1.4367 acc_train: 0.9215 loss_val: 1.5148 acc_val: 0.8050\n",
            "Epoch: 0081 (0.0754s) loss_train: 1.4378 acc_train: 0.9233 loss_val: 1.5160 acc_val: 0.8075\n",
            "Epoch: 0082 (0.0750s) loss_train: 1.4390 acc_train: 0.9251 loss_val: 1.5174 acc_val: 0.8075\n",
            "Epoch: 0083 (0.0759s) loss_train: 1.4401 acc_train: 0.9242 loss_val: 1.5185 acc_val: 0.8100\n",
            "Epoch: 0084 (0.0752s) loss_train: 1.4406 acc_train: 0.9206 loss_val: 1.5189 acc_val: 0.8100\n",
            "Epoch: 0085 (0.0752s) loss_train: 1.4413 acc_train: 0.9224 loss_val: 1.5192 acc_val: 0.8100\n",
            "Epoch: 0086 (0.0752s) loss_train: 1.4420 acc_train: 0.9233 loss_val: 1.5196 acc_val: 0.8100\n",
            "Epoch: 0087 (0.0752s) loss_train: 1.4425 acc_train: 0.9215 loss_val: 1.5198 acc_val: 0.8100\n",
            "Epoch: 0088 (0.0756s) loss_train: 1.4426 acc_train: 0.9233 loss_val: 1.5196 acc_val: 0.8100\n",
            "Epoch: 0089 (0.0751s) loss_train: 1.4425 acc_train: 0.9233 loss_val: 1.5193 acc_val: 0.8100\n",
            "Epoch: 0090 (0.0755s) loss_train: 1.4422 acc_train: 0.9251 loss_val: 1.5187 acc_val: 0.8075\n",
            "Epoch: 0091 (0.0751s) loss_train: 1.4416 acc_train: 0.9260 loss_val: 1.5179 acc_val: 0.7975\n",
            "Epoch: 0092 (0.0749s) loss_train: 1.4409 acc_train: 0.9251 loss_val: 1.5173 acc_val: 0.8050\n",
            "Epoch: 0093 (0.0754s) loss_train: 1.4402 acc_train: 0.9251 loss_val: 1.5169 acc_val: 0.8000\n",
            "Epoch: 0094 (0.0760s) loss_train: 1.4401 acc_train: 0.9242 loss_val: 1.5172 acc_val: 0.8000\n",
            "Epoch: 0095 (0.0748s) loss_train: 1.4404 acc_train: 0.9224 loss_val: 1.5178 acc_val: 0.8025\n",
            "Epoch: 0096 (0.0752s) loss_train: 1.4410 acc_train: 0.9224 loss_val: 1.5188 acc_val: 0.8000\n",
            "Epoch: 0097 (0.0758s) loss_train: 1.4412 acc_train: 0.9215 loss_val: 1.5195 acc_val: 0.7950\n",
            "Epoch: 0098 (0.0751s) loss_train: 1.4416 acc_train: 0.9251 loss_val: 1.5203 acc_val: 0.7950\n",
            "Epoch: 0099 (0.0749s) loss_train: 1.4420 acc_train: 0.9251 loss_val: 1.5208 acc_val: 0.7975\n",
            "Epoch: 0100 (0.0753s) loss_train: 1.4425 acc_train: 0.9269 loss_val: 1.5215 acc_val: 0.8000\n",
            "Epoch: 0101 (0.0758s) loss_train: 1.4425 acc_train: 0.9269 loss_val: 1.5221 acc_val: 0.8000\n",
            "Early stopping...\n",
            "Best Val Loss: 1.4856, Best Val Acc: 0.8450\n",
            "Optimization Finished!\n",
            "Total time elapsed: 7.7044s\n",
            "Loading 1th epoch\n",
            "Test set results: loss 1.4715 accuracy 0.8667\n"
          ]
        }
      ],
      "source": [
        "best_model, loss_values, acc_values, loss_test, acc_test = train_and_evaluate(GAT_cora, (features, adj_mat), labels, idx_train, idx_val, args['epochs'], args['patience'], args['print'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "2PUTUzQHVMAx",
        "outputId": "c74a46f0-e4ff-4703-be7e-fc806caf6d41"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#train_losses, train_scores, val_losses, val_scores\n",
        "def plot_accuracy2(epoch_list, train_losses, val_scores):\n",
        "    plt.figure(figsize = [10,5])\n",
        "    plt.plot(epoch_list, train_losses, 'b', label = \"validation loss\")\n",
        "    plt.plot(epoch_list, val_scores, 'r', label = \"validation accuracy\")\n",
        "    plt.title(\"Evolution of validation accuracy and loss w.r.t epochs\")\n",
        "    plt.ylim([0.0, 2.0])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.savefig('/content/drive/My Drive/GAT_cora_200_epoch.png')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "epoch_list = list(range(1, 102))\n",
        "plot_accuracy2(epoch_list, loss_values, acc_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "NmPRuLlqZII_"
      },
      "outputs": [],
      "source": [
        "def run_transductive_experiment_cora(iters=100):\n",
        "  losses = []\n",
        "  scores = []\n",
        "  for iter in range(iters):\n",
        "    #features, labels, adj_mat = load_data(device=device)\n",
        "    #idx = torch.randperm(len(labels)).to(device)\n",
        "    #idx_test, idx_val, idx_train = idx[:1000], idx[1000:1500], idx[1500:]\n",
        "    best_model, loss_values, acc_values, loss_test, acc_test = train_and_evaluate(GAT_cora, (features, adj_mat), labels, idx_train, idx_val, args['epochs'], args['patience'], False)\n",
        "    # best_model,_ ,_ ,_ ,_ = train(model, ppi_train_params, verbose=False)\n",
        "    # loss, score = evaluate(best_model, test_loader)\n",
        "    losses.append(loss_test)\n",
        "    scores.append(acc_test)\n",
        "  losses = torch.tensor(losses)\n",
        "  scores = torch.tensor(scores)\n",
        "  # return (torch.std_mean(losses), torch.std_mean(scores))\n",
        "  return losses, scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "co4I5QBXZPcc",
        "outputId": "376f33c8-dc39-4730-c83f-9b9d3d5efd68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping...\n",
            "Best Val Loss: 1.4969, Best Val Acc: 0.8200\n",
            "Optimization Finished!\n",
            "Total time elapsed: 21.5873s\n",
            "Loading 168th epoch\n",
            "Test set results: loss 1.4983 accuracy 0.8290\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5056, Best Val Acc: 0.8120\n",
            "Optimization Finished!\n",
            "Total time elapsed: 13.0155s\n",
            "Loading 62th epoch\n",
            "Test set results: loss 1.4992 accuracy 0.8180\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5013, Best Val Acc: 0.8180\n",
            "Optimization Finished!\n",
            "Total time elapsed: 12.5604s\n",
            "Loading 55th epoch\n",
            "Test set results: loss 1.4931 accuracy 0.8230\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5046, Best Val Acc: 0.8080\n",
            "Optimization Finished!\n",
            "Total time elapsed: 9.8877s\n",
            "Loading 28th epoch\n",
            "Test set results: loss 1.5071 accuracy 0.8110\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5033, Best Val Acc: 0.8120\n",
            "Optimization Finished!\n",
            "Total time elapsed: 13.4593s\n",
            "Loading 71th epoch\n",
            "Test set results: loss 1.5093 accuracy 0.8230\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5032, Best Val Acc: 0.8280\n",
            "Optimization Finished!\n",
            "Total time elapsed: 24.4065s\n",
            "Loading 192th epoch\n",
            "Test set results: loss 1.4944 accuracy 0.8230\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5067, Best Val Acc: 0.8260\n",
            "Optimization Finished!\n",
            "Total time elapsed: 20.3662s\n",
            "Loading 148th epoch\n",
            "Test set results: loss 1.5034 accuracy 0.8300\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5047, Best Val Acc: 0.8200\n",
            "Optimization Finished!\n",
            "Total time elapsed: 9.5854s\n",
            "Loading 23th epoch\n",
            "Test set results: loss 1.5002 accuracy 0.8340\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5072, Best Val Acc: 0.8240\n",
            "Optimization Finished!\n",
            "Total time elapsed: 20.2160s\n",
            "Loading 162th epoch\n",
            "Test set results: loss 1.5056 accuracy 0.8350\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5073, Best Val Acc: 0.8140\n",
            "Optimization Finished!\n",
            "Total time elapsed: 10.1956s\n",
            "Loading 28th epoch\n",
            "Test set results: loss 1.4999 accuracy 0.8140\n"
          ]
        }
      ],
      "source": [
        "losses_cora, scores_cora = run_transductive_experiment_cora(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Yfp66tyactV",
        "outputId": "5ec062f0-1a69-4110-a582-b3138a5f1b86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping...\n",
            "Best Val Loss: 1.4908, Best Val Acc: 0.8425\n",
            "Optimization Finished!\n",
            "Total time elapsed: 7.6540s\n",
            "Loading 1th epoch\n",
            "Test set results: loss 1.4761 accuracy 0.8583\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5007, Best Val Acc: 0.8325\n",
            "Optimization Finished!\n",
            "Total time elapsed: 7.6488s\n",
            "Loading 1th epoch\n",
            "Test set results: loss 1.4836 accuracy 0.8625\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5030, Best Val Acc: 0.8400\n",
            "Optimization Finished!\n",
            "Total time elapsed: 10.6371s\n",
            "Loading 39th epoch\n",
            "Test set results: loss 1.5032 accuracy 0.8175\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.5065s\n",
            "Loading 149th epoch\n",
            "Test set results: loss 1.5019 accuracy 0.8267\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.8922s\n",
            "Loading 125th epoch\n",
            "Test set results: loss 1.5035 accuracy 0.8208\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.4911s\n",
            "Loading 185th epoch\n",
            "Test set results: loss 1.5052 accuracy 0.8183\n",
            "Optimization Finished!\n",
            "Total time elapsed: 16.5558s\n",
            "Loading 126th epoch\n",
            "Test set results: loss 1.4995 accuracy 0.8258\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5046, Best Val Acc: 0.8075\n",
            "Optimization Finished!\n",
            "Total time elapsed: 10.6163s\n",
            "Loading 20th epoch\n",
            "Test set results: loss 1.5094 accuracy 0.8325\n",
            "Optimization Finished!\n",
            "Total time elapsed: 16.2072s\n",
            "Loading 180th epoch\n",
            "Test set results: loss 1.5048 accuracy 0.8242\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5138, Best Val Acc: 0.8025\n",
            "Optimization Finished!\n",
            "Total time elapsed: 10.1656s\n",
            "Loading 31th epoch\n",
            "Test set results: loss 1.5070 accuracy 0.8167\n"
          ]
        }
      ],
      "source": [
        "losses_cora_2, scores_cora_2 = run_transductive_experiment_cora(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySuaV4ENbTmy",
        "outputId": "de441988-9ffe-4751-baa1-340c2847ee64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test loss:\t\t1.4994 +/- 0.0108\n",
            "test micro F1 score: \t0.8303 +/- 0.0166\n"
          ]
        }
      ],
      "source": [
        "loss_std, loss_mean = torch.std_mean(losses_cora_2)\n",
        "score_std, score_mean = torch.std_mean(scores_cora_2)\n",
        "\n",
        "print(f'GAT test loss:\\t\\t{loss_mean:.4f} +/- {loss_std:.4f}')\n",
        "print(f'GAT test accuracy: \\t{score_mean:.4f} +/- {score_std:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7dmEaZ8ZcAy",
        "outputId": "9c6408b2-6efb-4626-c308-50bc035e4135"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test loss:\t\t1.5011 +/- 0.0053\n",
            "test micro F1 score: \t0.8240 +/- 0.0081\n"
          ]
        }
      ],
      "source": [
        "loss_std, loss_mean = torch.std_mean(losses_cora)\n",
        "score_std, score_mean = torch.std_mean(scores_cora)\n",
        "\n",
        "print(f'GAT test loss:\\t\\t{loss_mean:.4f} +/- {loss_std:.4f}')\n",
        "print(f'GAT test accuracy: \\t{score_mean:.4f} +/- {score_std:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4SU3OeFbjnU",
        "outputId": "b76632f7-6d49-435d-e0aa-27040ef37c5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimization Finished!\n",
            "Total time elapsed: 16.1800s\n",
            "Loading 165th epoch\n",
            "Test set results: loss 1.4999 accuracy 0.8300\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.5838s\n",
            "Loading 150th epoch\n",
            "Test set results: loss 1.5006 accuracy 0.8258\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5069, Best Val Acc: 0.8150\n",
            "Optimization Finished!\n",
            "Total time elapsed: 13.6452s\n",
            "Loading 70th epoch\n",
            "Test set results: loss 1.5001 accuracy 0.8208\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.4441s\n",
            "Loading 139th epoch\n",
            "Test set results: loss 1.5034 accuracy 0.8242\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5054, Best Val Acc: 0.8150\n",
            "Optimization Finished!\n",
            "Total time elapsed: 14.0373s\n",
            "Loading 79th epoch\n",
            "Test set results: loss 1.5001 accuracy 0.8267\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5100, Best Val Acc: 0.8150\n",
            "Optimization Finished!\n",
            "Total time elapsed: 11.3669s\n",
            "Loading 47th epoch\n",
            "Test set results: loss 1.5020 accuracy 0.8250\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.4568s\n",
            "Loading 174th epoch\n",
            "Test set results: loss 1.5013 accuracy 0.8292\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5130, Best Val Acc: 0.8100\n",
            "Optimization Finished!\n",
            "Total time elapsed: 10.6122s\n",
            "Loading 38th epoch\n",
            "Test set results: loss 1.5040 accuracy 0.8267\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.6803s\n",
            "Loading 137th epoch\n",
            "Test set results: loss 1.5104 accuracy 0.8317\n",
            "Optimization Finished!\n",
            "Total time elapsed: 16.9441s\n",
            "Loading 183th epoch\n",
            "Test set results: loss 1.5000 accuracy 0.8267\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.4095s\n",
            "Loading 180th epoch\n",
            "Test set results: loss 1.5025 accuracy 0.8300\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.7862s\n",
            "Loading 194th epoch\n",
            "Test set results: loss 1.4961 accuracy 0.8183\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.6846s\n",
            "Loading 178th epoch\n",
            "Test set results: loss 1.5045 accuracy 0.8233\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5074, Best Val Acc: 0.8100\n",
            "Optimization Finished!\n",
            "Total time elapsed: 13.6154s\n",
            "Loading 67th epoch\n",
            "Test set results: loss 1.5019 accuracy 0.8292\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.9787s\n",
            "Loading 198th epoch\n",
            "Test set results: loss 1.4963 accuracy 0.8317\n",
            "Optimization Finished!\n",
            "Total time elapsed: 16.9270s\n",
            "Loading 188th epoch\n",
            "Test set results: loss 1.4981 accuracy 0.8292\n",
            "Optimization Finished!\n",
            "Total time elapsed: 16.6993s\n",
            "Loading 180th epoch\n",
            "Test set results: loss 1.4988 accuracy 0.8258\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5102, Best Val Acc: 0.8175\n",
            "Optimization Finished!\n",
            "Total time elapsed: 13.7204s\n",
            "Loading 75th epoch\n",
            "Test set results: loss 1.5076 accuracy 0.8242\n",
            "Optimization Finished!\n",
            "Total time elapsed: 16.0878s\n",
            "Loading 173th epoch\n",
            "Test set results: loss 1.5067 accuracy 0.8358\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5108, Best Val Acc: 0.8150\n",
            "Optimization Finished!\n",
            "Total time elapsed: 9.8154s\n",
            "Loading 24th epoch\n",
            "Test set results: loss 1.5019 accuracy 0.8183\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5116, Best Val Acc: 0.8050\n",
            "Optimization Finished!\n",
            "Total time elapsed: 12.1279s\n",
            "Loading 51th epoch\n",
            "Test set results: loss 1.5078 accuracy 0.8225\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5129, Best Val Acc: 0.8125\n",
            "Optimization Finished!\n",
            "Total time elapsed: 11.8664s\n",
            "Loading 51th epoch\n",
            "Test set results: loss 1.5078 accuracy 0.8325\n",
            "Optimization Finished!\n",
            "Total time elapsed: 16.8055s\n",
            "Loading 189th epoch\n",
            "Test set results: loss 1.5053 accuracy 0.8325\n",
            "Optimization Finished!\n",
            "Total time elapsed: 16.1601s\n",
            "Loading 196th epoch\n",
            "Test set results: loss 1.4987 accuracy 0.8242\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5115, Best Val Acc: 0.8125\n",
            "Optimization Finished!\n",
            "Total time elapsed: 11.1652s\n",
            "Loading 47th epoch\n",
            "Test set results: loss 1.5115 accuracy 0.8250\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.7749s\n",
            "Loading 175th epoch\n",
            "Test set results: loss 1.5022 accuracy 0.8250\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5112, Best Val Acc: 0.8075\n",
            "Optimization Finished!\n",
            "Total time elapsed: 13.4678s\n",
            "Loading 64th epoch\n",
            "Test set results: loss 1.5038 accuracy 0.8267\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5080, Best Val Acc: 0.8050\n",
            "Optimization Finished!\n",
            "Total time elapsed: 12.6101s\n",
            "Loading 57th epoch\n",
            "Test set results: loss 1.5109 accuracy 0.8258\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5139, Best Val Acc: 0.8050\n",
            "Optimization Finished!\n",
            "Total time elapsed: 11.7359s\n",
            "Loading 48th epoch\n",
            "Test set results: loss 1.5106 accuracy 0.8250\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.9861s\n",
            "Loading 159th epoch\n",
            "Test set results: loss 1.5035 accuracy 0.8158\n",
            "Optimization Finished!\n",
            "Total time elapsed: 16.2131s\n",
            "Loading 163th epoch\n",
            "Test set results: loss 1.5037 accuracy 0.8242\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5105, Best Val Acc: 0.8100\n",
            "Optimization Finished!\n",
            "Total time elapsed: 13.2211s\n",
            "Loading 73th epoch\n",
            "Test set results: loss 1.5161 accuracy 0.8275\n",
            "Optimization Finished!\n",
            "Total time elapsed: 16.5028s\n",
            "Loading 196th epoch\n",
            "Test set results: loss 1.4994 accuracy 0.8217\n",
            "Optimization Finished!\n",
            "Total time elapsed: 16.0018s\n",
            "Loading 123th epoch\n",
            "Test set results: loss 1.5050 accuracy 0.8267\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5119, Best Val Acc: 0.8075\n",
            "Optimization Finished!\n",
            "Total time elapsed: 11.4574s\n",
            "Loading 51th epoch\n",
            "Test set results: loss 1.5131 accuracy 0.8233\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5086, Best Val Acc: 0.8200\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.3596s\n",
            "Loading 86th epoch\n",
            "Test set results: loss 1.5021 accuracy 0.8258\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.4413s\n",
            "Loading 157th epoch\n",
            "Test set results: loss 1.4994 accuracy 0.8183\n",
            "Optimization Finished!\n",
            "Total time elapsed: 16.0131s\n",
            "Loading 160th epoch\n",
            "Test set results: loss 1.5033 accuracy 0.8317\n",
            "Optimization Finished!\n",
            "Total time elapsed: 16.4117s\n",
            "Loading 139th epoch\n",
            "Test set results: loss 1.5012 accuracy 0.8233\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5146, Best Val Acc: 0.8025\n",
            "Optimization Finished!\n",
            "Total time elapsed: 7.7757s\n",
            "Loading 2th epoch\n",
            "Test set results: loss 1.5103 accuracy 0.8217\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5114, Best Val Acc: 0.8125\n",
            "Optimization Finished!\n",
            "Total time elapsed: 13.6241s\n",
            "Loading 64th epoch\n",
            "Test set results: loss 1.5095 accuracy 0.8292\n",
            "Optimization Finished!\n",
            "Total time elapsed: 16.3429s\n",
            "Loading 159th epoch\n",
            "Test set results: loss 1.5069 accuracy 0.8267\n",
            "Optimization Finished!\n",
            "Total time elapsed: 16.4241s\n",
            "Loading 124th epoch\n",
            "Test set results: loss 1.5011 accuracy 0.8342\n",
            "Optimization Finished!\n",
            "Total time elapsed: 16.5468s\n",
            "Loading 171th epoch\n",
            "Test set results: loss 1.4993 accuracy 0.8225\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.3748s\n",
            "Loading 157th epoch\n",
            "Test set results: loss 1.5047 accuracy 0.8250\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5118, Best Val Acc: 0.8100\n",
            "Optimization Finished!\n",
            "Total time elapsed: 9.9855s\n",
            "Loading 28th epoch\n",
            "Test set results: loss 1.5031 accuracy 0.8175\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.8272s\n",
            "Loading 141th epoch\n",
            "Test set results: loss 1.5182 accuracy 0.8242\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5152, Best Val Acc: 0.8200\n",
            "Optimization Finished!\n",
            "Total time elapsed: 11.9870s\n",
            "Loading 36th epoch\n",
            "Test set results: loss 1.5079 accuracy 0.8225\n",
            "Optimization Finished!\n",
            "Total time elapsed: 16.9814s\n",
            "Loading 138th epoch\n",
            "Test set results: loss 1.4937 accuracy 0.8283\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5108, Best Val Acc: 0.8050\n",
            "Optimization Finished!\n",
            "Total time elapsed: 13.1803s\n",
            "Loading 65th epoch\n",
            "Test set results: loss 1.5020 accuracy 0.8325\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.5613s\n",
            "Loading 114th epoch\n",
            "Test set results: loss 1.5018 accuracy 0.8258\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5105, Best Val Acc: 0.8125\n",
            "Optimization Finished!\n",
            "Total time elapsed: 12.4199s\n",
            "Loading 57th epoch\n",
            "Test set results: loss 1.5030 accuracy 0.8342\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.5434s\n",
            "Loading 167th epoch\n",
            "Test set results: loss 1.5050 accuracy 0.8275\n",
            "Optimization Finished!\n",
            "Total time elapsed: 16.0770s\n",
            "Loading 142th epoch\n",
            "Test set results: loss 1.4990 accuracy 0.8233\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.6629s\n",
            "Loading 164th epoch\n",
            "Test set results: loss 1.5014 accuracy 0.8200\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.7066s\n",
            "Loading 189th epoch\n",
            "Test set results: loss 1.4981 accuracy 0.8325\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5143, Best Val Acc: 0.8000\n",
            "Optimization Finished!\n",
            "Total time elapsed: 7.6554s\n",
            "Loading 1th epoch\n",
            "Test set results: loss 1.5024 accuracy 0.8325\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.8765s\n",
            "Loading 137th epoch\n",
            "Test set results: loss 1.5025 accuracy 0.8208\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.6712s\n",
            "Loading 143th epoch\n",
            "Test set results: loss 1.5001 accuracy 0.8283\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.8324s\n",
            "Loading 179th epoch\n",
            "Test set results: loss 1.5026 accuracy 0.8300\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.8709s\n",
            "Loading 111th epoch\n",
            "Test set results: loss 1.5026 accuracy 0.8250\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5122, Best Val Acc: 0.8100\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.6930s\n",
            "Loading 99th epoch\n",
            "Test set results: loss 1.5096 accuracy 0.8292\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.9989s\n",
            "Loading 178th epoch\n",
            "Test set results: loss 1.5100 accuracy 0.8367\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5088, Best Val Acc: 0.8075\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.8686s\n",
            "Loading 94th epoch\n",
            "Test set results: loss 1.5085 accuracy 0.8342\n",
            "Optimization Finished!\n",
            "Total time elapsed: 16.3195s\n",
            "Loading 108th epoch\n",
            "Test set results: loss 1.5014 accuracy 0.8242\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.4878s\n",
            "Loading 196th epoch\n",
            "Test set results: loss 1.5074 accuracy 0.8317\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.4703s\n",
            "Loading 189th epoch\n",
            "Test set results: loss 1.5071 accuracy 0.8217\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5097, Best Val Acc: 0.8200\n",
            "Optimization Finished!\n",
            "Total time elapsed: 13.6532s\n",
            "Loading 72th epoch\n",
            "Test set results: loss 1.5096 accuracy 0.8242\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.6291s\n",
            "Loading 180th epoch\n",
            "Test set results: loss 1.5025 accuracy 0.8133\n",
            "Optimization Finished!\n",
            "Total time elapsed: 16.5380s\n",
            "Loading 198th epoch\n",
            "Test set results: loss 1.5012 accuracy 0.8225\n",
            "Optimization Finished!\n",
            "Total time elapsed: 16.4520s\n",
            "Loading 127th epoch\n",
            "Test set results: loss 1.4992 accuracy 0.8192\n",
            "Optimization Finished!\n",
            "Total time elapsed: 16.0459s\n",
            "Loading 166th epoch\n",
            "Test set results: loss 1.5075 accuracy 0.8242\n",
            "Optimization Finished!\n",
            "Total time elapsed: 16.6165s\n",
            "Loading 116th epoch\n",
            "Test set results: loss 1.5000 accuracy 0.8275\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.7027s\n",
            "Loading 142th epoch\n",
            "Test set results: loss 1.4997 accuracy 0.8317\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.6783s\n",
            "Loading 198th epoch\n",
            "Test set results: loss 1.5085 accuracy 0.8242\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.6481s\n",
            "Loading 165th epoch\n",
            "Test set results: loss 1.5053 accuracy 0.8217\n",
            "Optimization Finished!\n",
            "Total time elapsed: 16.2867s\n",
            "Loading 200th epoch\n",
            "Test set results: loss 1.5054 accuracy 0.8283\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5115, Best Val Acc: 0.8125\n",
            "Optimization Finished!\n",
            "Total time elapsed: 14.4575s\n",
            "Loading 84th epoch\n",
            "Test set results: loss 1.5196 accuracy 0.8308\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5122, Best Val Acc: 0.8100\n",
            "Optimization Finished!\n",
            "Total time elapsed: 16.6791s\n",
            "Loading 99th epoch\n",
            "Test set results: loss 1.5040 accuracy 0.8358\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5151, Best Val Acc: 0.8100\n",
            "Optimization Finished!\n",
            "Total time elapsed: 11.4990s\n",
            "Loading 50th epoch\n",
            "Test set results: loss 1.5153 accuracy 0.8242\n",
            "Optimization Finished!\n",
            "Total time elapsed: 16.4124s\n",
            "Loading 147th epoch\n",
            "Test set results: loss 1.5148 accuracy 0.8250\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.7659s\n",
            "Loading 138th epoch\n",
            "Test set results: loss 1.5114 accuracy 0.8283\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5119, Best Val Acc: 0.8225\n",
            "Optimization Finished!\n",
            "Total time elapsed: 10.2399s\n",
            "Loading 31th epoch\n",
            "Test set results: loss 1.5025 accuracy 0.8267\n",
            "Optimization Finished!\n",
            "Total time elapsed: 16.0004s\n",
            "Loading 193th epoch\n",
            "Test set results: loss 1.5090 accuracy 0.8258\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.8503s\n",
            "Loading 142th epoch\n",
            "Test set results: loss 1.5025 accuracy 0.8258\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5140, Best Val Acc: 0.8075\n",
            "Optimization Finished!\n",
            "Total time elapsed: 10.8991s\n",
            "Loading 33th epoch\n",
            "Test set results: loss 1.5136 accuracy 0.8200\n",
            "Optimization Finished!\n",
            "Total time elapsed: 16.6555s\n",
            "Loading 170th epoch\n",
            "Test set results: loss 1.4986 accuracy 0.8333\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.5469s\n",
            "Loading 120th epoch\n",
            "Test set results: loss 1.5061 accuracy 0.8258\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.3438s\n",
            "Loading 194th epoch\n",
            "Test set results: loss 1.5072 accuracy 0.8283\n",
            "Optimization Finished!\n",
            "Total time elapsed: 16.1202s\n",
            "Loading 148th epoch\n",
            "Test set results: loss 1.5016 accuracy 0.8217\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5135, Best Val Acc: 0.8125\n",
            "Optimization Finished!\n",
            "Total time elapsed: 12.3337s\n",
            "Loading 54th epoch\n",
            "Test set results: loss 1.5036 accuracy 0.8175\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5146, Best Val Acc: 0.8050\n",
            "Optimization Finished!\n",
            "Total time elapsed: 9.1141s\n",
            "Loading 19th epoch\n",
            "Test set results: loss 1.5204 accuracy 0.8200\n",
            "Optimization Finished!\n",
            "Total time elapsed: 17.1749s\n",
            "Loading 127th epoch\n",
            "Test set results: loss 1.5024 accuracy 0.8142\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.7402s\n",
            "Loading 165th epoch\n",
            "Test set results: loss 1.5007 accuracy 0.8267\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.6710s\n",
            "Loading 108th epoch\n",
            "Test set results: loss 1.5062 accuracy 0.8308\n",
            "Early stopping...\n",
            "Best Val Loss: 1.5161, Best Val Acc: 0.8075\n",
            "Optimization Finished!\n",
            "Total time elapsed: 10.2728s\n",
            "Loading 33th epoch\n",
            "Test set results: loss 1.5038 accuracy 0.8250\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.7513s\n",
            "Loading 141th epoch\n",
            "Test set results: loss 1.5074 accuracy 0.8267\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.9716s\n",
            "Loading 109th epoch\n",
            "Test set results: loss 1.5048 accuracy 0.8200\n",
            "Optimization Finished!\n",
            "Total time elapsed: 16.0035s\n",
            "Loading 196th epoch\n",
            "Test set results: loss 1.5089 accuracy 0.8192\n",
            "Optimization Finished!\n",
            "Total time elapsed: 15.9785s\n",
            "Loading 172th epoch\n",
            "Test set results: loss 1.5052 accuracy 0.8225\n"
          ]
        }
      ],
      "source": [
        "losses_cora_100, scores_cora_100 = run_transductive_experiment_cora(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTMWmx8feJ7R",
        "outputId": "bf8bfe19-e341-46a6-da67-c6cb80935f85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test loss:\t1.5047 +/- 0.0051\n",
            "test accuracy: \t0.8258 +/- 0.0049\n"
          ]
        }
      ],
      "source": [
        "loss_std_cora_100, loss_mean_cora_100 = torch.std_mean(losses_cora_100)\n",
        "score_std_cora_100, score_mean_cora_100 = torch.std_mean(scores_cora_100)\n",
        "\n",
        "print(f'GAT test loss:\\t{loss_mean_cora_100:.4f} +/- {loss_std_cora_100:.4f}')\n",
        "print(f'GAT test accuracy: \\t{score_mean_cora_100:.4f} +/- {score_std_cora_100:.4f}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
